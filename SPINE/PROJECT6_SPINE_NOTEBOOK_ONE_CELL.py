# Paste this entire block into ONE Jupyter cell in All of Us Workbench and run.
# This is a true single-file runner: it loads all pipeline modules in memory (no writing .py files).

import importlib
import sys
import types
from pathlib import Path

# Dependency check only (no auto-upgrades in Workbench).
REQUIRED_IMPORTS = {
    "numpy": "numpy",
    "pandas": "pandas",
    "matplotlib": "matplotlib",
    "scipy": "scipy",
    "sklearn": "scikit-learn",
    "statsmodels": "statsmodels",
    "pyarrow": "pyarrow",
    "google.cloud.bigquery": "google-cloud-bigquery",
}
missing = []
for mod_name, pip_name in REQUIRED_IMPORTS.items():
    try:
        importlib.import_module(mod_name)
    except Exception:
        missing.append(pip_name)
if missing:
    raise RuntimeError(
        "Missing required packages. Install in a separate cell, then rerun this cell:\n"
        + "pip install "
        + " ".join(sorted(set(missing)))
    )

module_sources = {
    'config': '"""Configuration for Project 6 degenerative spine + diabetes medication analyses."""\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\n\nCHANGE_LOG = [\n    "2026-02-17: Second-pass tuning set overlap-first weighting defaults with ESS-floor candidate selection and tighter truncation options.",\n    "2026-02-17: Rebuilt PROJECT6_SPINE_ALL_IN_ONE.py as a true standalone monolith generated from the modular pipeline.",\n    "2026-02-17: Added MI-first handling for BMI/HbA1c missingness with pooled PS/outcome estimates plus double-weighting sensitivity.",\n    "2026-02-17: Added overlap/ATT/IPTW weighting selection by best post-weight balance and explicit extreme-weight diagnostics (max/p95/p99/ESS).",\n    "2026-02-17: Added HbA1c unit/range normalization, winsorization guards, and Cox penalization defaults for sparse-event stability.",\n    "2026-02-16: Refactored SPINE.rtf notebook-style code into a modular pipeline with a single main() entrypoint.",\n    "2026-02-16: Preserved prior treatment group logic (metformin_only, glp1_only, combo) and made combo window configurable.",\n    "2026-02-16: Preserved prior T2DM anchor concept (201826) and prior diabetes exclusion anchors (24612, 313217, 199074).",\n    "2026-02-16: Preserved prior spine concept set IDs from existing code and exposed them in config for transparent versioning.",\n    "2026-02-16: Added explicit prevalence exclusion at/before index and incident definition > index with 730-day binary window.",\n    "2026-02-16: Added centralized All of Us n<20 suppression and model-level subgroup/event guardrails.",\n    "2026-02-16: Added BMI missingness diagnostics, BMI-observed IPW, and pattern-mixture sensitivity analyses.",\n    "2026-02-16: Added confounding-by-indication severity proxies + propensity-score IPTW + balance diagnostics.",\n    "2026-02-16: Added surveillance/utilization bias analyses and utilization-stratified sensitivity models.",\n    "2026-02-16: Added Cox time-to-event models with 2-year censoring plus logistic 2-year replication models.",\n    "2026-02-16: Added post-index weight-change extraction (6/12/24 months) and exploratory outcome analyses.",\n]\n\nASSUMPTIONS = [\n    "Execution is inside All of Us Researcher Workbench Controlled Tier with BigQuery credentials and WORKSPACE_CDR set.",\n    "Medication exposure is assigned from earliest qualifying drug_exposure_start_date for metformin and GLP-1 descendants.",\n    "Combo therapy is defined by metformin and GLP-1 starts within combo_window_days of each other.",\n    "T2DM case definition uses condition-occurrence descendants of anchor concept 201826 before/on index date.",\n    "Diabetes exclusions use descendants of concept anchors retained from existing code (24612, 313217, 199074).",\n    "Spine outcome concept list currently mirrors the existing code export; update config when final 13-SNOMED set is finalized.",\n    "Cox models use 2-year censoring and include all participants with >0 observed follow-up; binary 2-year models can remain restricted.",\n    "PH assumption diagnostics prefer lifelines when available; otherwise fallback checks are limited and reported.",\n    "Primary causal models use multiple imputation for BMI/HbA1c plus overlap/robust weighting; double-weighting for missingness is retained as sensitivity.",\n]\n\nCONCEPTS = {\n    "T2DM_ANCESTOR_CONCEPTS": [201826],\n    "DM_EXCLUSION_ANCESTOR_CONCEPTS": [24612, 313217, 199074],\n    "METFORMIN_INGREDIENT_NAMES": ["metformin"],\n    "GLP1_INGREDIENT_NAMES": [\n        "liraglutide",\n        "semaglutide",\n        "dulaglutide",\n        "exenatide",\n        "lixisenatide",\n        "albiglutide",\n        "tirzepatide",\n    ],\n    # Preserved from prior SPINE code export.\n    "SPINE_OUTCOME_SNOMED_CONCEPT_IDS": [\n        137548,\n        198520,\n        36717608,\n        4134121,\n        4134122,\n        4167097,\n        4187244,\n        608177,\n        761918,\n        77079,\n        79119,\n        80816,\n    ],\n    "BMI_MEASUREMENT_CONCEPT_IDS": [3038553, 4245997, 40762636],\n    "HBA1C_MEASUREMENT_CONCEPT_IDS": [3004410],\n    "OUTPATIENT_VISIT_ANCESTOR_CONCEPT_IDS": [9202],\n}\n\nCONFIG = {\n    "dataset": os.environ.get("WORKSPACE_CDR", "").strip(),\n    "bq_location": os.environ.get("GOOGLE_CLOUD_REGION", "US"),\n    # Optional writable dataset for cached intermediate tables.\n    "temp_dataset": os.environ.get("WORKSPACE_TEMP_DATASET", "").strip(),\n    "random_seed": 42,\n    "adult_age_min": 18,\n    "adult_age_max": 90,\n    "lookback_days": 365,\n    "combo_window_days": 30,\n    "outcome_window_days": 730,\n    "bmi_baseline_window_start_days": -365,\n    "bmi_baseline_window_end_days": 30,\n    "bmi_min": 10.0,\n    "bmi_max": 80.0,\n    "bmi_winsor_quantiles": (0.005, 0.995),\n    "bmi_center_value": 30.0,\n    "hba1c_plausible_min": 3.0,\n    "hba1c_plausible_max": 20.0,\n    "hba1c_winsor_quantiles": (0.005, 0.995),\n    "duration_winsor_quantiles": (0.005, 0.995),\n    "utilization_winsor_quantiles": (0.005, 0.995),\n    "require_730d_for_binary_models": True,\n    "cox_time_horizon_days": 730,\n    "cox_require_positive_followup_days": True,\n    "cox_penalizer": 0.1,\n    "logit_event_cell_warn_threshold": 5,\n    "logit_events_per_parameter_warn_threshold": 10.0,\n    "logit_penalty_alpha": 1e-3,\n    "logit_penalty_maxiter": 2000,\n    "force_penalized_logit_models": [\n        "logistic_interaction_obesity",\n        "logistic_interaction_continuous_bmi",\n        "utilization_adjusted_logit",\n    ],\n    "force_penalized_logit_prefixes": [\n        "stratified_logit_",\n        "utilization_tertile_",\n    ],\n    "print_tables_in_notebook": True,\n    "print_table_max_rows": 30,\n    "reference_levels": {\n        "exposure_group": "metformin_only",\n        "sex_simple": "Male",\n        "race_simple": "White",\n        "ethnicity_simple": "Hispanic or Latino",\n    },\n    "category_levels": {\n        "exposure_group": ["metformin_only", "glp1_only", "combo"],\n        "sex_simple": ["Male", "Female", "Other/Unknown"],\n        "race_simple": ["White", "Black or African American", "Asian", "Other/Unknown"],\n        "ethnicity_simple": ["Hispanic or Latino", "Not Hispanic or Latino", "Unknown"],\n    },\n    "required_analysis_columns": [\n        "person_id",\n        "exposure_group",\n        "days_followup",\n        "incident_spine_2y",\n        "event_full_followup",\n        "time_to_event_or_censor_days",\n        "person_time_2y_days",\n        "has_min_followup_2y",\n        "age_at_index",\n        "bmi",\n    ],\n    "small_cell_threshold": 20,\n    "mi_num_imputations": 5,\n    "mi_max_iter": 20,\n    "mi_target_columns": ["bmi", "hba1c_recent", "hba1c_mean_year"],\n    "missingness_weight_truncation_quantiles": (0.01, 0.99),\n    "ps_weighting_strategies": ["overlap"],\n    "ps_stabilized_weights": True,\n    "ps_weight_truncation_options": [(0.01, 0.99), (0.005, 0.995), (0.02, 0.98)],\n    "ps_weight_truncation_quantiles": (0.01, 0.99),\n    "ps_balance_target_abs_smd": 0.10,\n    "ps_min_ess_ratio": 0.30,\n    "ps_use_ml_if_available": True,\n    "ps_ml_n_estimators": 300,\n    "ps_ml_learning_rate": 0.05,\n    "ps_ml_subsample": 0.8,\n    "weight_change_target_days": [180, 365, 730],\n    "weight_change_window_tolerance_days": 60,\n    "output_dir": str(Path(__file__).resolve().parents[1] / "project6_outputs"),\n}\n\nREQUIRED_OUTPUT_FILES = [\n    "cohort_flow.csv",\n    "table1_baseline_by_treatment.csv",\n    "severity_baseline_table.csv",\n    "bmi_missingness_table.csv",\n    "complete_vs_missing_bmi_risk.csv",\n    "logistic_main_hc3.csv",\n    "logistic_interaction_obesity.csv",\n    "interaction_results.csv",\n    "logit_model_diagnostics.csv",\n    "cox_model_diagnostics.csv",\n    "cox_time_to_spine.csv",\n    "propensity_score_results.csv",\n    "balance_diagnostics.csv",\n    "utilization_bias_results.csv",\n    "weight_change_analysis.csv",\n    "forest_plot_ready.csv",\n    "km_curve_data.csv",\n    "REPORT.md",\n]\n\n\ndef validate_config() -> None:\n    dataset = CONFIG["dataset"]\n    if not dataset:\n        raise ValueError("WORKSPACE_CDR is empty. Set WORKSPACE_CDR before running.")\n\n\ndef ensure_output_dir() -> Path:\n    out_dir = Path(CONFIG["output_dir"]).resolve()\n    out_dir.mkdir(parents=True, exist_ok=True)\n    return out_dir\n',
    'suppression': '"""All of Us small-cell suppression helpers."""\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import Iterable\n\nimport pandas as pd\n\nPOLICY_NOTE = "Excluded due to All of Us cell-size policy (n<20)."\n\n\ndef _infer_count_columns(df: pd.DataFrame) -> list[str]:\n    candidates: list[str] = []\n    tokens = ("n", "count", "events", "total", "denominator", "numerator", "person_time")\n    for col in df.columns:\n        lower = col.lower()\n        if any(tok in lower for tok in tokens) and pd.api.types.is_numeric_dtype(df[col]):\n            candidates.append(col)\n    return candidates\n\n\ndef suppress_small_cells(\n    df: pd.DataFrame,\n    threshold: int = 20,\n    count_columns: Iterable[str] | None = None,\n) -> tuple[pd.DataFrame, pd.DataFrame]:\n    """Return (kept_rows, excluded_rows) where excluded rows violate any count threshold."""\n    if df.empty:\n        return df.copy(), df.copy()\n\n    cols = list(count_columns) if count_columns is not None else _infer_count_columns(df)\n    cols = [c for c in cols if c in df.columns]\n\n    if not cols:\n        return df.copy(), pd.DataFrame(columns=[*df.columns, "policy_note", "suppression_columns"])\n\n    mask = pd.Series(False, index=df.index)\n    for col in cols:\n        mask = mask | (df[col].fillna(0) < threshold)\n\n    kept = df.loc[~mask].copy()\n    excluded = df.loc[mask].copy()\n    if not excluded.empty:\n        excluded["policy_note"] = POLICY_NOTE\n        excluded["suppression_columns"] = ",".join(cols)\n        logging.warning(\n            "Suppressed %s rows due to n<%s policy. columns=%s",\n            len(excluded),\n            threshold,\n            cols,\n        )\n    return kept, excluded\n\n\ndef model_is_policy_compliant(\n    n: int,\n    events: int,\n    threshold: int,\n    *,\n    require_nonevents: bool = True,\n) -> tuple[bool, str]:\n    nonevents = n - events\n    if n < threshold:\n        return False, POLICY_NOTE\n    if events < threshold:\n        return False, POLICY_NOTE\n    if require_nonevents and nonevents < threshold:\n        return False, POLICY_NOTE\n    return True, ""\n\n\ndef append_policy_note(df: pd.DataFrame, note: str = POLICY_NOTE) -> pd.DataFrame:\n    out = df.copy()\n    out["policy_note"] = note\n    return out\n',
    'bq_utils': '"""BigQuery helper functions for parameterized cohort queries."""\n\nfrom __future__ import annotations\n\nimport logging\nimport re\nfrom dataclasses import dataclass\nfrom typing import Iterable, Sequence\n\nimport pandas as pd\n\ntry:\n    from google.api_core.exceptions import BadRequest, Forbidden, GoogleAPICallError, NotFound\n    from google.cloud import bigquery\nexcept Exception:  # pragma: no cover - local environments may not include google sdk\n    BadRequest = Forbidden = GoogleAPICallError = NotFound = Exception\n    bigquery = None  # type: ignore[assignment]\n\nDATASET_PATTERN = re.compile(r"^[A-Za-z0-9_.-]+$")\n\n\n@dataclass\nclass QueryArtifact:\n    name: str\n    row_count: int\n    table_fqn: str | None = None\n\n\ndef validate_dataset_id(dataset: str) -> str:\n    if not dataset:\n        raise ValueError("WORKSPACE_CDR is empty.")\n    if not DATASET_PATTERN.match(dataset):\n        raise ValueError(\n            "Invalid dataset id. Allowed characters: letters, numbers, underscore, dot, hyphen."\n        )\n    return dataset\n\n\ndef create_bq_client(location: str = "US") -> bigquery.Client:\n    if bigquery is None:\n        raise ImportError(\n            "google-cloud-bigquery is not installed. Run inside AoU Workbench or install: "\n            "`pip install google-cloud-bigquery`."\n        )\n    return bigquery.Client(location=location)\n\n\ndef scalar_param(name: str, ptype: str, value: object) -> bigquery.ScalarQueryParameter:\n    if bigquery is None:\n        raise ImportError("google-cloud-bigquery is required for query parameters.")\n    return bigquery.ScalarQueryParameter(name, ptype, value)\n\n\ndef array_param(name: str, ptype: str, values: Sequence[object]) -> bigquery.ArrayQueryParameter:\n    if bigquery is None:\n        raise ImportError("google-cloud-bigquery is required for query parameters.")\n    return bigquery.ArrayQueryParameter(name, ptype, list(values))\n\n\ndef run_query(\n    client: bigquery.Client,\n    sql: str,\n    params: Iterable[bigquery.ScalarQueryParameter | bigquery.ArrayQueryParameter] | None,\n    *,\n    job_name: str,\n) -> pd.DataFrame:\n    query_parameters = list(params) if params else []\n    job_config = bigquery.QueryJobConfig(query_parameters=query_parameters)\n    logging.info("Running query: %s", job_name)\n    try:\n        job = client.query(sql, job_config=job_config)\n        result = job.result()\n        df = result.to_dataframe(create_bqstorage_client=True)\n        logging.info("Finished query: %s | rows=%s", job_name, len(df))\n        return df\n    except (BadRequest, Forbidden, NotFound, GoogleAPICallError) as exc:\n        logging.exception("BigQuery query failed: %s", job_name)\n        raise RuntimeError(f"BigQuery query failed ({job_name}): {exc}") from exc\n\n\ndef materialize_query(\n    client: bigquery.Client,\n    select_sql: str,\n    params: Iterable[bigquery.ScalarQueryParameter | bigquery.ArrayQueryParameter] | None,\n    *,\n    table_fqn: str,\n    job_name: str,\n) -> QueryArtifact:\n    query_parameters = list(params) if params else []\n    sql = f"CREATE OR REPLACE TABLE `{table_fqn}` AS\\n{select_sql}"\n    job_config = bigquery.QueryJobConfig(query_parameters=query_parameters)\n    logging.info("Materializing query: %s -> %s", job_name, table_fqn)\n    try:\n        job = client.query(sql, job_config=job_config)\n        job.result()\n        cnt_df = client.query(f"SELECT COUNT(*) AS n FROM `{table_fqn}`").result().to_dataframe()\n        row_count = int(cnt_df.loc[0, "n"]) if not cnt_df.empty else 0\n        return QueryArtifact(name=job_name, row_count=row_count, table_fqn=table_fqn)\n    except (BadRequest, Forbidden, NotFound, GoogleAPICallError) as exc:\n        logging.exception("BigQuery materialization failed: %s", job_name)\n        raise RuntimeError(f"BigQuery materialization failed ({job_name}): {exc}") from exc\n',
    'cohort': '"""Cohort construction + analytic dataset extraction for Project 6."""\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Any\n\nimport pandas as pd\n\ntry:\n    from google.cloud import bigquery\nexcept Exception:  # pragma: no cover - local shells may not include google sdk\n    bigquery = Any  # type: ignore[assignment]\n\nfrom bq_utils import array_param, materialize_query, run_query, scalar_param\n\n\n@dataclass\nclass CohortData:\n    cohort_flow: pd.DataFrame\n    analytic_df: pd.DataFrame\n    post_index_bmi_df: pd.DataFrame\n\n\ndef _query_params(config: dict, concepts: dict) -> list[bigquery.ArrayQueryParameter | bigquery.ScalarQueryParameter]:\n    return [\n        array_param("metformin_names", "STRING", [x.lower() for x in concepts["METFORMIN_INGREDIENT_NAMES"]]),\n        array_param("glp1_names", "STRING", [x.lower() for x in concepts["GLP1_INGREDIENT_NAMES"]]),\n        array_param("t2dm_ancestor_concepts", "INT64", concepts["T2DM_ANCESTOR_CONCEPTS"]),\n        array_param("dm_exclusion_ancestor_concepts", "INT64", concepts["DM_EXCLUSION_ANCESTOR_CONCEPTS"]),\n        array_param("spine_concept_ids", "INT64", concepts["SPINE_OUTCOME_SNOMED_CONCEPT_IDS"]),\n        array_param("bmi_measurement_concept_ids", "INT64", concepts["BMI_MEASUREMENT_CONCEPT_IDS"]),\n        array_param("hba1c_measurement_concept_ids", "INT64", concepts["HBA1C_MEASUREMENT_CONCEPT_IDS"]),\n        array_param(\n            "outpatient_visit_ancestor_concept_ids",\n            "INT64",\n            concepts["OUTPATIENT_VISIT_ANCESTOR_CONCEPT_IDS"],\n        ),\n        scalar_param("adult_age_min", "INT64", int(config["adult_age_min"])),\n        scalar_param("adult_age_max", "INT64", int(config["adult_age_max"])),\n        scalar_param("lookback_days", "INT64", int(config["lookback_days"])),\n        scalar_param("combo_window_days", "INT64", int(config["combo_window_days"])),\n        scalar_param("outcome_window_days", "INT64", int(config["outcome_window_days"])),\n        scalar_param("bmi_window_start_days", "INT64", int(config["bmi_baseline_window_start_days"])),\n        scalar_param("bmi_window_end_days", "INT64", int(config["bmi_baseline_window_end_days"])),\n        scalar_param("bmi_min", "FLOAT64", float(config["bmi_min"])),\n        scalar_param("bmi_max", "FLOAT64", float(config["bmi_max"])),\n    ]\n\n\ndef build_query_params(\n    config: dict, concepts: dict\n) -> list[bigquery.ArrayQueryParameter | bigquery.ScalarQueryParameter]:\n    return _query_params(config, concepts)\n\n\ndef _core_ctes(dataset: str) -> str:\n    return f"""\nWITH\nmetformin_ingred AS (\n  SELECT concept_id\n  FROM `{dataset}.concept`\n  WHERE vocabulary_id = \'RxNorm\'\n    AND concept_class_id = \'Ingredient\'\n    AND LOWER(concept_name) IN UNNEST(@metformin_names)\n),\nmetformin_concepts AS (\n  SELECT concept_id FROM metformin_ingred\n  UNION DISTINCT\n  SELECT ca.descendant_concept_id AS concept_id\n  FROM `{dataset}.concept_ancestor` ca\n  JOIN metformin_ingred mi\n    ON ca.ancestor_concept_id = mi.concept_id\n),\n\nglp1_ingred AS (\n  SELECT concept_id\n  FROM `{dataset}.concept`\n  WHERE vocabulary_id = \'RxNorm\'\n    AND concept_class_id = \'Ingredient\'\n    AND LOWER(concept_name) IN UNNEST(@glp1_names)\n),\nglp1_concepts AS (\n  SELECT concept_id FROM glp1_ingred\n  UNION DISTINCT\n  SELECT ca.descendant_concept_id AS concept_id\n  FROM `{dataset}.concept_ancestor` ca\n  JOIN glp1_ingred gi\n    ON ca.ancestor_concept_id = gi.concept_id\n),\n\ninsulin_ingred AS (\n  SELECT concept_id\n  FROM `{dataset}.concept`\n  WHERE vocabulary_id = \'RxNorm\'\n    AND concept_class_id = \'Ingredient\'\n    AND LOWER(concept_name) LIKE \'insulin%\'\n),\ninsulin_concepts AS (\n  SELECT concept_id FROM insulin_ingred\n  UNION DISTINCT\n  SELECT ca.descendant_concept_id AS concept_id\n  FROM `{dataset}.concept_ancestor` ca\n  JOIN insulin_ingred ii\n    ON ca.ancestor_concept_id = ii.concept_id\n),\n\nt2dm_concepts AS (\n  SELECT DISTINCT descendant_concept_id AS concept_id\n  FROM `{dataset}.concept_ancestor`\n  WHERE ancestor_concept_id IN UNNEST(@t2dm_ancestor_concepts)\n  UNION DISTINCT\n  SELECT concept_id\n  FROM `{dataset}.concept`\n  WHERE concept_id IN UNNEST(@t2dm_ancestor_concepts)\n),\n\ndm_exclusion_concepts AS (\n  SELECT DISTINCT descendant_concept_id AS concept_id\n  FROM `{dataset}.concept_ancestor`\n  WHERE ancestor_concept_id IN UNNEST(@dm_exclusion_ancestor_concepts)\n  UNION DISTINCT\n  SELECT concept_id\n  FROM `{dataset}.concept`\n  WHERE concept_id IN UNNEST(@dm_exclusion_ancestor_concepts)\n),\n\nspine_outcome_concepts AS (\n  SELECT concept_id\n  FROM `{dataset}.concept`\n  WHERE concept_id IN UNNEST(@spine_concept_ids)\n),\n\noutpatient_visit_concepts AS (\n  SELECT DISTINCT descendant_concept_id AS concept_id\n  FROM `{dataset}.concept_ancestor`\n  WHERE ancestor_concept_id IN UNNEST(@outpatient_visit_ancestor_concept_ids)\n  UNION DISTINCT\n  SELECT concept_id\n  FROM `{dataset}.concept`\n  WHERE concept_id IN UNNEST(@outpatient_visit_ancestor_concept_ids)\n),\n\nfirst_t2dm_dx AS (\n  SELECT\n    co.person_id,\n    MIN(co.condition_start_date) AS first_t2dm_dx_date\n  FROM `{dataset}.condition_occurrence` co\n  JOIN t2dm_concepts t2\n    ON co.condition_concept_id = t2.concept_id\n  WHERE co.condition_start_date IS NOT NULL\n  GROUP BY co.person_id\n),\n\ndrug_starts AS (\n  SELECT\n    de.person_id,\n    MIN(IF(mc.concept_id IS NOT NULL, de.drug_exposure_start_date, NULL)) AS metformin_start_date,\n    MIN(IF(gc.concept_id IS NOT NULL, de.drug_exposure_start_date, NULL)) AS glp1_start_date\n  FROM `{dataset}.drug_exposure` de\n  LEFT JOIN metformin_concepts mc\n    ON de.drug_concept_id = mc.concept_id\n  LEFT JOIN glp1_concepts gc\n    ON de.drug_concept_id = gc.concept_id\n  WHERE de.drug_exposure_start_date IS NOT NULL\n  GROUP BY de.person_id\n),\n\nexposure_assigned AS (\n  SELECT\n    ds.person_id,\n    ds.metformin_start_date,\n    ds.glp1_start_date,\n    CASE\n      WHEN ds.metformin_start_date IS NOT NULL\n           AND ds.glp1_start_date IS NULL\n        THEN \'metformin_only\'\n      WHEN ds.glp1_start_date IS NOT NULL\n           AND ds.metformin_start_date IS NULL\n        THEN \'glp1_only\'\n      WHEN ds.metformin_start_date IS NOT NULL\n           AND ds.glp1_start_date IS NOT NULL\n           AND ABS(DATE_DIFF(ds.metformin_start_date, ds.glp1_start_date, DAY)) <= @combo_window_days\n        THEN \'combo\'\n      ELSE NULL\n    END AS exposure_group,\n    CASE\n      WHEN ds.metformin_start_date IS NOT NULL\n           AND ds.glp1_start_date IS NULL\n        THEN ds.metformin_start_date\n      WHEN ds.glp1_start_date IS NOT NULL\n           AND ds.metformin_start_date IS NULL\n        THEN ds.glp1_start_date\n      WHEN ds.metformin_start_date IS NOT NULL\n           AND ds.glp1_start_date IS NOT NULL\n           AND ABS(DATE_DIFF(ds.metformin_start_date, ds.glp1_start_date, DAY)) <= @combo_window_days\n        THEN LEAST(ds.metformin_start_date, ds.glp1_start_date)\n      ELSE NULL\n    END AS index_date\n  FROM drug_starts ds\n),\n\nexposure_clean AS (\n  SELECT *\n  FROM exposure_assigned\n  WHERE exposure_group IS NOT NULL\n    AND index_date IS NOT NULL\n),\n\nt2dm_eligible AS (\n  SELECT\n    ec.person_id,\n    ec.exposure_group,\n    ec.index_date,\n    ec.metformin_start_date,\n    ec.glp1_start_date,\n    t2.first_t2dm_dx_date\n  FROM exposure_clean ec\n  JOIN first_t2dm_dx t2\n    ON t2.person_id = ec.person_id\n   AND t2.first_t2dm_dx_date <= ec.index_date\n),\n\nadult_population AS (\n  SELECT\n    te.*,\n    DATE(p.birth_datetime) AS birth_date,\n    SAFE_DIVIDE(DATE_DIFF(te.index_date, DATE(p.birth_datetime), DAY), 365.25) AS age_at_index,\n    p.sex_at_birth_concept_id,\n    p.race_concept_id,\n    p.ethnicity_concept_id\n  FROM t2dm_eligible te\n  JOIN `{dataset}.person` p\n    ON p.person_id = te.person_id\n  WHERE SAFE_DIVIDE(DATE_DIFF(te.index_date, DATE(p.birth_datetime), DAY), 365.25)\n        BETWEEN @adult_age_min AND @adult_age_max\n),\n\nobservation_eligible AS (\n  SELECT\n    ap.*,\n    op.observation_period_start_date,\n    op.observation_period_end_date,\n    ROW_NUMBER() OVER (\n      PARTITION BY ap.person_id\n      ORDER BY op.observation_period_end_date DESC\n    ) AS op_rn\n  FROM adult_population ap\n  JOIN `{dataset}.observation_period` op\n    ON op.person_id = ap.person_id\n   AND ap.index_date BETWEEN op.observation_period_start_date AND op.observation_period_end_date\n   AND op.observation_period_start_date <= DATE_SUB(ap.index_date, INTERVAL @lookback_days DAY)\n),\n\nbaseline_cohort AS (\n  SELECT *\n  FROM observation_eligible\n  WHERE op_rn = 1\n),\n\ndm_exclusion_flag AS (\n  SELECT DISTINCT b.person_id\n  FROM baseline_cohort b\n  JOIN `{dataset}.condition_occurrence` co\n    ON co.person_id = b.person_id\n   AND co.condition_start_date <= b.index_date\n  JOIN dm_exclusion_concepts de\n    ON co.condition_concept_id = de.concept_id\n),\n\nspine_dx AS (\n  SELECT\n    b.person_id,\n    b.index_date,\n    MIN(IF(co.condition_start_date <= b.index_date, co.condition_start_date, NULL)) AS spine_dx_on_or_before_index,\n    MIN(IF(co.condition_start_date > b.index_date, co.condition_start_date, NULL)) AS first_spine_dx_after_index,\n    MIN(IF(co.condition_start_date > b.index_date\n           AND co.condition_start_date <= DATE_ADD(b.index_date, INTERVAL @outcome_window_days DAY),\n           co.condition_start_date, NULL)) AS first_spine_dx_2y\n  FROM baseline_cohort b\n  LEFT JOIN `{dataset}.condition_occurrence` co\n    ON co.person_id = b.person_id\n  LEFT JOIN spine_outcome_concepts soc\n    ON co.condition_concept_id = soc.concept_id\n  WHERE soc.concept_id IS NOT NULL OR co.person_id IS NULL\n  GROUP BY b.person_id, b.index_date\n),\n\nbmi_baseline_ranked AS (\n  SELECT\n    b.person_id,\n    b.index_date,\n    m.measurement_date,\n    m.value_as_number AS bmi,\n    ROW_NUMBER() OVER (\n      PARTITION BY b.person_id, b.index_date\n      ORDER BY\n        CASE WHEN m.measurement_date <= b.index_date THEN 0 ELSE 1 END,\n        ABS(DATE_DIFF(m.measurement_date, b.index_date, DAY)),\n        m.measurement_date DESC\n    ) AS rn\n  FROM baseline_cohort b\n  JOIN `{dataset}.measurement` m\n    ON m.person_id = b.person_id\n   AND m.measurement_concept_id IN UNNEST(@bmi_measurement_concept_ids)\n   AND m.value_as_number BETWEEN @bmi_min AND @bmi_max\n   AND m.measurement_date BETWEEN DATE_ADD(b.index_date, INTERVAL @bmi_window_start_days DAY)\n                             AND DATE_ADD(b.index_date, INTERVAL @bmi_window_end_days DAY)\n),\n\nbmi_baseline AS (\n  SELECT\n    person_id,\n    index_date,\n    bmi,\n    measurement_date AS bmi_measurement_date\n  FROM bmi_baseline_ranked\n  WHERE rn = 1\n),\n\nhba1c_raw AS (\n  SELECT\n    b.person_id,\n    b.index_date,\n    m.measurement_date,\n    m.value_as_number AS hba1c_value\n  FROM baseline_cohort b\n  JOIN `{dataset}.measurement` m\n    ON m.person_id = b.person_id\n   AND m.measurement_concept_id IN UNNEST(@hba1c_measurement_concept_ids)\n   AND m.value_as_number IS NOT NULL\n   AND m.measurement_date BETWEEN DATE_SUB(b.index_date, INTERVAL @lookback_days DAY)\n                             AND b.index_date\n),\n\nhba1c_baseline AS (\n  SELECT\n    person_id,\n    index_date,\n    ARRAY_AGG(hba1c_value ORDER BY measurement_date DESC LIMIT 1)[SAFE_OFFSET(0)] AS hba1c_recent,\n    AVG(hba1c_value) AS hba1c_mean_year,\n    COUNT(*) AS hba1c_measurements_baseline\n  FROM hba1c_raw\n  GROUP BY person_id, index_date\n),\n\ninsulin_baseline AS (\n  SELECT\n    b.person_id,\n    b.index_date,\n    MAX(IF(ic.concept_id IS NOT NULL, 1, 0)) AS insulin_use_baseline\n  FROM baseline_cohort b\n  LEFT JOIN `{dataset}.drug_exposure` de\n    ON de.person_id = b.person_id\n   AND de.drug_exposure_start_date BETWEEN DATE_SUB(b.index_date, INTERVAL @lookback_days DAY)\n                                      AND b.index_date\n  LEFT JOIN insulin_concepts ic\n    ON de.drug_concept_id = ic.concept_id\n  GROUP BY b.person_id, b.index_date\n),\n\ncomplications_baseline AS (\n  SELECT\n    b.person_id,\n    b.index_date,\n    MAX(IF(LOWER(c.concept_name) LIKE \'%diabet%neuropathy%\', 1, 0)) AS neuropathy_baseline,\n    MAX(IF(LOWER(c.concept_name) LIKE \'%diabet%nephropathy%\' OR LOWER(c.concept_name) LIKE \'%diabetic kidney%\', 1, 0)) AS nephropathy_baseline,\n    MAX(IF(LOWER(c.concept_name) LIKE \'%diabet%retinopathy%\', 1, 0)) AS retinopathy_baseline,\n    COUNT(DISTINCT co.condition_concept_id) AS baseline_condition_count\n  FROM baseline_cohort b\n  LEFT JOIN `{dataset}.condition_occurrence` co\n    ON co.person_id = b.person_id\n   AND co.condition_start_date BETWEEN DATE_SUB(b.index_date, INTERVAL @lookback_days DAY)\n                                  AND b.index_date\n  LEFT JOIN `{dataset}.concept` c\n    ON c.concept_id = co.condition_concept_id\n  GROUP BY b.person_id, b.index_date\n),\n\nutilization AS (\n  SELECT\n    b.person_id,\n    b.index_date,\n    COUNTIF(\n      vo.visit_start_date BETWEEN DATE_SUB(b.index_date, INTERVAL @lookback_days DAY) AND b.index_date\n      AND ovc.concept_id IS NOT NULL\n    ) AS baseline_outpatient_visits,\n    COUNTIF(\n      vo.visit_start_date > b.index_date\n      AND vo.visit_start_date <= DATE_ADD(b.index_date, INTERVAL @outcome_window_days DAY)\n      AND ovc.concept_id IS NOT NULL\n    ) AS followup_outpatient_visits_2y,\n    COUNTIF(\n      vo.visit_start_date BETWEEN DATE_SUB(b.index_date, INTERVAL @lookback_days DAY) AND b.index_date\n      AND LOWER(sc.concept_name) LIKE \'%endocrin%\'\n    ) AS baseline_endocrinology_visits,\n    COUNTIF(\n      vo.visit_start_date BETWEEN DATE_SUB(b.index_date, INTERVAL @lookback_days DAY) AND b.index_date\n      AND LOWER(sc.concept_name) LIKE \'%orthop%\'\n    ) AS baseline_orthopedics_visits\n  FROM baseline_cohort b\n  LEFT JOIN `{dataset}.visit_occurrence` vo\n    ON vo.person_id = b.person_id\n  LEFT JOIN outpatient_visit_concepts ovc\n    ON vo.visit_concept_id = ovc.concept_id\n  LEFT JOIN `{dataset}.provider` pr\n    ON pr.provider_id = vo.provider_id\n  LEFT JOIN `{dataset}.concept` sc\n    ON sc.concept_id = pr.specialty_concept_id\n  GROUP BY b.person_id, b.index_date\n),\n\nimaging_utilization AS (\n  SELECT\n    b.person_id,\n    b.index_date,\n    COUNTIF(\n      po.procedure_date BETWEEN DATE_SUB(b.index_date, INTERVAL @lookback_days DAY) AND b.index_date\n      AND (\n        LOWER(pc.concept_name) LIKE \'%spine%\'\n        OR LOWER(pc.concept_name) LIKE \'%lumbar%\'\n        OR LOWER(pc.concept_name) LIKE \'%cervical%\'\n        OR LOWER(pc.concept_name) LIKE \'%thoracic%\'\n      )\n      AND (\n        LOWER(pc.concept_name) LIKE \'%mri%\'\n        OR LOWER(pc.concept_name) LIKE \'%ct%\'\n        OR LOWER(pc.concept_name) LIKE \'%x-ray%\'\n        OR LOWER(pc.concept_name) LIKE \'%radiograph%\'\n      )\n    ) AS baseline_spine_imaging,\n    COUNTIF(\n      po.procedure_date > b.index_date\n      AND po.procedure_date <= DATE_ADD(b.index_date, INTERVAL @outcome_window_days DAY)\n      AND (\n        LOWER(pc.concept_name) LIKE \'%spine%\'\n        OR LOWER(pc.concept_name) LIKE \'%lumbar%\'\n        OR LOWER(pc.concept_name) LIKE \'%cervical%\'\n        OR LOWER(pc.concept_name) LIKE \'%thoracic%\'\n      )\n      AND (\n        LOWER(pc.concept_name) LIKE \'%mri%\'\n        OR LOWER(pc.concept_name) LIKE \'%ct%\'\n        OR LOWER(pc.concept_name) LIKE \'%x-ray%\'\n        OR LOWER(pc.concept_name) LIKE \'%radiograph%\'\n      )\n    ) AS followup_spine_imaging_2y\n  FROM baseline_cohort b\n  LEFT JOIN `{dataset}.procedure_occurrence` po\n    ON po.person_id = b.person_id\n  LEFT JOIN `{dataset}.concept` pc\n    ON pc.concept_id = po.procedure_concept_id\n  GROUP BY b.person_id, b.index_date\n),\n\nback_pain_baseline AS (\n  SELECT\n    b.person_id,\n    b.index_date,\n    MAX(IF(\n      LOWER(c.concept_name) LIKE \'%back pain%\'\n      OR LOWER(c.concept_name) LIKE \'%low back pain%\'\n      OR LOWER(c.concept_name) LIKE \'%lumbago%\'\n      OR LOWER(c.concept_name) LIKE \'%sciatica%\',\n      1, 0\n    )) AS baseline_back_pain_flag\n  FROM baseline_cohort b\n  LEFT JOIN `{dataset}.condition_occurrence` co\n    ON co.person_id = b.person_id\n   AND co.condition_start_date BETWEEN DATE_SUB(b.index_date, INTERVAL @lookback_days DAY)\n                                  AND b.index_date\n  LEFT JOIN `{dataset}.concept` c\n    ON c.concept_id = co.condition_concept_id\n  GROUP BY b.person_id, b.index_date\n),\n\nfinal_cohort AS (\n  SELECT\n    b.person_id,\n    b.exposure_group,\n    b.index_date,\n    b.observation_period_start_date,\n    b.observation_period_end_date,\n    DATE_DIFF(b.observation_period_end_date, b.index_date, DAY) AS days_followup,\n    IF(DATE_DIFF(b.observation_period_end_date, b.index_date, DAY) >= @outcome_window_days, 1, 0) AS has_min_followup_2y,\n    b.birth_date,\n    b.age_at_index,\n    sex.concept_name AS sex_at_birth,\n    race.concept_name AS race,\n    eth.concept_name AS ethnicity,\n    DATE_DIFF(b.index_date, b.first_t2dm_dx_date, DAY) AS diabetes_duration_days,\n    bmi.bmi,\n    IF(bmi.bmi IS NOT NULL, 1, 0) AS bmi_present,\n    IF(bmi.bmi >= 30, 1, 0) AS obese_bmi30,\n    hb.hba1c_recent,\n    hb.hba1c_mean_year,\n    hb.hba1c_measurements_baseline,\n    COALESCE(ins.insulin_use_baseline, 0) AS insulin_use_baseline,\n    COALESCE(comp.neuropathy_baseline, 0) AS neuropathy_baseline,\n    COALESCE(comp.nephropathy_baseline, 0) AS nephropathy_baseline,\n    COALESCE(comp.retinopathy_baseline, 0) AS retinopathy_baseline,\n    COALESCE(comp.baseline_condition_count, 0) AS baseline_condition_count,\n    COALESCE(util.baseline_outpatient_visits, 0) AS baseline_outpatient_visits,\n    COALESCE(util.followup_outpatient_visits_2y, 0) AS followup_outpatient_visits_2y,\n    COALESCE(util.baseline_endocrinology_visits, 0) AS baseline_endocrinology_visits,\n    COALESCE(util.baseline_orthopedics_visits, 0) AS baseline_orthopedics_visits,\n    COALESCE(img.baseline_spine_imaging, 0) AS baseline_spine_imaging,\n    COALESCE(img.followup_spine_imaging_2y, 0) AS followup_spine_imaging_2y,\n    COALESCE(bp.baseline_back_pain_flag, 0) AS baseline_back_pain_flag,\n    sd.first_spine_dx_after_index,\n    sd.first_spine_dx_2y,\n    IF(sd.first_spine_dx_2y IS NOT NULL, 1, 0) AS incident_spine_2y,\n    IF(sd.first_spine_dx_after_index IS NOT NULL, 1, 0) AS event_full_followup,\n    IF(\n      sd.first_spine_dx_after_index IS NOT NULL,\n      DATE_DIFF(sd.first_spine_dx_after_index, b.index_date, DAY),\n      DATE_DIFF(b.observation_period_end_date, b.index_date, DAY)\n    ) AS time_to_event_or_censor_days,\n    LEAST(\n      DATE_DIFF(b.observation_period_end_date, b.index_date, DAY),\n      @outcome_window_days\n    ) AS person_time_2y_days\n  FROM baseline_cohort b\n  LEFT JOIN `{dataset}.concept` sex ON sex.concept_id = b.sex_at_birth_concept_id\n  LEFT JOIN `{dataset}.concept` race ON race.concept_id = b.race_concept_id\n  LEFT JOIN `{dataset}.concept` eth ON eth.concept_id = b.ethnicity_concept_id\n  LEFT JOIN bmi_baseline bmi\n    ON bmi.person_id = b.person_id AND bmi.index_date = b.index_date\n  LEFT JOIN hba1c_baseline hb\n    ON hb.person_id = b.person_id AND hb.index_date = b.index_date\n  LEFT JOIN insulin_baseline ins\n    ON ins.person_id = b.person_id AND ins.index_date = b.index_date\n  LEFT JOIN complications_baseline comp\n    ON comp.person_id = b.person_id AND comp.index_date = b.index_date\n  LEFT JOIN utilization util\n    ON util.person_id = b.person_id AND util.index_date = b.index_date\n  LEFT JOIN imaging_utilization img\n    ON img.person_id = b.person_id AND img.index_date = b.index_date\n  LEFT JOIN back_pain_baseline bp\n    ON bp.person_id = b.person_id AND bp.index_date = b.index_date\n  LEFT JOIN spine_dx sd\n    ON sd.person_id = b.person_id AND sd.index_date = b.index_date\n  LEFT JOIN dm_exclusion_flag dex\n    ON dex.person_id = b.person_id\n  WHERE dex.person_id IS NULL\n    AND sd.spine_dx_on_or_before_index IS NULL\n)\n"""\n\n\ndef build_analytic_dataset_sql(dataset: str) -> str:\n    return _core_ctes(dataset) + "\\nSELECT * FROM final_cohort"\n\n\ndef build_cohort_flow_sql(dataset: str) -> str:\n    return (\n        _core_ctes(dataset)\n        + """\nSELECT \'01_drug_start_any\' AS step, COUNT(DISTINCT person_id) AS n\nFROM drug_starts\nWHERE metformin_start_date IS NOT NULL OR glp1_start_date IS NOT NULL\nUNION ALL\nSELECT \'02_exposure_group_assigned\', COUNT(DISTINCT person_id)\nFROM exposure_clean\nUNION ALL\nSELECT \'03_t2dm_prior_to_index\', COUNT(DISTINCT person_id)\nFROM t2dm_eligible\nUNION ALL\nSELECT \'04_adult_population\', COUNT(DISTINCT person_id)\nFROM adult_population\nUNION ALL\nSELECT \'05_observation_lookback_eligible\', COUNT(DISTINCT person_id)\nFROM baseline_cohort\nUNION ALL\nSELECT \'06_after_dm_exclusions\', COUNT(DISTINCT b.person_id)\nFROM baseline_cohort b\nLEFT JOIN dm_exclusion_flag d ON d.person_id = b.person_id\nWHERE d.person_id IS NULL\nUNION ALL\nSELECT \'07_after_prevalent_spine_exclusion\', COUNT(DISTINCT b.person_id)\nFROM baseline_cohort b\nLEFT JOIN spine_dx s\n  ON s.person_id = b.person_id AND s.index_date = b.index_date\nLEFT JOIN dm_exclusion_flag d\n  ON d.person_id = b.person_id\nWHERE d.person_id IS NULL\n  AND s.spine_dx_on_or_before_index IS NULL\nUNION ALL\nSELECT \'08_final_analytic_cohort\', COUNT(DISTINCT person_id)\nFROM final_cohort\nUNION ALL\nSELECT \'09_has_730d_followup\', COUNT(DISTINCT person_id)\nFROM final_cohort\nWHERE has_min_followup_2y = 1\n"""\n    )\n\n\ndef build_post_index_bmi_sql(dataset: str) -> str:\n    return (\n        _core_ctes(dataset)\n        + """\nSELECT\n  fc.person_id,\n  fc.exposure_group,\n  fc.index_date,\n  m.measurement_date,\n  m.value_as_number AS bmi\nFROM final_cohort fc\nJOIN `{dataset}.measurement` m\n  ON m.person_id = fc.person_id\n AND m.measurement_concept_id IN UNNEST(@bmi_measurement_concept_ids)\n AND m.value_as_number BETWEEN @bmi_min AND @bmi_max\n AND m.measurement_date > fc.index_date\n AND m.measurement_date <= DATE_ADD(fc.index_date, INTERVAL @outcome_window_days DAY)\n""".format(dataset=dataset)\n    )\n\n\ndef fetch_cohort_data(client: bigquery.Client, dataset: str, config: dict, concepts: dict) -> CohortData:\n    params = _query_params(config, concepts)\n    flow_sql = build_cohort_flow_sql(dataset)\n    analytic_sql = build_analytic_dataset_sql(dataset)\n    post_index_bmi_sql = build_post_index_bmi_sql(dataset)\n\n    cohort_flow = run_query(\n        client,\n        flow_sql,\n        params,\n        job_name="project6_cohort_flow",\n    )\n\n    temp_dataset = str(config.get("temp_dataset", "")).strip()\n    if temp_dataset:\n        analytic_table = f"{temp_dataset}.project6_analytic_cache"\n        bmi_table = f"{temp_dataset}.project6_post_index_bmi_cache"\n        materialize_query(\n            client,\n            analytic_sql,\n            params,\n            table_fqn=analytic_table,\n            job_name="project6_materialize_analytic_cache",\n        )\n        materialize_query(\n            client,\n            post_index_bmi_sql,\n            params,\n            table_fqn=bmi_table,\n            job_name="project6_materialize_post_index_bmi_cache",\n        )\n        analytic_df = run_query(\n            client,\n            f"SELECT * FROM `{analytic_table}`",\n            None,\n            job_name="project6_read_analytic_cache",\n        )\n        post_index_bmi_df = run_query(\n            client,\n            f"SELECT * FROM `{bmi_table}`",\n            None,\n            job_name="project6_read_post_index_bmi_cache",\n        )\n    else:\n        analytic_df = run_query(\n            client,\n            analytic_sql,\n            params,\n            job_name="project6_analytic_dataset",\n        )\n        post_index_bmi_df = run_query(\n            client,\n            post_index_bmi_sql,\n            params,\n            job_name="project6_post_index_bmi",\n        )\n\n    date_cols = [\n        "index_date",\n        "observation_period_start_date",\n        "observation_period_end_date",\n        "birth_date",\n        "first_spine_dx_after_index",\n        "first_spine_dx_2y",\n    ]\n    for col in date_cols:\n        if col in analytic_df.columns:\n            analytic_df[col] = pd.to_datetime(analytic_df[col], errors="coerce")\n\n    if "measurement_date" in post_index_bmi_df.columns:\n        post_index_bmi_df["measurement_date"] = pd.to_datetime(\n            post_index_bmi_df["measurement_date"], errors="coerce"\n        )\n    if "index_date" in post_index_bmi_df.columns:\n        post_index_bmi_df["index_date"] = pd.to_datetime(post_index_bmi_df["index_date"], errors="coerce")\n\n    return CohortData(\n        cohort_flow=cohort_flow,\n        analytic_df=analytic_df,\n        post_index_bmi_df=post_index_bmi_df,\n    )\n',
    'analysis': '"""Analysis modules for Project 6 spine outcomes."""\n\nfrom __future__ import annotations\n\nimport logging\nimport math\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import Iterable\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning, PerfectSeparationError, PerfectSeparationWarning\n\nfrom suppression import POLICY_NOTE, model_is_policy_compliant\n\ntry:\n    from scipy.stats import chi2_contingency\nexcept Exception:  # pragma: no cover - optional dependency\n    chi2_contingency = None\n\ntry:\n    from lifelines import CoxPHFitter\n    from lifelines.statistics import proportional_hazard_test\nexcept Exception:  # pragma: no cover - optional dependency\n    CoxPHFitter = None\n    proportional_hazard_test = None\n\ntry:\n    from statsmodels.duration.hazard_regression import PHReg\nexcept Exception:  # pragma: no cover - optional dependency\n    PHReg = None\n\ntry:  # pragma: no cover - optional dependency\n    from sklearn.experimental import enable_iterative_imputer  # noqa: F401\n    from sklearn.impute import IterativeImputer\nexcept Exception:  # pragma: no cover - optional dependency\n    IterativeImputer = None\n\ntry:  # pragma: no cover - optional dependency\n    from sklearn.ensemble import GradientBoostingClassifier\nexcept Exception:  # pragma: no cover - optional dependency\n    GradientBoostingClassifier = None\n\n\n@dataclass\nclass AnalysisBundle:\n    table1: pd.DataFrame\n    severity_table: pd.DataFrame\n    bmi_missingness: pd.DataFrame\n    complete_vs_missing: pd.DataFrame\n    logistic_main: pd.DataFrame\n    logistic_interaction: pd.DataFrame\n    interaction_results: pd.DataFrame\n    cox_results: pd.DataFrame\n    ps_results: pd.DataFrame\n    balance_diagnostics: pd.DataFrame\n    utilization_results: pd.DataFrame\n    weight_change_results: pd.DataFrame\n    forest_ready: pd.DataFrame\n    km_curve_data: pd.DataFrame\n    suppression_exclusions: pd.DataFrame\n    logit_diagnostics: pd.DataFrame\n    cox_diagnostics: pd.DataFrame\n    notes: list[str]\n    artifacts: dict[str, object]\n\n\ndef _safe_numeric(df: pd.DataFrame, cols: Iterable[str]) -> pd.DataFrame:\n    out = df.copy()\n    for col in cols:\n        if col in out.columns:\n            out[col] = pd.to_numeric(out[col], errors="coerce")\n    return out\n\n\ndef _winsorize_series(series: pd.Series, lower_q: float, upper_q: float) -> pd.Series:\n    out = pd.to_numeric(series, errors="coerce").astype(float).copy()\n    non_null = out.dropna()\n    if non_null.empty:\n        return out\n    lo = float(non_null.quantile(lower_q))\n    hi = float(non_null.quantile(upper_q))\n    return out.clip(lower=lo, upper=hi)\n\n\ndef _normalize_hba1c_series(series: pd.Series, config: dict, notes: list[str] | None = None, *, label: str) -> pd.Series:\n    out = pd.to_numeric(series, errors="coerce").copy()\n    raw_non_null = int(out.notna().sum())\n    if raw_non_null == 0:\n        return out\n\n    # Heuristic rescaling for mixed encodings seen in AoU exports.\n    scale_10 = out.gt(20) & out.le(200)\n    scale_100 = out.gt(200) & out.le(2000)\n    scale_1000 = out.gt(2000) & out.le(200000)\n    out.loc[scale_10] = out.loc[scale_10] / 10.0\n    out.loc[scale_100] = out.loc[scale_100] / 100.0\n    out.loc[scale_1000] = out.loc[scale_1000] / 1000.0\n\n    plausible_min = float(config.get("hba1c_plausible_min", 3.0))\n    plausible_max = float(config.get("hba1c_plausible_max", 20.0))\n    invalid_mask = out.notna() & ~out.between(plausible_min, plausible_max)\n    dropped = int(invalid_mask.sum())\n    out.loc[invalid_mask] = np.nan\n\n    low_q, high_q = config.get("hba1c_winsor_quantiles", (0.005, 0.995))\n    out = _winsorize_series(out, float(low_q), float(high_q))\n\n    scaled_n = int(scale_10.sum() + scale_100.sum() + scale_1000.sum())\n    if notes is not None and (scaled_n > 0 or dropped > 0):\n        notes.append(\n            f"{label}: normalized HBA1c scaling for {scaled_n} rows and dropped {dropped} implausible rows."\n        )\n    return out\n\n\ndef _two_sided_p_from_z(z_value: float) -> float:\n    if not np.isfinite(z_value):\n        return np.nan\n    return float(math.erfc(abs(float(z_value)) / math.sqrt(2.0)))\n\n\ndef _effective_sample_size(weights: pd.Series) -> float:\n    w = pd.to_numeric(weights, errors="coerce").fillna(0.0)\n    if w.empty:\n        return 0.0\n    denom = float(np.sum(np.square(w)))\n    if denom <= 0:\n        return 0.0\n    num = float(np.sum(w))\n    return float((num * num) / denom)\n\n\ndef _category_levels(config: dict, field: str, fallback: list[str]) -> list[str]:\n    raw_levels = config.get("category_levels", {}).get(field)\n    if raw_levels:\n        return [str(x) for x in raw_levels]\n    return fallback\n\n\ndef _reference_term(field: str, config: dict) -> str:\n    ref = str(config.get("reference_levels", {}).get(field, "")).strip()\n    if ref:\n        return f\'C({field}, Treatment(reference="{ref}"))\'\n    return f"C({field})"\n\n\ndef _base_logit_terms(config: dict, *, include_exposure: bool = True) -> list[str]:\n    terms: list[str] = []\n    if include_exposure:\n        terms.append(_reference_term("exposure_group", config))\n    terms.extend(\n        [\n            _reference_term("sex_simple", config),\n            _reference_term("race_simple", config),\n            _reference_term("ethnicity_simple", config),\n            "age_at_index",\n        ]\n    )\n    return terms\n\n\ndef _ensure_required_columns(df: pd.DataFrame, config: dict, notes: list[str] | None = None) -> pd.DataFrame:\n    out = df.copy()\n    required_cols = [str(c) for c in config.get("required_analysis_columns", [])]\n    if not required_cols:\n        return out\n\n    missing = [col for col in required_cols if col not in out.columns]\n    if not missing:\n        return out\n\n    for col in missing:\n        out[col] = np.nan\n    msg = f"prepare_analysis_df: required columns missing and filled as NaN: {\', \'.join(sorted(missing))}"\n    logging.warning(msg)\n    if notes is not None:\n        notes.append(msg)\n    return out\n\n\ndef _standardize_sex(x: object) -> str:\n    if pd.isna(x):\n        return "Other/Unknown"\n    s = str(x).strip().lower()\n    if s == "male":\n        return "Male"\n    if s == "female":\n        return "Female"\n    return "Other/Unknown"\n\n\ndef _standardize_race(x: object) -> str:\n    if pd.isna(x):\n        return "Other/Unknown"\n    s = str(x).strip().lower()\n    if "white" in s:\n        return "White"\n    if "black" in s or "african" in s:\n        return "Black or African American"\n    if "asian" in s:\n        return "Asian"\n    return "Other/Unknown"\n\n\ndef _standardize_ethnicity(x: object) -> str:\n    if pd.isna(x):\n        return "Unknown"\n    s = str(x).strip().lower()\n    if "not hispanic" in s:\n        return "Not Hispanic or Latino"\n    if "hispanic" in s or "latino" in s:\n        return "Hispanic or Latino"\n    return "Unknown"\n\n\ndef _age_bin(x: float) -> str:\n    if pd.isna(x):\n        return "Unknown"\n    if x < 40:\n        return "18-39"\n    if x < 50:\n        return "40-49"\n    if x < 60:\n        return "50-59"\n    if x < 70:\n        return "60-69"\n    return "70+"\n\n\ndef prepare_analysis_df(df: pd.DataFrame, config: dict, notes: list[str] | None = None) -> pd.DataFrame:\n    out = _ensure_required_columns(df, config, notes=notes)\n    out = _safe_numeric(\n        out,\n        [\n            "age_at_index",\n            "bmi",\n            "days_followup",\n            "incident_spine_2y",\n            "event_full_followup",\n            "time_to_event_or_censor_days",\n            "person_time_2y_days",\n            "hba1c_recent",\n            "hba1c_mean_year",\n            "diabetes_duration_days",\n            "insulin_use_baseline",\n            "neuropathy_baseline",\n            "nephropathy_baseline",\n            "retinopathy_baseline",\n            "baseline_condition_count",\n            "baseline_outpatient_visits",\n            "followup_outpatient_visits_2y",\n            "baseline_spine_imaging",\n            "followup_spine_imaging_2y",\n            "baseline_endocrinology_visits",\n            "baseline_orthopedics_visits",\n            "baseline_back_pain_flag",\n            "has_min_followup_2y",\n        ],\n    )\n\n    for col in ["index_date", "first_spine_dx_after_index", "first_spine_dx_2y"]:\n        if col in out.columns:\n            out[col] = pd.to_datetime(out[col], errors="coerce")\n\n    out["incident_spine_2y"] = out["incident_spine_2y"].fillna(0).astype(int)\n    out["event_full_followup"] = out["event_full_followup"].fillna(0).astype(int)\n    out["has_min_followup_2y"] = out["has_min_followup_2y"].fillna(0).astype(int)\n\n    if "exposure_group" not in out.columns:\n        out["exposure_group"] = np.nan\n\n    exposure_levels = _category_levels(config, "exposure_group", ["metformin_only", "glp1_only", "combo"])\n    exposure_raw = out["exposure_group"].astype(str).str.strip().str.lower()\n    exposure_missing = out["exposure_group"].isna()\n    unexpected_mask = (~exposure_missing) & (~exposure_raw.isin(exposure_levels))\n    unexpected_count = int(unexpected_mask.sum())\n    if unexpected_count:\n        bad_values = sorted(set(exposure_raw.loc[unexpected_mask].tolist()))\n        preview = ", ".join(bad_values[:5])\n        msg = (\n            f"Found {unexpected_count} rows with unexpected exposure_group values "\n            f"({preview}); setting them to missing for analysis."\n        )\n        logging.warning(msg)\n        if notes is not None:\n            notes.append(msg)\n        out.loc[unexpected_mask, "exposure_group"] = np.nan\n\n    out.loc[out["exposure_group"].notna(), "exposure_group"] = (\n        out.loc[out["exposure_group"].notna(), "exposure_group"].astype(str).str.strip().str.lower()\n    )\n\n    sex_src = out["sex_at_birth"] if "sex_at_birth" in out.columns else pd.Series(np.nan, index=out.index)\n    race_src = out["race"] if "race" in out.columns else pd.Series(np.nan, index=out.index)\n    eth_src = out["ethnicity"] if "ethnicity" in out.columns else pd.Series(np.nan, index=out.index)\n\n    out["sex_simple"] = sex_src.map(_standardize_sex)\n    out["race_simple"] = race_src.map(_standardize_race)\n    out["ethnicity_simple"] = eth_src.map(_standardize_ethnicity)\n\n    for hcol in ["hba1c_recent", "hba1c_mean_year"]:\n        if hcol in out.columns:\n            out[hcol] = _normalize_hba1c_series(out[hcol], config, notes=notes, label=hcol)\n\n    out["bmi_present"] = out["bmi"].notna().astype(int)\n    out["bmi_missing"] = (out["bmi_present"] == 0).astype(int)\n    out["obese_bmi30"] = np.where(out["bmi"].notna() & (out["bmi"] >= 30), 1, 0)\n\n    out["age_bin"] = out["age_at_index"].apply(_age_bin)\n    base_outpatient = out["baseline_outpatient_visits"] if "baseline_outpatient_visits" in out.columns else 0\n    base_spine_imaging = out["baseline_spine_imaging"] if "baseline_spine_imaging" in out.columns else 0\n    base_endo = out["baseline_endocrinology_visits"] if "baseline_endocrinology_visits" in out.columns else 0\n    base_ortho = out["baseline_orthopedics_visits"] if "baseline_orthopedics_visits" in out.columns else 0\n    out["utilization_total_baseline"] = (\n        pd.Series(base_outpatient, index=out.index).fillna(0)\n        + pd.Series(base_spine_imaging, index=out.index).fillna(0)\n        + pd.Series(base_endo, index=out.index).fillna(0)\n        + pd.Series(base_ortho, index=out.index).fillna(0)\n    )\n    util_winsor = config.get("utilization_winsor_quantiles", (0.005, 0.995))\n    for ucol in [\n        "baseline_outpatient_visits",\n        "followup_outpatient_visits_2y",\n        "baseline_spine_imaging",\n        "followup_spine_imaging_2y",\n        "baseline_endocrinology_visits",\n        "baseline_orthopedics_visits",\n        "utilization_total_baseline",\n    ]:\n        if ucol in out.columns:\n            out[ucol] = out[ucol].clip(lower=0)\n            out[ucol] = _winsorize_series(out[ucol], float(util_winsor[0]), float(util_winsor[1]))\n\n    if "diabetes_duration_days" in out.columns:\n        out.loc[out["diabetes_duration_days"] < 0, "diabetes_duration_days"] = np.nan\n        dur_winsor = config.get("duration_winsor_quantiles", (0.005, 0.995))\n        out["diabetes_duration_days"] = _winsorize_series(\n            out["diabetes_duration_days"], float(dur_winsor[0]), float(dur_winsor[1])\n        )\n        out["diabetes_duration_years"] = out["diabetes_duration_days"] / 365.25\n    else:\n        out["diabetes_duration_years"] = np.nan\n\n    # Keep implausible BMI out of analyses while preserving BMI-missing diagnostics.\n    lo, hi = float(config["bmi_min"]), float(config["bmi_max"])\n    out.loc[(out["bmi"].notna()) & ((out["bmi"] < lo) | (out["bmi"] > hi)), "bmi"] = np.nan\n    out["bmi_present"] = out["bmi"].notna().astype(int)\n    out["bmi_missing"] = 1 - out["bmi_present"]\n    out["obese_bmi30"] = np.where(out["bmi"].notna() & (out["bmi"] >= 30), 1, 0)\n    bmi_center = float(config.get("bmi_center_value", 30.0))\n    out["bmi_c"] = out["bmi"] - bmi_center\n    bmi_winsor = config.get("bmi_winsor_quantiles", (0.005, 0.995))\n    out["bmi"] = _winsorize_series(out["bmi"], float(bmi_winsor[0]), float(bmi_winsor[1]))\n    out["bmi_c"] = out["bmi"] - bmi_center\n\n    # Collapse sparse microvascular flags to stabilize sparse-outcome models.\n    micro_cols = [c for c in ["neuropathy_baseline", "nephropathy_baseline", "retinopathy_baseline"] if c in out.columns]\n    for col in micro_cols:\n        out[col] = pd.to_numeric(out[col], errors="coerce").fillna(0).clip(0, 1)\n    if micro_cols:\n        out["microvascular_any_baseline"] = (out[micro_cols].sum(axis=1) > 0).astype(int)\n    else:\n        out["microvascular_any_baseline"] = 0\n\n    out["sex_female_flag"] = (out["sex_simple"] == "Female").astype(int)\n    out["age_sq"] = out["age_at_index"] ** 2\n    out["bmi_sq"] = out["bmi"] ** 2\n    out["duration_sq"] = out["diabetes_duration_days"] ** 2\n    outpatient = (\n        pd.to_numeric(out["baseline_outpatient_visits"], errors="coerce")\n        if "baseline_outpatient_visits" in out.columns\n        else pd.Series(np.nan, index=out.index)\n    )\n    insulin = (\n        pd.to_numeric(out["insulin_use_baseline"], errors="coerce")\n        if "insulin_use_baseline" in out.columns\n        else pd.Series(np.nan, index=out.index)\n    )\n    comorbidity = (\n        pd.to_numeric(out["baseline_condition_count"], errors="coerce")\n        if "baseline_condition_count" in out.columns\n        else pd.Series(np.nan, index=out.index)\n    )\n    out["outpatient_visits_sq"] = outpatient ** 2\n    out["duration_x_insulin"] = out["diabetes_duration_days"] * insulin.fillna(0)\n    out["age_x_comorbidity"] = out["age_at_index"] * comorbidity.fillna(0)\n    out["sex_female_x_bmi"] = out["sex_female_flag"] * out["bmi"]\n\n    # Stable categories for reference groups.\n    out["exposure_group"] = pd.Categorical(\n        out["exposure_group"],\n        categories=exposure_levels,\n    )\n    out["sex_simple"] = pd.Categorical(\n        out["sex_simple"],\n        categories=_category_levels(config, "sex_simple", ["Male", "Female", "Other/Unknown"]),\n    )\n    out["race_simple"] = pd.Categorical(\n        out["race_simple"],\n        categories=_category_levels(\n            config,\n            "race_simple",\n            ["White", "Black or African American", "Asian", "Other/Unknown"],\n        ),\n    )\n    out["ethnicity_simple"] = pd.Categorical(\n        out["ethnicity_simple"],\n        categories=_category_levels(\n            config,\n            "ethnicity_simple",\n            ["Hispanic or Latino", "Not Hispanic or Latino", "Unknown"],\n        ),\n    )\n\n    return out\n\n\ndef _logit_or_table(fit, label: str) -> pd.DataFrame:\n    conf = fit.conf_int()\n    stderr = fit.bse.values if hasattr(fit, "bse") else np.repeat(np.nan, len(fit.params))\n    out = pd.DataFrame(\n        {\n            "term": fit.params.index,\n            "coef": fit.params.values,\n            "std_error": stderr,\n            "or": np.exp(fit.params.values),\n            "ci_low": np.exp(conf[0].values),\n            "ci_high": np.exp(conf[1].values),\n            "p_value": fit.pvalues.values,\n            "model": label,\n            "effect_type": "OR",\n            "effect_scale": "log",\n        }\n    )\n    return out\n\n\ndef _glm_or_table(fit, label: str) -> pd.DataFrame:\n    params = fit.params\n    conf = fit.conf_int()\n    stderr = fit.bse.values if hasattr(fit, "bse") else np.repeat(np.nan, len(params))\n    out = pd.DataFrame(\n        {\n            "term": params.index,\n            "coef": params.values,\n            "std_error": stderr,\n            "or": np.exp(params.values),\n            "ci_low": np.exp(conf[0].values),\n            "ci_high": np.exp(conf[1].values),\n            "p_value": fit.pvalues.values,\n            "model": label,\n            "effect_type": "OR",\n            "effect_scale": "log",\n        }\n    )\n    return out\n\n\ndef _create_policy_note_df(name: str) -> pd.DataFrame:\n    return pd.DataFrame({"analysis": [name], "policy_note": [POLICY_NOTE]})\n\n\ndef _fit_regularized_logit(model, label: str, alpha: float, maxiter: int) -> tuple[pd.DataFrame, object]:\n    # Suppress known solver chatter; hard failures still raise exceptions.\n    with warnings.catch_warnings():\n        warnings.filterwarnings("ignore", category=PerfectSeparationWarning)\n        warnings.filterwarnings("ignore", category=ConvergenceWarning)\n        warnings.filterwarnings("ignore", category=RuntimeWarning)\n        fit = model.fit_regularized(alpha=alpha, L1_wt=0.0, disp=False, maxiter=maxiter)\n\n    names = getattr(model, "exog_names", None)\n    raw_params = np.asarray(fit.params).reshape(-1)\n    if names is None or len(names) != len(raw_params):\n        names = [f"param_{i}" for i in range(len(raw_params))]\n    params = pd.Series(raw_params, index=names)\n\n    out = pd.DataFrame(\n        {\n            "term": params.index,\n            "coef": params.values,\n            "std_error": np.nan,\n            "or": np.exp(params.values),\n            "ci_low": np.nan,\n            "ci_high": np.nan,\n            "p_value": np.nan,\n            "model": label,\n            "estimator": "penalized_logit_ridge",\n            "penalty_alpha": alpha,\n            "effect_type": "OR",\n            "effect_scale": "log",\n        }\n    )\n    return out, fit\n\n\ndef _should_force_penalized(label: str, config: dict | None) -> bool:\n    cfg = config or {}\n    forced_labels = {str(x) for x in cfg.get("force_penalized_logit_models", [])}\n    if label in forced_labels:\n        return True\n    forced_prefixes = [str(x) for x in cfg.get("force_penalized_logit_prefixes", [])]\n    return any(label.startswith(prefix) for prefix in forced_prefixes)\n\n\ndef _penalized_logit_with_note(\n    *,\n    model,\n    label: str,\n    reason: str,\n    config: dict | None,\n    notes: list[str],\n    model_store: dict[str, object] | None,\n) -> pd.DataFrame:\n    alpha = float((config or {}).get("logit_penalty_alpha", 1e-3))\n    maxiter = int((config or {}).get("logit_penalty_maxiter", 2000))\n    notes.append(\n        f"{label}: {reason}; using ridge-penalized logit (alpha={alpha}, maxiter={maxiter})."\n    )\n    try:\n        out, fit = _fit_regularized_logit(model, label=label, alpha=alpha, maxiter=maxiter)\n        if model_store is not None:\n            model_store[label] = fit\n        return out\n    except Exception as penalized_exc:  # pragma: no cover - numerical edge cases\n        notes.append(f"{label}: penalized fallback failed ({penalized_exc}).")\n        return pd.DataFrame({"analysis": [label], "error": [str(penalized_exc)]})\n\n\ndef _build_logit_diagnostics(\n    data: pd.DataFrame,\n    *,\n    label: str,\n    event_col: str,\n    parameter_count: int | None,\n    config: dict | None = None,\n) -> pd.DataFrame:\n    tiny_threshold = int((config or {}).get("logit_event_cell_warn_threshold", 5))\n    epv_warn = float((config or {}).get("logit_events_per_parameter_warn_threshold", 10.0))\n\n    n = int(len(data))\n    events = int(data[event_col].sum()) if event_col in data.columns else 0\n    event_rate = (events / n) if n else np.nan\n\n    diag_rows: list[dict[str, object]] = [\n        {\n            "analysis": label,\n            "scope": "overall",\n            "level": "overall",\n            "n": n,\n            "events": events,\n            "nonevents": int(n - events),\n            "event_rate": event_rate,\n            "n_parameters": parameter_count,\n        }\n    ]\n    logging.info("%s: n=%s events=%s event_rate=%.4f", label, n, events, event_rate if n else 0.0)\n\n    if "exposure_group" in data.columns:\n        by_exp = (\n            data.groupby("exposure_group", dropna=False, observed=False)\n            .agg(n=(event_col, "size"), events=(event_col, "sum"))\n            .reset_index()\n        )\n        if not by_exp.empty:\n            by_exp["nonevents"] = by_exp["n"] - by_exp["events"]\n            by_exp["event_rate"] = by_exp["events"] / by_exp["n"]\n            for _, row in by_exp.iterrows():\n                diag_rows.append(\n                    {\n                        "analysis": label,\n                        "scope": "by_exposure",\n                        "level": str(row["exposure_group"]),\n                        "n": int(row["n"]),\n                        "events": int(row["events"]),\n                        "nonevents": int(row["nonevents"]),\n                        "event_rate": float(row["event_rate"]) if pd.notna(row["event_rate"]) else np.nan,\n                        "n_parameters": parameter_count,\n                    }\n                )\n            tiny = by_exp.loc[(by_exp["events"] < tiny_threshold) | (by_exp["nonevents"] < tiny_threshold)]\n            if not tiny.empty:\n                tiny_levels = ", ".join(str(x) for x in tiny["exposure_group"].tolist())\n                logging.warning(\n                    "%s: tiny event/non-event cells by exposure (<%s): %s",\n                    label,\n                    tiny_threshold,\n                    tiny_levels,\n                )\n\n    if parameter_count is not None:\n        denom = max(parameter_count - 1, 1)\n        epv = min(events, n - events) / denom if n else np.nan\n        logging.info("%s: parameters=%s EPV=%.3f", label, parameter_count, epv if pd.notna(epv) else np.nan)\n        if pd.notna(epv) and epv < epv_warn:\n            logging.warning("%s: low events-per-parameter (%.3f < %.3f)", label, epv, epv_warn)\n\n    return pd.DataFrame(diag_rows)\n\n\ndef _fit_logit_if_allowed(\n    data: pd.DataFrame,\n    formula: str,\n    *,\n    label: str,\n    event_col: str,\n    threshold: int,\n    notes: list[str],\n    config: dict | None = None,\n    model_store: dict[str, object] | None = None,\n    diagnostics_store: list[pd.DataFrame] | None = None,\n) -> pd.DataFrame:\n    n = int(len(data))\n    events = int(data[event_col].sum()) if event_col in data.columns else 0\n\n    try:\n        model = smf.logit(formula=formula, data=data)\n        n_params = int(model.exog.shape[1]) if hasattr(model, "exog") else None\n    except Exception as exc:  # pragma: no cover\n        notes.append(f"{label}: could not build design matrix ({exc}).")\n        return pd.DataFrame({"analysis": [label], "error": [f"design matrix build failed: {exc}"]})\n\n    diag = _build_logit_diagnostics(\n        data=data,\n        label=label,\n        event_col=event_col,\n        parameter_count=n_params,\n        config=config,\n    )\n    if diagnostics_store is not None:\n        diagnostics_store.append(diag)\n\n    compliant, note = model_is_policy_compliant(n=n, events=events, threshold=threshold)\n    if not compliant:\n        notes.append(f"{label}: {note}")\n        return _create_policy_note_df(label)\n\n    if _should_force_penalized(label, config):\n        return _penalized_logit_with_note(\n            model=model,\n            label=label,\n            reason="configured to use penalized estimation",\n            config=config,\n            notes=notes,\n            model_store=model_store,\n        )\n\n    try:\n        with warnings.catch_warnings():\n            warnings.filterwarnings("error", category=PerfectSeparationWarning)\n            warnings.filterwarnings("error", category=ConvergenceWarning)\n            warnings.filterwarnings("error", category=RuntimeWarning)\n            fit = model.fit(disp=False, cov_type="HC3")\n        mle_retvals = getattr(fit, "mle_retvals", {})\n        if isinstance(mle_retvals, dict) and not bool(mle_retvals.get("converged", True)):\n            return _penalized_logit_with_note(\n                model=model,\n                label=label,\n                reason="standard MLE did not converge",\n                config=config,\n                notes=notes,\n                model_store=model_store,\n            )\n        if model_store is not None:\n            model_store[label] = fit\n        return _logit_or_table(fit, label)\n    except (\n        PerfectSeparationError,\n        PerfectSeparationWarning,\n        ConvergenceWarning,\n        np.linalg.LinAlgError,\n        RuntimeWarning,\n        OverflowError,\n    ) as exc:  # pragma: no cover - depends on data\n        return _penalized_logit_with_note(\n            model=model,\n            label=label,\n            reason=f"unstable standard MLE ({exc})",\n            config=config,\n            notes=notes,\n            model_store=model_store,\n        )\n    except Exception as exc:  # pragma: no cover - numerical edge cases\n        notes.append(f"{label}: model failed ({exc}).")\n        return pd.DataFrame({"analysis": [label], "error": [str(exc)]})\n\n\ndef _risk_table_by_group(df: pd.DataFrame, group_col: str, event_col: str) -> pd.DataFrame:\n    out = (\n        df.groupby(group_col, dropna=False)\n        .agg(n=(event_col, "size"), events=(event_col, "sum"))\n        .reset_index()\n    )\n    out["risk"] = out["events"] / out["n"]\n    return out\n\n\ndef build_table1(df: pd.DataFrame) -> pd.DataFrame:\n    rows: list[dict[str, object]] = []\n    numeric_vars = [\n        "age_at_index",\n        "bmi",\n        "hba1c_recent",\n        "hba1c_mean_year",\n        "diabetes_duration_days",\n        "baseline_condition_count",\n        "baseline_outpatient_visits",\n        "baseline_spine_imaging",\n        "baseline_endocrinology_visits",\n        "baseline_orthopedics_visits",\n    ]\n    cat_vars = [\n        "sex_simple",\n        "race_simple",\n        "ethnicity_simple",\n        "obese_bmi30",\n        "insulin_use_baseline",\n        "neuropathy_baseline",\n        "nephropathy_baseline",\n        "retinopathy_baseline",\n        "baseline_back_pain_flag",\n    ]\n\n    for arm, g in df.groupby("exposure_group", dropna=False, observed=False):\n        arm_name = str(arm)\n        rows.append(\n            {\n                "exposure_group": arm_name,\n                "variable": "N",\n                "level": "overall",\n                "n": int(len(g)),\n                "value": float(len(g)),\n                "stat": "count",\n            }\n        )\n        for var in numeric_vars:\n            if var not in g.columns:\n                continue\n            non_null = g[var].dropna()\n            rows.append(\n                {\n                    "exposure_group": arm_name,\n                    "variable": var,\n                    "level": "mean",\n                    "n": int(non_null.shape[0]),\n                    "value": float(non_null.mean()) if len(non_null) else np.nan,\n                    "stat": "mean",\n                }\n            )\n            rows.append(\n                {\n                    "exposure_group": arm_name,\n                    "variable": var,\n                    "level": "sd",\n                    "n": int(non_null.shape[0]),\n                    "value": float(non_null.std(ddof=1)) if len(non_null) > 1 else np.nan,\n                    "stat": "sd",\n                }\n            )\n        for var in cat_vars:\n            if var not in g.columns:\n                continue\n            counts = g[var].value_counts(dropna=False)\n            for level, cnt in counts.items():\n                rows.append(\n                    {\n                        "exposure_group": arm_name,\n                        "variable": var,\n                        "level": str(level),\n                        "n": int(cnt),\n                        "value": float(cnt / len(g)) if len(g) else np.nan,\n                        "stat": "proportion",\n                    }\n                )\n    return pd.DataFrame(rows)\n\n\ndef build_severity_baseline_table(df: pd.DataFrame) -> pd.DataFrame:\n    if "exposure_group" not in df.columns:\n        return pd.DataFrame()\n\n    grouped = df.groupby("exposure_group", dropna=False, observed=False)\n    out = grouped.size().rename("n").reset_index()\n\n    metric_map = {\n        "hba1c_recent": "hba1c_recent_mean",\n        "hba1c_mean_year": "hba1c_mean_year_mean",\n        "diabetes_duration_days": "diabetes_duration_days_mean",\n        "insulin_use_baseline": "insulin_use_baseline_rate",\n        "neuropathy_baseline": "neuropathy_baseline_rate",\n        "nephropathy_baseline": "nephropathy_baseline_rate",\n        "retinopathy_baseline": "retinopathy_baseline_rate",\n        "baseline_condition_count": "baseline_condition_count_mean",\n        "baseline_back_pain_flag": "baseline_back_pain_flag_rate",\n    }\n\n    for src, dst in metric_map.items():\n        if src not in df.columns:\n            continue\n        stat = grouped[src].mean().rename(dst).reset_index()\n        out = out.merge(stat, on="exposure_group", how="left")\n    return out\n\n\ndef build_bmi_missingness_table(df: pd.DataFrame, threshold: int, notes: list[str]) -> pd.DataFrame:\n    rows: list[pd.DataFrame] = []\n\n    overall = pd.DataFrame(\n        {\n            "stratifier": ["overall"],\n            "level": ["overall"],\n            "n_total": [int(len(df))],\n            "n_bmi_missing": [int(df["bmi_missing"].sum())],\n        }\n    )\n    overall["n_bmi_present"] = overall["n_total"] - overall["n_bmi_missing"]\n    overall["missing_rate"] = overall["n_bmi_missing"] / overall["n_total"]\n    overall["chi2_p_value"] = np.nan\n    rows.append(overall)\n\n    stratifiers = ["exposure_group", "sex_simple", "race_simple", "ethnicity_simple", "age_bin"]\n    for strat in stratifiers:\n        if strat not in df.columns:\n            continue\n        tmp = (\n            df.groupby(strat, dropna=False, observed=False)\n            .agg(n_total=("person_id", "size"), n_bmi_missing=("bmi_missing", "sum"))\n            .reset_index()\n            .rename(columns={strat: "level"})\n        )\n        tmp["stratifier"] = strat\n        tmp["n_bmi_present"] = tmp["n_total"] - tmp["n_bmi_missing"]\n        tmp["missing_rate"] = tmp["n_bmi_missing"] / tmp["n_total"]\n        tmp["chi2_p_value"] = np.nan\n\n        if chi2_contingency is not None and len(tmp) >= 2:\n            contingency = pd.crosstab(df[strat], df["bmi_missing"], dropna=False)\n            if contingency.shape[1] == 2 and int(contingency.values.min()) >= threshold:\n                try:\n                    _, pval, _, _ = chi2_contingency(contingency)\n                    tmp["chi2_p_value"] = pval\n                except Exception as exc:  # pragma: no cover\n                    notes.append(f"BMI missingness chi-square failed for {strat}: {exc}")\n        rows.append(tmp)\n\n    return pd.concat(rows, ignore_index=True)\n\n\ndef bmi_missingness_selection_models(\n    df: pd.DataFrame,\n    config: dict,\n    threshold: int,\n    notes: list[str],\n    model_store: dict[str, object] | None = None,\n    diagnostics_store: list[pd.DataFrame] | None = None,\n) -> tuple[pd.DataFrame, pd.DataFrame]:\n    out_rows: list[pd.DataFrame] = []\n    working = df.copy()\n\n    # Crude risk comparison by BMI presence.\n    crude = _risk_table_by_group(\n        working.loc[working["has_min_followup_2y"] == 1],\n        group_col="bmi_present",\n        event_col="incident_spine_2y",\n    )\n    crude["analysis"] = "crude_risk_by_bmi_present"\n    out_rows.append(crude)\n\n    # Adjusted logistic selection diagnostic.\n    model_df = working.loc[\n        (working["has_min_followup_2y"] == 1)\n        & working["age_at_index"].notna()\n        & working["exposure_group"].notna()\n    ].copy()\n\n    base_terms = _base_logit_terms(config)\n    formula = "incident_spine_2y ~ bmi_present + " + " + ".join(base_terms)\n    adjusted = _fit_logit_if_allowed(\n        model_df,\n        formula,\n        label="bmi_present_adjusted_logit",\n        event_col="incident_spine_2y",\n        threshold=threshold,\n        notes=notes,\n        config=config,\n        model_store=model_store,\n        diagnostics_store=diagnostics_store,\n    )\n    out_rows.append(adjusted)\n\n    # IPW for BMI-observed process.\n    ipw_formula = (\n        "bmi_present ~ "\n        + " + ".join(base_terms)\n        + " + insulin_use_baseline + baseline_condition_count + baseline_outpatient_visits"\n    )\n\n    ipw_df = model_df.dropna(\n        subset=[\n            "bmi_present",\n            "age_at_index",\n            "insulin_use_baseline",\n            "baseline_condition_count",\n            "baseline_outpatient_visits",\n        ]\n    ).copy()\n\n    ipw_result = pd.DataFrame()\n    if len(ipw_df) >= threshold:\n        try:\n            ipw_fit = smf.logit(ipw_formula, data=ipw_df).fit(disp=False)\n            if model_store is not None:\n                model_store["bmi_observed_ipw_model"] = ipw_fit\n            ipw_df["p_obs"] = ipw_fit.predict(ipw_df).clip(1e-4, 1 - 1e-4)\n            p_obs_marginal = float(ipw_df["bmi_present"].mean())\n            ipw_df["ipw_bmi_observed"] = np.where(\n                ipw_df["bmi_present"] == 1,\n                p_obs_marginal / ipw_df["p_obs"],\n                (1 - p_obs_marginal) / (1 - ipw_df["p_obs"]),\n            )\n            q1, q99 = ipw_df["ipw_bmi_observed"].quantile([0.01, 0.99]).tolist()\n            ipw_df["ipw_bmi_observed_trunc"] = ipw_df["ipw_bmi_observed"].clip(q1, q99)\n\n            ipw_result = _logit_or_table(ipw_fit, "bmi_observed_ipw_model")\n            ipw_result["mean_weight"] = ipw_df["ipw_bmi_observed_trunc"].mean()\n            ipw_result["max_weight"] = ipw_df["ipw_bmi_observed_trunc"].max()\n        except Exception as exc:  # pragma: no cover\n            notes.append(f"BMI observed IPW model failed: {exc}")\n            ipw_result = pd.DataFrame({"analysis": ["bmi_observed_ipw_model"], "error": [str(exc)]})\n    else:\n        notes.append("BMI observed IPW model skipped due to small sample.")\n        ipw_result = _create_policy_note_df("bmi_observed_ipw_model")\n\n    out_rows.append(ipw_result)\n\n    # Pattern-mixture sensitivity for BMI-missing participants.\n    pm_rows: list[pd.DataFrame] = []\n    if len(model_df) >= threshold:\n        base_complete = model_df.copy()\n        group_means = base_complete.groupby("exposure_group", dropna=False, observed=False)["bmi"].transform("mean")\n        for delta in (-2.0, 2.0, 5.0):\n            tmp = base_complete.copy()\n            tmp["bmi_pm"] = tmp["bmi"]\n            tmp.loc[tmp["bmi_pm"].isna(), "bmi_pm"] = group_means.loc[tmp["bmi_pm"].isna()] + delta\n            tmp["bmi_pm"] = tmp["bmi_pm"].fillna(tmp["bmi"].mean())\n\n            pm_formula = "incident_spine_2y ~ " + " + ".join(base_terms) + " + bmi_pm"\n            pm_out = _fit_logit_if_allowed(\n                tmp,\n                pm_formula,\n                label=f"pattern_mixture_delta_{delta:+.1f}",\n                event_col="incident_spine_2y",\n                threshold=threshold,\n                notes=notes,\n                config=config,\n                model_store=model_store,\n                diagnostics_store=diagnostics_store,\n            )\n            pm_rows.append(pm_out)\n    if pm_rows:\n        out_rows.extend(pm_rows)\n\n    combined = pd.concat(out_rows, ignore_index=True, sort=False)\n    return combined, ipw_df\n\n\ndef run_logistic_models(\n    df: pd.DataFrame,\n    config: dict,\n    threshold: int,\n    notes: list[str],\n    model_store: dict[str, object] | None = None,\n    diagnostics_store: list[pd.DataFrame] | None = None,\n) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    require_2y = bool(config["require_730d_for_binary_models"])\n    analysis_df = df.copy()\n    if require_2y:\n        analysis_df = analysis_df.loc[analysis_df["has_min_followup_2y"] == 1].copy()\n\n    analysis_df = analysis_df.loc[\n        analysis_df["age_at_index"].notna()\n        & analysis_df["exposure_group"].notna()\n        & analysis_df["bmi_c"].notna()\n    ].copy()\n\n    exposure_term = _reference_term("exposure_group", config)\n    shared_terms = _base_logit_terms(config, include_exposure=False)\n    main_formula = "incident_spine_2y ~ " + " + ".join([exposure_term, *shared_terms, "bmi_c"])\n\n    main_out = _fit_logit_if_allowed(\n        analysis_df,\n        main_formula,\n        label="logistic_main_hc3",\n        event_col="incident_spine_2y",\n        threshold=threshold,\n        notes=notes,\n        config=config,\n        model_store=model_store,\n        diagnostics_store=diagnostics_store,\n    )\n\n    int_formula = "incident_spine_2y ~ " + " + ".join(\n        [f"{exposure_term} * obese_bmi30", *shared_terms, "bmi_c"]\n    )\n\n    int_out = _fit_logit_if_allowed(\n        analysis_df,\n        int_formula,\n        label="logistic_interaction_obesity",\n        event_col="incident_spine_2y",\n        threshold=threshold,\n        notes=notes,\n        config=config,\n        model_store=model_store,\n        diagnostics_store=diagnostics_store,\n    )\n\n    interaction_rows: list[dict[str, object]] = []\n    n_all = int(len(analysis_df))\n    events_all = int(analysis_df["incident_spine_2y"].sum()) if n_all else 0\n    compliant_all, note_all = model_is_policy_compliant(n_all, events_all, threshold)\n    if compliant_all:\n        if _should_force_penalized("logistic_interaction_obesity", config):\n            interaction_rows.append(\n                {\n                    "analysis": "interaction_joint_wald",\n                    "policy_note": "Joint Wald skipped because logistic_interaction_obesity used penalized estimation.",\n                }\n            )\n        else:\n            try:\n                fit_int = None\n                if model_store is not None:\n                    candidate = model_store.get("logistic_interaction_obesity")\n                    # Penalized fits do not provide valid Wald covariance for this joint test.\n                    if candidate is not None and hasattr(candidate, "wald_test") and hasattr(candidate, "params"):\n                        fit_int = candidate\n                if fit_int is None:\n                    with warnings.catch_warnings():\n                        warnings.filterwarnings("error", category=ConvergenceWarning)\n                        warnings.filterwarnings("error", category=RuntimeWarning)\n                        fit_int = smf.logit(formula=int_formula, data=analysis_df).fit(disp=False, cov_type="HC3")\n                    if model_store is not None:\n                        model_store["logistic_interaction_obesity_joint_wald_fit"] = fit_int\n                terms = [\n                    f"{exposure_term}[T.glp1_only]:obese_bmi30",\n                    f"{exposure_term}[T.combo]:obese_bmi30",\n                ]\n                present = [t for t in terms if t in fit_int.params.index]\n                if len(present) == 2:\n                    names = list(fit_int.params.index)\n                    r = np.zeros((2, len(names)))\n                    r[0, names.index(present[0])] = 1\n                    r[1, names.index(present[1])] = 1\n                    wald = fit_int.wald_test(r, scalar=True)\n                    interaction_rows.append(\n                        {\n                            "analysis": "interaction_joint_wald",\n                            "statistic": float(np.asarray(wald.statistic).reshape(-1)[0]),\n                            "p_value": float(wald.pvalue),\n                            "df": int(np.asarray(wald.df_denom).reshape(-1)[0]) if hasattr(wald, "df_denom") else np.nan,\n                        }\n                    )\n                else:\n                    interaction_rows.append(\n                        {\n                            "analysis": "interaction_joint_wald",\n                            "policy_note": "Interaction terms not fully present in fitted model.",\n                        }\n                    )\n            except Exception as exc:  # pragma: no cover\n                notes.append(f"Interaction Wald test failed: {exc}")\n                interaction_rows.append({"analysis": "interaction_joint_wald", "error": str(exc)})\n    else:\n        interaction_rows.append({"analysis": "interaction_joint_wald", "policy_note": note_all})\n\n    # Obesity-stratified models with policy checks.\n    for obese_value, obese_label in [(0, "bmi_lt30"), (1, "bmi_ge30")]:\n        subset = analysis_df.loc[analysis_df["obese_bmi30"] == obese_value].copy()\n        n = int(len(subset))\n        events = int(subset["incident_spine_2y"].sum()) if not subset.empty else 0\n        compliant, note = model_is_policy_compliant(n, events, threshold)\n        if not compliant:\n            interaction_rows.append(\n                {\n                    "analysis": f"stratified_logit_{obese_label}",\n                    "policy_note": note,\n                }\n            )\n            continue\n\n        formula = "incident_spine_2y ~ " + " + ".join([exposure_term, *shared_terms, "bmi_c"])\n        tab = _fit_logit_if_allowed(\n            subset,\n            formula,\n            label=f"stratified_logit_{obese_label}",\n            event_col="incident_spine_2y",\n            threshold=threshold,\n            notes=notes,\n            config=config,\n            model_store=model_store,\n            diagnostics_store=diagnostics_store,\n        )\n        interaction_rows.extend(tab.to_dict("records"))\n\n    return main_out, int_out, pd.DataFrame(interaction_rows)\n\n\ndef _prepare_cox_model_df(df: pd.DataFrame, config: dict) -> pd.DataFrame:\n    cox_df = df.copy()\n    horizon_days = int(config.get("cox_time_horizon_days", config.get("outcome_window_days", 730)))\n\n    if bool(config.get("cox_require_positive_followup_days", True)) and "days_followup" in cox_df.columns:\n        cox_df = cox_df.loc[pd.to_numeric(cox_df["days_followup"], errors="coerce") > 0].copy()\n\n    if "person_time_2y_days" in cox_df.columns and "incident_spine_2y" in cox_df.columns:\n        cox_df["cox_time_days"] = pd.to_numeric(cox_df["person_time_2y_days"], errors="coerce")\n        cox_df["cox_event"] = pd.to_numeric(cox_df["incident_spine_2y"], errors="coerce").fillna(0).astype(int)\n    else:\n        raw_time = pd.to_numeric(cox_df["time_to_event_or_censor_days"], errors="coerce")\n        cox_df["cox_time_days"] = raw_time.clip(upper=horizon_days)\n        full_event = pd.to_numeric(cox_df["event_full_followup"], errors="coerce").fillna(0).astype(int)\n        cox_df["cox_event"] = np.where((full_event == 1) & (raw_time <= horizon_days), 1, 0)\n\n    cox_df = cox_df.loc[cox_df["cox_time_days"].notna() & (cox_df["cox_time_days"] > 0)].copy()\n    return cox_df\n\n\ndef _build_cox_diagnostics(df: pd.DataFrame, label: str) -> pd.DataFrame:\n    if df.empty:\n        return pd.DataFrame(\n            [\n                {\n                    "analysis": label,\n                    "scope": "overall",\n                    "level": "overall",\n                    "n": 0,\n                    "events": 0,\n                    "nonevents": 0,\n                    "event_rate": np.nan,\n                    "person_time_days": 0.0,\n                }\n            ]\n        )\n\n    out_rows: list[dict[str, object]] = [\n        {\n            "analysis": label,\n            "scope": "overall",\n            "level": "overall",\n            "n": int(len(df)),\n            "events": int(df["cox_event"].sum()),\n            "nonevents": int(len(df) - int(df["cox_event"].sum())),\n            "event_rate": float(df["cox_event"].mean()),\n            "person_time_days": float(df["cox_time_days"].sum()),\n        }\n    ]\n    if "exposure_group" in df.columns:\n        by_group = (\n            df.groupby("exposure_group", dropna=False, observed=False)\n            .agg(\n                n=("cox_event", "size"),\n                events=("cox_event", "sum"),\n                person_time_days=("cox_time_days", "sum"),\n            )\n            .reset_index()\n        )\n        by_group["nonevents"] = by_group["n"] - by_group["events"]\n        by_group["event_rate"] = by_group["events"] / by_group["n"]\n        for _, row in by_group.iterrows():\n            out_rows.append(\n                {\n                    "analysis": label,\n                    "scope": "by_exposure",\n                    "level": str(row["exposure_group"]),\n                    "n": int(row["n"]),\n                    "events": int(row["events"]),\n                    "nonevents": int(row["nonevents"]),\n                    "event_rate": float(row["event_rate"]) if pd.notna(row["event_rate"]) else np.nan,\n                    "person_time_days": float(row["person_time_days"]),\n                }\n            )\n    return pd.DataFrame(out_rows)\n\n\ndef _cox_lifelines(\n    df: pd.DataFrame,\n    label: str,\n    notes: list[str],\n    config: dict,\n    model_store: dict[str, object] | None = None,\n) -> tuple[pd.DataFrame, pd.DataFrame]:\n    if CoxPHFitter is None:\n        return pd.DataFrame(), pd.DataFrame()\n\n    cox_df = df.copy()\n    if cox_df.empty:\n        return pd.DataFrame(), pd.DataFrame()\n\n    categorical_cols = ["exposure_group", "sex_simple", "race_simple", "ethnicity_simple"]\n    cox_df = pd.get_dummies(cox_df, columns=categorical_cols, drop_first=True)\n\n    covars = [\n        c\n        for c in [\n            "age_at_index",\n            "bmi",\n            "hba1c_recent",\n            "diabetes_duration_days",\n            "insulin_use_baseline",\n            "microvascular_any_baseline",\n            "baseline_condition_count",\n            "baseline_outpatient_visits",\n            "baseline_spine_imaging",\n            "baseline_back_pain_flag",\n        ]\n        if c in cox_df.columns\n    ]\n\n    covars.extend([c for c in cox_df.columns if c.startswith("exposure_group_")])\n    covars.extend([c for c in cox_df.columns if c.startswith("sex_simple_")])\n    covars.extend([c for c in cox_df.columns if c.startswith("race_simple_")])\n    covars.extend([c for c in cox_df.columns if c.startswith("ethnicity_simple_")])\n\n    model_df = cox_df[["cox_time_days", "cox_event", *covars]].copy()\n    for c in covars:\n        if model_df[c].isna().any():\n            model_df[c] = model_df[c].fillna(model_df[c].median())\n\n    cph = CoxPHFitter(penalizer=float(config.get("cox_penalizer", 0.0)))\n    try:\n        cph.fit(\n            model_df,\n            duration_col="cox_time_days",\n            event_col="cox_event",\n            robust=True,\n        )\n        if model_store is not None:\n            model_store[label] = cph\n        summary = cph.summary.reset_index().rename(columns={"index": "term"})\n        out = pd.DataFrame(\n            {\n                "term": summary["covariate"],\n                "coef": summary["coef"],\n                "std_error": summary.get("se(coef)", np.nan),\n                "hr": summary["exp(coef)"],\n                "ci_low": summary["exp(coef) lower 95%"],\n                "ci_high": summary["exp(coef) upper 95%"],\n                "p_value": summary["p"],\n                "model": label,\n                "effect_type": "HR",\n                "effect_scale": "log",\n            }\n        )\n\n        ph_df = pd.DataFrame()\n        if proportional_hazard_test is not None:\n            ph = proportional_hazard_test(cph, model_df, time_transform="rank")\n            ph_df = ph.summary.reset_index().rename(columns={"index": "term"})\n            ph_df["analysis"] = "ph_assumption_test"\n        return out, ph_df\n    except Exception as exc:  # pragma: no cover\n        notes.append(f"Cox model failed ({label}): {exc}")\n        return pd.DataFrame({"analysis": [label], "error": [str(exc)]}), pd.DataFrame()\n\n\ndef _cox_phreg(\n    df: pd.DataFrame,\n    label: str,\n    notes: list[str],\n    model_store: dict[str, object] | None = None,\n) -> tuple[pd.DataFrame, pd.DataFrame]:\n    if PHReg is None:\n        return pd.DataFrame(), pd.DataFrame()\n\n    cox_df = df.copy()\n    if cox_df.empty:\n        return pd.DataFrame(), pd.DataFrame()\n\n    if "person_id" in cox_df.columns:\n        groups = pd.to_numeric(cox_df["person_id"], errors="coerce").fillna(-1).to_numpy()\n    else:\n        groups = np.arange(len(cox_df))\n\n    cox_df = pd.get_dummies(\n        cox_df,\n        columns=["exposure_group", "sex_simple", "race_simple", "ethnicity_simple"],\n        drop_first=True,\n    )\n    covars = [\n        c\n        for c in [\n            "age_at_index",\n            "bmi",\n            "hba1c_recent",\n            "diabetes_duration_days",\n            "insulin_use_baseline",\n            "microvascular_any_baseline",\n            "baseline_condition_count",\n            "baseline_outpatient_visits",\n            "baseline_spine_imaging",\n            "baseline_back_pain_flag",\n        ]\n        if c in cox_df.columns\n    ]\n    covars.extend([c for c in cox_df.columns if c.startswith("exposure_group_")])\n    covars.extend([c for c in cox_df.columns if c.startswith("sex_simple_")])\n    covars.extend([c for c in cox_df.columns if c.startswith("race_simple_")])\n    covars.extend([c for c in cox_df.columns if c.startswith("ethnicity_simple_")])\n    covars = list(dict.fromkeys(covars))\n    if not covars:\n        return pd.DataFrame(), pd.DataFrame()\n\n    design = cox_df[covars].copy()\n    for col in covars:\n        if design[col].isna().any():\n            design[col] = design[col].fillna(design[col].median())\n        design[col] = pd.to_numeric(design[col], errors="coerce").fillna(0.0).astype(float)\n\n    variance = design.var(axis=0, ddof=0)\n    keep_cols = variance[variance > 0].index.tolist()\n    if not keep_cols:\n        notes.append(f"PHReg Cox model ({label}) has no non-constant covariates after preprocessing.")\n        return pd.DataFrame(), pd.DataFrame()\n    if len(keep_cols) < len(covars):\n        covars = keep_cols\n        design = design[covars]\n\n    endog = pd.to_numeric(cox_df["cox_time_days"], errors="coerce").astype(float).to_numpy()\n    status = pd.to_numeric(cox_df["cox_event"], errors="coerce").fillna(0).astype(int).to_numpy()\n    exog = design.to_numpy(dtype=float)\n\n    try:\n        model = PHReg(endog=endog, exog=exog, status=status, ties="breslow")\n        res = model.fit(groups=groups)\n        if model_store is not None:\n            model_store[label] = res\n        conf = res.conf_int()\n        out = pd.DataFrame(\n            {\n                "term": covars,\n                "coef": res.params,\n                "std_error": getattr(res, "bse", np.repeat(np.nan, len(covars))),\n                "hr": np.exp(res.params),\n                "ci_low": np.exp(conf[:, 0]),\n                "ci_high": np.exp(conf[:, 1]),\n                "p_value": res.pvalues,\n                "model": label,\n                "effect_type": "HR",\n                "effect_scale": "log",\n            }\n        )\n        ph_note = pd.DataFrame(\n            {\n                "analysis": ["ph_assumption_test"],\n                "policy_note": ["PH diagnostics limited in PHReg fallback."],\n            }\n        )\n        return out, ph_note\n    except Exception as exc:  # pragma: no cover\n        notes.append(f"PHReg Cox model failed ({label}): {exc}")\n        return pd.DataFrame({"analysis": [label], "error": [str(exc)]}), pd.DataFrame()\n\n\ndef build_km_curve_data(df: pd.DataFrame, config: dict) -> pd.DataFrame:\n    out_rows: list[dict[str, object]] = []\n    working = _prepare_cox_model_df(df, config)\n    if working.empty:\n        return pd.DataFrame()\n\n    for arm, g in working.groupby("exposure_group", dropna=False, observed=False):\n        if g.empty:\n            continue\n        times = np.sort(g["cox_time_days"].astype(float).unique())\n        n_at_risk = len(g)\n        surv = 1.0\n        for t in times:\n            events = int(((g["cox_time_days"] == t) & (g["cox_event"] == 1)).sum())\n            censored = int(((g["cox_time_days"] == t) & (g["cox_event"] == 0)).sum())\n            if n_at_risk > 0:\n                surv *= (1 - (events / n_at_risk))\n            out_rows.append(\n                {\n                    "exposure_group": str(arm),\n                    "time_days": float(t),\n                    "n_at_risk": int(n_at_risk),\n                    "n_events": int(events),\n                    "n_censored": int(censored),\n                    "survival_probability": float(surv),\n                }\n            )\n            n_at_risk -= events + censored\n            if n_at_risk <= 0:\n                break\n    return pd.DataFrame(out_rows)\n\n\ndef run_cox_models(\n    df: pd.DataFrame,\n    config: dict,\n    threshold: int,\n    notes: list[str],\n    model_store: dict[str, object] | None = None,\n) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    cox_df = _prepare_cox_model_df(df, config)\n    cox_diag = _build_cox_diagnostics(cox_df, "cox_time_to_spine")\n\n    n = int(len(cox_df))\n    events = int(cox_df["cox_event"].sum()) if not cox_df.empty else 0\n    logging.info("cox_time_to_spine: n=%s events=%s", n, events)\n    compliant, note = model_is_policy_compliant(n=n, events=events, threshold=threshold)\n    if not compliant:\n        notes.append(f"cox_time_to_spine: {note}")\n        return _create_policy_note_df("cox_time_to_spine"), pd.DataFrame(), cox_diag, cox_df\n\n    out, ph_df = _cox_lifelines(cox_df, "cox_time_to_spine", notes, config=config, model_store=model_store)\n    if out.empty:\n        out, ph_df = _cox_phreg(cox_df, "cox_time_to_spine", notes, model_store=model_store)\n    if out.empty:\n        notes.append(\n            "Neither lifelines nor PHReg Cox model succeeded; cox_time_to_spine includes only policy/error notes."\n        )\n        out = _create_policy_note_df("cox_time_to_spine")\n    else:\n        out["n_total"] = n\n        out["events"] = events\n        out["person_time_days"] = float(cox_df["cox_time_days"].sum())\n\n    if not ph_df.empty:\n        ph_df = ph_df.rename(columns={"p": "p_value", "test_statistic": "statistic"})\n    return out, ph_df, cox_diag, cox_df\n\n\ndef run_additional_interaction_models(\n    df: pd.DataFrame,\n    config: dict,\n    threshold: int,\n    notes: list[str],\n    model_store: dict[str, object] | None = None,\n    diagnostics_store: list[pd.DataFrame] | None = None,\n) -> pd.DataFrame:\n    rows: list[pd.DataFrame] = []\n\n    require_2y = bool(config["require_730d_for_binary_models"])\n    logistic_df = df.copy()\n    if require_2y:\n        logistic_df = logistic_df.loc[logistic_df["has_min_followup_2y"] == 1].copy()\n    logistic_df = logistic_df.loc[\n        logistic_df["bmi_c"].notna()\n        & logistic_df["age_at_index"].notna()\n        & logistic_df["exposure_group"].notna()\n    ].copy()\n\n    exposure_term = _reference_term("exposure_group", config)\n    shared_terms = _base_logit_terms(config, include_exposure=False)\n    cont_int_formula = "incident_spine_2y ~ " + " + ".join([f"{exposure_term} * bmi_c", *shared_terms])\n    rows.append(\n        _fit_logit_if_allowed(\n            logistic_df,\n            cont_int_formula,\n            label="logistic_interaction_continuous_bmi",\n            event_col="incident_spine_2y",\n            threshold=threshold,\n            notes=notes,\n            config=config,\n            model_store=model_store,\n            diagnostics_store=diagnostics_store,\n        )\n    )\n\n    # Cox interactions.\n    cox_df = _prepare_cox_model_df(df, config)\n    n = int(len(cox_df))\n    events = int(cox_df["cox_event"].sum()) if "cox_event" in cox_df.columns else 0\n    compliant, note = model_is_policy_compliant(n=n, events=events, threshold=threshold)\n    if not compliant:\n        rows.append(_create_policy_note_df("cox_interaction_obesity"))\n        rows.append(_create_policy_note_df("cox_interaction_continuous_bmi"))\n        return pd.concat(rows, ignore_index=True, sort=False)\n\n    if CoxPHFitter is None and PHReg is None:\n        notes.append("Neither lifelines nor PHReg is available; Cox interaction models skipped.")\n        rows.append(_create_policy_note_df("cox_interaction_obesity"))\n        rows.append(_create_policy_note_df("cox_interaction_continuous_bmi"))\n        return pd.concat(rows, ignore_index=True, sort=False)\n\n    def _fit_cox_interaction_model(design_df: pd.DataFrame, covars: list[str], model_label: str) -> pd.DataFrame:\n        local = design_df[["cox_time_days", "cox_event", *covars]].copy()\n        for col in covars:\n            local[col] = pd.to_numeric(local[col], errors="coerce").fillna(0.0).astype(float)\n        variance = local[covars].var(axis=0, ddof=0)\n        covars = variance[variance > 0].index.tolist()\n        if not covars:\n            return pd.DataFrame({"analysis": [model_label], "error": ["No non-constant covariates"]})\n        local = local[["cox_time_days", "cox_event", *covars]]\n\n        if CoxPHFitter is not None:\n            try:\n                cph = CoxPHFitter(penalizer=float(config.get("cox_penalizer", 0.0)))\n                cph.fit(\n                    local,\n                    duration_col="cox_time_days",\n                    event_col="cox_event",\n                    robust=True,\n                )\n                if model_store is not None:\n                    model_store[model_label] = cph\n                summ = cph.summary.reset_index().rename(columns={"index": "term"})\n                return pd.DataFrame(\n                    {\n                        "term": summ["covariate"],\n                        "coef": summ["coef"],\n                        "hr": summ["exp(coef)"],\n                        "ci_low": summ["exp(coef) lower 95%"],\n                        "ci_high": summ["exp(coef) upper 95%"],\n                        "p_value": summ["p"],\n                        "model": model_label,\n                    }\n                )\n            except Exception as exc:  # pragma: no cover\n                notes.append(f"{model_label} lifelines fit failed: {exc}")\n\n        if PHReg is not None:\n            try:\n                if "person_id" in design_df.columns:\n                    groups = pd.to_numeric(design_df["person_id"], errors="coerce").fillna(-1).to_numpy()\n                else:\n                    groups = np.arange(len(design_df))\n                endog = local["cox_time_days"].astype(float).to_numpy()\n                status = local["cox_event"].astype(int).to_numpy()\n                exog = local[covars].to_numpy(dtype=float)\n                ph = PHReg(endog=endog, exog=exog, status=status, ties="breslow")\n                res = ph.fit(groups=groups)\n                if model_store is not None:\n                    model_store[model_label] = res\n                conf = res.conf_int()\n                return pd.DataFrame(\n                    {\n                        "term": covars,\n                        "coef": res.params,\n                        "hr": np.exp(res.params),\n                        "ci_low": np.exp(conf[:, 0]),\n                        "ci_high": np.exp(conf[:, 1]),\n                        "p_value": res.pvalues,\n                        "model": model_label,\n                    }\n                )\n            except Exception as exc:  # pragma: no cover\n                notes.append(f"{model_label} PHReg fit failed: {exc}")\n\n        return pd.DataFrame({"analysis": [model_label], "error": ["Cox interaction model failed"]})\n\n    if cox_df.empty:\n        rows.append(_create_policy_note_df("cox_interaction_obesity"))\n        rows.append(_create_policy_note_df("cox_interaction_continuous_bmi"))\n        return pd.concat(rows, ignore_index=True, sort=False)\n\n    # Fill missing BMI for Cox interaction models and include missingness indicator.\n    cox_df["bmi_missing"] = cox_df["bmi"].isna().astype(int)\n    cox_df["bmi_filled"] = cox_df["bmi"].fillna(cox_df["bmi"].median())\n\n    cox_df = pd.get_dummies(\n        cox_df,\n        columns=["exposure_group", "sex_simple", "race_simple", "ethnicity_simple"],\n        drop_first=True,\n    )\n\n    exposure_cols = [c for c in cox_df.columns if c.startswith("exposure_group_")]\n    base_covars = [\n        c\n        for c in [\n            "age_at_index",\n            "bmi_filled",\n            "bmi_missing",\n            "hba1c_recent",\n            "diabetes_duration_days",\n            "insulin_use_baseline",\n            "microvascular_any_baseline",\n            "baseline_condition_count",\n            "baseline_outpatient_visits",\n            "baseline_spine_imaging",\n        ]\n        if c in cox_df.columns\n    ]\n    base_covars.extend([c for c in cox_df.columns if c.startswith("sex_simple_")])\n    base_covars.extend([c for c in cox_df.columns if c.startswith("race_simple_")])\n    base_covars.extend([c for c in cox_df.columns if c.startswith("ethnicity_simple_")])\n\n    for col in base_covars:\n        if cox_df[col].isna().any():\n            cox_df[col] = cox_df[col].fillna(cox_df[col].median())\n\n    # Obesity interaction.\n    if "obese_bmi30" in cox_df.columns:\n        cox_ob = cox_df.copy()\n        inter_cols = []\n        for col in exposure_cols:\n            inter_col = f"{col}:obese_bmi30"\n            cox_ob[inter_col] = cox_ob[col] * cox_ob["obese_bmi30"]\n            inter_cols.append(inter_col)\n        covars_ob = [*base_covars, *exposure_cols, "obese_bmi30", *inter_cols]\n        covars_ob = [c for c in covars_ob if c in cox_ob.columns]\n        rows.append(_fit_cox_interaction_model(cox_ob, covars_ob, "cox_interaction_obesity"))\n    else:\n        rows.append(_create_policy_note_df("cox_interaction_obesity"))\n\n    # Continuous BMI interaction.\n    cox_cont = cox_df.copy()\n    cont_inter_cols = []\n    for col in exposure_cols:\n        inter_col = f"{col}:bmi_filled"\n        cox_cont[inter_col] = cox_cont[col] * cox_cont["bmi_filled"]\n        cont_inter_cols.append(inter_col)\n    covars_cont = [*base_covars, *exposure_cols, *cont_inter_cols]\n    covars_cont = [c for c in covars_cont if c in cox_cont.columns]\n    rows.append(_fit_cox_interaction_model(cox_cont, covars_cont, "cox_interaction_continuous_bmi"))\n\n    return pd.concat(rows, ignore_index=True, sort=False)\n\n\ndef _weighted_mean(x: pd.Series, w: pd.Series) -> float:\n    return float(np.average(x, weights=w)) if len(x) else np.nan\n\n\ndef _weighted_var(x: pd.Series, w: pd.Series) -> float:\n    mu = _weighted_mean(x, w)\n    return float(np.average((x - mu) ** 2, weights=w)) if len(x) else np.nan\n\n\ndef _smd_numeric(x_t: pd.Series, x_c: pd.Series, wt_t: pd.Series | None = None, wt_c: pd.Series | None = None) -> float:\n    if wt_t is None or wt_c is None:\n        m_t, m_c = x_t.mean(), x_c.mean()\n        v_t, v_c = x_t.var(ddof=1), x_c.var(ddof=1)\n    else:\n        m_t, m_c = _weighted_mean(x_t, wt_t), _weighted_mean(x_c, wt_c)\n        v_t, v_c = _weighted_var(x_t, wt_t), _weighted_var(x_c, wt_c)\n    denom = np.sqrt((v_t + v_c) / 2)\n    if denom == 0 or np.isnan(denom):\n        return np.nan\n    return float((m_t - m_c) / denom)\n\n\ndef _ps_numeric_covariates(ps_df: pd.DataFrame) -> list[str]:\n    preferred = [\n        "age_at_index",\n        "age_sq",\n        "bmi",\n        "bmi_sq",\n        "hba1c_recent",\n        "hba1c_mean_year",\n        "diabetes_duration_days",\n        "duration_sq",\n        "insulin_use_baseline",\n        "microvascular_any_baseline",\n        "baseline_condition_count",\n        "baseline_outpatient_visits",\n        "outpatient_visits_sq",\n        "baseline_spine_imaging",\n        "baseline_back_pain_flag",\n        "duration_x_insulin",\n        "age_x_comorbidity",\n        "sex_female_x_bmi",\n    ]\n    return [c for c in preferred if c in ps_df.columns and ps_df[c].notna().any()]\n\n\ndef _ps_categorical_covariates(ps_df: pd.DataFrame) -> list[str]:\n    return [c for c in ["sex_simple", "race_simple", "ethnicity_simple"] if c in ps_df.columns]\n\n\ndef _build_ps_formula(ps_df: pd.DataFrame, config: dict) -> tuple[str, list[str]]:\n    linear_terms: list[str] = []\n    for col in [\n        "age_at_index",\n        "bmi",\n        "hba1c_recent",\n        "hba1c_mean_year",\n        "diabetes_duration_days",\n        "insulin_use_baseline",\n        "microvascular_any_baseline",\n        "baseline_condition_count",\n        "baseline_outpatient_visits",\n        "baseline_spine_imaging",\n        "baseline_back_pain_flag",\n    ]:\n        if col in ps_df.columns and ps_df[col].notna().any():\n            linear_terms.append(col)\n    nonlinear_terms: list[str] = []\n    if "age_at_index" in linear_terms:\n        nonlinear_terms.append("I(age_at_index ** 2)")\n    if "bmi" in linear_terms:\n        nonlinear_terms.append("I(bmi ** 2)")\n    if "diabetes_duration_days" in linear_terms:\n        nonlinear_terms.append("I(diabetes_duration_days ** 2)")\n    if "baseline_outpatient_visits" in linear_terms:\n        nonlinear_terms.append("I(baseline_outpatient_visits ** 2)")\n\n    interactions: list[str] = []\n    if "sex_simple" in ps_df.columns and "bmi" in linear_terms:\n        interactions.append(f"{_reference_term(\'sex_simple\', config)}:bmi")\n    if "diabetes_duration_days" in linear_terms and "insulin_use_baseline" in linear_terms:\n        interactions.append("diabetes_duration_days:insulin_use_baseline")\n    if "age_at_index" in linear_terms and "baseline_condition_count" in linear_terms:\n        interactions.append("age_at_index:baseline_condition_count")\n\n    cat_terms = [_reference_term(col, config) for col in _ps_categorical_covariates(ps_df)]\n    rhs_terms = list(dict.fromkeys([*linear_terms, *nonlinear_terms, *cat_terms, *interactions]))\n    formula = "treat_glp1 ~ " + (" + ".join(rhs_terms) if rhs_terms else "1")\n    return formula, rhs_terms\n\n\ndef _refresh_imputed_ps_df(ps_df: pd.DataFrame, config: dict) -> pd.DataFrame:\n    out = ps_df.copy()\n    if "bmi" in out.columns:\n        lo, hi = float(config.get("bmi_min", 10.0)), float(config.get("bmi_max", 80.0))\n        out["bmi"] = pd.to_numeric(out["bmi"], errors="coerce").clip(lo, hi)\n        bmi_winsor = config.get("bmi_winsor_quantiles", (0.005, 0.995))\n        out["bmi"] = _winsorize_series(out["bmi"], float(bmi_winsor[0]), float(bmi_winsor[1]))\n        out["obese_bmi30"] = (out["bmi"] >= 30).astype(int)\n        out["bmi_c"] = out["bmi"] - float(config.get("bmi_center_value", 30.0))\n        out["bmi_sq"] = out["bmi"] ** 2\n    for col in ["hba1c_recent", "hba1c_mean_year"]:\n        if col in out.columns:\n            out[col] = pd.to_numeric(out[col], errors="coerce")\n            out[col] = out[col].clip(\n                float(config.get("hba1c_plausible_min", 3.0)),\n                float(config.get("hba1c_plausible_max", 20.0)),\n            )\n            hq = config.get("hba1c_winsor_quantiles", (0.005, 0.995))\n            out[col] = _winsorize_series(out[col], float(hq[0]), float(hq[1]))\n    if "diabetes_duration_days" in out.columns:\n        out["diabetes_duration_days"] = pd.to_numeric(out["diabetes_duration_days"], errors="coerce").clip(lower=0)\n        out["duration_sq"] = out["diabetes_duration_days"] ** 2\n    if "age_at_index" in out.columns:\n        out["age_sq"] = pd.to_numeric(out["age_at_index"], errors="coerce") ** 2\n    if "baseline_outpatient_visits" in out.columns:\n        out["outpatient_visits_sq"] = pd.to_numeric(out["baseline_outpatient_visits"], errors="coerce") ** 2\n    if "diabetes_duration_days" in out.columns and "insulin_use_baseline" in out.columns:\n        out["duration_x_insulin"] = out["diabetes_duration_days"] * pd.to_numeric(\n            out["insulin_use_baseline"], errors="coerce"\n        ).fillna(0)\n    if "age_at_index" in out.columns and "baseline_condition_count" in out.columns:\n        out["age_x_comorbidity"] = out["age_at_index"] * pd.to_numeric(\n            out["baseline_condition_count"], errors="coerce"\n        ).fillna(0)\n    if "sex_simple" in out.columns:\n        out["sex_female_flag"] = (out["sex_simple"].astype(str) == "Female").astype(int)\n    if "sex_female_flag" in out.columns and "bmi" in out.columns:\n        out["sex_female_x_bmi"] = out["sex_female_flag"] * out["bmi"]\n    if "microvascular_any_baseline" not in out.columns:\n        micro_cols = [c for c in ["neuropathy_baseline", "nephropathy_baseline", "retinopathy_baseline"] if c in out.columns]\n        if micro_cols:\n            out["microvascular_any_baseline"] = (out[micro_cols].fillna(0).sum(axis=1) > 0).astype(int)\n        else:\n            out["microvascular_any_baseline"] = 0\n    return out\n\n\ndef _build_imputed_ps_datasets(ps_df: pd.DataFrame, config: dict, notes: list[str]) -> list[pd.DataFrame]:\n    target_cols = [c for c in config.get("mi_target_columns", ["bmi", "hba1c_recent", "hba1c_mean_year"]) if c in ps_df.columns]\n    missing_targets = [c for c in target_cols if ps_df[c].isna().any()]\n    if not missing_targets:\n        return [_refresh_imputed_ps_df(ps_df, config)]\n\n    m = max(1, int(config.get("mi_num_imputations", 5)))\n    max_iter = max(5, int(config.get("mi_max_iter", 20)))\n    seed = int(config.get("random_seed", 42))\n    base = ps_df.copy()\n\n    predictor_numeric = [\n        c\n        for c in [\n            *target_cols,\n            "treat_glp1",\n            "incident_spine_2y",\n            "event_full_followup",\n            "time_to_event_or_censor_days",\n            "age_at_index",\n            "baseline_condition_count",\n            "baseline_outpatient_visits",\n            "baseline_spine_imaging",\n            "baseline_back_pain_flag",\n            "insulin_use_baseline",\n            "microvascular_any_baseline",\n        ]\n        if c in base.columns\n    ]\n    cat_cols = [c for c in ["exposure_group", "sex_simple", "race_simple", "ethnicity_simple"] if c in base.columns]\n\n    design = pd.DataFrame(index=base.index)\n    for col in predictor_numeric:\n        design[col] = pd.to_numeric(base[col], errors="coerce")\n    if cat_cols:\n        dummies = pd.get_dummies(base[cat_cols].astype("category"), prefix=cat_cols, dummy_na=True)\n        design = pd.concat([design, dummies], axis=1)\n    for col in design.columns:\n        if design[col].notna().sum() == 0:\n            design[col] = 0.0\n        else:\n            design[col] = _winsorize_series(design[col], 0.001, 0.999)\n\n    if IterativeImputer is None:\n        notes.append("IterativeImputer unavailable; using single stochastic imputation fallback.")\n        out = base.copy()\n        rng = np.random.default_rng(seed)\n        strata_cols = [c for c in ["exposure_group", "sex_simple"] if c in out.columns]\n        for col in missing_targets:\n            series = pd.to_numeric(out[col], errors="coerce")\n            if strata_cols:\n                by_mean = out.groupby(strata_cols, observed=False)[col].transform("mean")\n                by_std = out.groupby(strata_cols, observed=False)[col].transform("std")\n                mu = by_mean.fillna(series.mean())\n                sigma = by_std.fillna(series.std(ddof=1))\n            else:\n                mu = pd.Series(series.mean(), index=out.index)\n                sigma = pd.Series(series.std(ddof=1), index=out.index)\n            sigma = sigma.fillna(0.0).clip(lower=0.0)\n            draws = rng.normal(mu.to_numpy(), sigma.to_numpy())\n            miss = series.isna()\n            series.loc[miss] = draws[miss]\n            out[col] = series.fillna(series.median())\n        return [_refresh_imputed_ps_df(out, config)]\n\n    outputs: list[pd.DataFrame] = []\n    for i in range(m):\n        imputer = IterativeImputer(\n            random_state=seed + i,\n            sample_posterior=True,\n            max_iter=max_iter,\n            initial_strategy="median",\n        )\n        with warnings.catch_warnings():\n            warnings.filterwarnings("ignore", category=RuntimeWarning)\n            warnings.filterwarnings("ignore", category=UserWarning)\n            imputed_matrix = imputer.fit_transform(design)\n        imputed_df = pd.DataFrame(imputed_matrix, index=design.index, columns=design.columns)\n        out = base.copy()\n        for col in missing_targets:\n            if col in imputed_df.columns:\n                out[col] = pd.to_numeric(imputed_df[col], errors="coerce")\n        outputs.append(_refresh_imputed_ps_df(out, config))\n\n    notes.append(f"Multiple imputation used {len(outputs)} datasets for: {\', \'.join(missing_targets)}.")\n    return outputs\n\n\ndef _compute_missingness_weights(ps_df: pd.DataFrame, config: dict, notes: list[str]) -> pd.Series:\n    weight = pd.Series(1.0, index=ps_df.index, dtype=float, name="missingness_weight")\n    if "bmi" not in ps_df.columns:\n        return weight\n\n    model_df = ps_df.copy()\n    model_df["labs_observed"] = model_df["bmi"].notna().astype(int)\n    if "hba1c_recent" in model_df.columns:\n        model_df["labs_observed"] = (\n            model_df["bmi"].notna() & model_df["hba1c_recent"].notna()\n        ).astype(int)\n\n    for col in [\n        "age_at_index",\n        "baseline_condition_count",\n        "baseline_outpatient_visits",\n        "baseline_spine_imaging",\n        "insulin_use_baseline",\n        "microvascular_any_baseline",\n    ]:\n        if col in model_df.columns:\n            model_df[col] = pd.to_numeric(model_df[col], errors="coerce").fillna(model_df[col].median())\n\n    rhs_terms = [\n        c\n        for c in [\n            "age_at_index",\n            "baseline_condition_count",\n            "baseline_outpatient_visits",\n            "baseline_spine_imaging",\n            "insulin_use_baseline",\n            "microvascular_any_baseline",\n        ]\n        if c in model_df.columns\n    ]\n    for cat in _ps_categorical_covariates(model_df):\n        rhs_terms.append(_reference_term(cat, config))\n\n    formula = "labs_observed ~ " + (" + ".join(rhs_terms) if rhs_terms else "1")\n    try:\n        fit = smf.glm(formula=formula, data=model_df, family=sm.families.Binomial()).fit()\n        p_obs = fit.predict(model_df).clip(1e-4, 1 - 1e-4)\n        p_marg = float(model_df["labs_observed"].mean())\n        raw = np.where(\n            model_df["labs_observed"] == 1,\n            p_marg / p_obs,\n            (1.0 - p_marg) / (1.0 - p_obs),\n        )\n        raw_series = pd.Series(raw, index=model_df.index, dtype=float)\n        low_q, high_q = config.get("missingness_weight_truncation_quantiles", (0.01, 0.99))\n        lo, hi = raw_series.quantile([float(low_q), float(high_q)]).tolist()\n        weight = raw_series.clip(lo, hi)\n    except Exception as exc:  # pragma: no cover\n        notes.append(f"Missingness IPW model failed; using unit weights ({exc}).")\n\n    return weight\n\n\ndef _compute_treatment_weights(\n    treat: pd.Series,\n    ps: pd.Series,\n    *,\n    strategy: str,\n    stabilized: bool,\n) -> pd.Series:\n    t = pd.to_numeric(treat, errors="coerce").fillna(0).astype(int)\n    e = pd.to_numeric(ps, errors="coerce").clip(1e-4, 1 - 1e-4)\n    p_treated = float(t.mean())\n\n    if strategy == "overlap":\n        w = np.where(t == 1, 1 - e, e)\n    elif strategy == "att":\n        w = np.where(t == 1, 1.0, e / (1 - e))\n    else:  # stabilized IPTW\n        if stabilized:\n            w = np.where(t == 1, p_treated / e, (1 - p_treated) / (1 - e))\n        else:\n            w = np.where(t == 1, 1.0 / e, 1.0 / (1 - e))\n    return pd.Series(w, index=treat.index, dtype=float)\n\n\ndef _truncate_weights(weights: pd.Series, lower_q: float, upper_q: float) -> pd.Series:\n    w = pd.to_numeric(weights, errors="coerce").fillna(0.0)\n    lo, hi = w.quantile([lower_q, upper_q]).tolist()\n    return w.clip(lo, hi)\n\n\ndef _compute_balance_table(\n    ps_df: pd.DataFrame,\n    *,\n    weight_col: str,\n    weighting_label: str,\n) -> pd.DataFrame:\n    treated = ps_df.loc[ps_df["treat_glp1"] == 1].copy()\n    control = ps_df.loc[ps_df["treat_glp1"] == 0].copy()\n    if treated.empty or control.empty:\n        return pd.DataFrame()\n\n    rows: list[dict[str, object]] = []\n    numeric_covars = _ps_numeric_covariates(ps_df)\n    for col in numeric_covars:\n        t_series = pd.to_numeric(treated[col], errors="coerce")\n        c_series = pd.to_numeric(control[col], errors="coerce")\n        if t_series.notna().sum() == 0 or c_series.notna().sum() == 0:\n            continue\n        smd_before = _smd_numeric(t_series.fillna(t_series.median()), c_series.fillna(c_series.median()))\n        smd_after = _smd_numeric(\n            t_series.fillna(t_series.median()),\n            c_series.fillna(c_series.median()),\n            wt_t=treated[weight_col],\n            wt_c=control[weight_col],\n        )\n        rows.append(\n            {\n                "covariate": col,\n                "smd_unweighted": smd_before,\n                "smd_weighted": smd_after,\n                "abs_smd_unweighted": abs(smd_before) if pd.notna(smd_before) else np.nan,\n                "abs_smd_weighted": abs(smd_after) if pd.notna(smd_after) else np.nan,\n                "n_treated": int(len(treated)),\n                "n_control": int(len(control)),\n                "weighting_label": weighting_label,\n            }\n        )\n\n    for cat_col in _ps_categorical_covariates(ps_df):\n        dummies = pd.get_dummies(ps_df[cat_col], prefix=cat_col, dummy_na=False)\n        for dcol in dummies.columns:\n            t_series = dummies.loc[ps_df["treat_glp1"] == 1, dcol]\n            c_series = dummies.loc[ps_df["treat_glp1"] == 0, dcol]\n            smd_before = _smd_numeric(t_series, c_series)\n            smd_after = _smd_numeric(\n                t_series,\n                c_series,\n                wt_t=treated[weight_col],\n                wt_c=control[weight_col],\n            )\n            rows.append(\n                {\n                    "covariate": dcol,\n                    "smd_unweighted": smd_before,\n                    "smd_weighted": smd_after,\n                    "abs_smd_unweighted": abs(smd_before) if pd.notna(smd_before) else np.nan,\n                    "abs_smd_weighted": abs(smd_after) if pd.notna(smd_after) else np.nan,\n                    "n_treated": int(len(treated)),\n                    "n_control": int(len(control)),\n                    "weighting_label": weighting_label,\n                }\n            )\n    return pd.DataFrame(rows)\n\n\ndef _weight_diagnostics_rows(\n    weights: pd.Series,\n    *,\n    label: str,\n    n_total: int,\n    treat_n: int,\n    control_n: int,\n) -> pd.DataFrame:\n    w = pd.to_numeric(weights, errors="coerce").fillna(0.0)\n    rows = [\n        {"model": "weight_diagnostics", "term": "mean_weight", "value": float(w.mean())},\n        {"model": "weight_diagnostics", "term": "max_weight", "value": float(w.max())},\n        {"model": "weight_diagnostics", "term": "p99_weight", "value": float(w.quantile(0.99))},\n        {"model": "weight_diagnostics", "term": "p95_weight", "value": float(w.quantile(0.95))},\n        {"model": "weight_diagnostics", "term": "min_weight", "value": float(w.min())},\n        {"model": "weight_diagnostics", "term": "ess", "value": _effective_sample_size(w)},\n    ]\n    out = pd.DataFrame(rows)\n    out["weighting_label"] = label\n    out["n_total"] = int(n_total)\n    out["treated_n"] = int(treat_n)\n    out["control_n"] = int(control_n)\n    return out\n\n\ndef _fit_aipw_binary(\n    ps_df: pd.DataFrame,\n    *,\n    ps_col: str,\n    config: dict,\n    notes: list[str],\n) -> pd.DataFrame:\n    rhs = []\n    for col in [\n        "age_at_index",\n        "bmi",\n        "hba1c_recent",\n        "hba1c_mean_year",\n        "diabetes_duration_days",\n        "insulin_use_baseline",\n        "microvascular_any_baseline",\n        "baseline_condition_count",\n        "baseline_outpatient_visits",\n        "baseline_spine_imaging",\n        "baseline_back_pain_flag",\n    ]:\n        if col in ps_df.columns:\n            rhs.append(col)\n    for cat in _ps_categorical_covariates(ps_df):\n        rhs.append(_reference_term(cat, config))\n    formula = "incident_spine_2y ~ treat_glp1" + ((" + " + " + ".join(rhs)) if rhs else "")\n\n    model_df = ps_df.copy()\n    for col in _ps_numeric_covariates(model_df):\n        model_df[col] = pd.to_numeric(model_df[col], errors="coerce").fillna(model_df[col].median())\n    try:\n        out_fit = smf.glm(formula=formula, data=model_df, family=sm.families.Binomial()).fit()\n        m1_df = model_df.copy()\n        m0_df = model_df.copy()\n        m1_df["treat_glp1"] = 1\n        m0_df["treat_glp1"] = 0\n        mu1 = out_fit.predict(m1_df)\n        mu0 = out_fit.predict(m0_df)\n        y = pd.to_numeric(model_df["incident_spine_2y"], errors="coerce").fillna(0).astype(float)\n        t = pd.to_numeric(model_df["treat_glp1"], errors="coerce").fillna(0).astype(int)\n        e = pd.to_numeric(model_df[ps_col], errors="coerce").clip(1e-4, 1 - 1e-4)\n        influence = mu1 - mu0 + (t * (y - mu1) / e) - ((1 - t) * (y - mu0) / (1 - e))\n        psi = float(np.mean(influence))\n        se = float(np.std(influence, ddof=1) / np.sqrt(len(influence))) if len(influence) > 1 else np.nan\n        z = psi / se if se and np.isfinite(se) and se > 0 else np.nan\n        ci_low = psi - 1.96 * se if np.isfinite(se) else np.nan\n        ci_high = psi + 1.96 * se if np.isfinite(se) else np.nan\n        return pd.DataFrame(\n            {\n                "term": ["treat_glp1"],\n                "coef": [psi],\n                "std_error": [se],\n                "estimate": [psi],\n                "ci_low": [ci_low],\n                "ci_high": [ci_high],\n                "p_value": [_two_sided_p_from_z(z)],\n                "model": ["mi_aipw_ate_risk_difference"],\n                "effect_type": ["RD"],\n                "effect_scale": ["identity"],\n            }\n        )\n    except Exception as exc:  # pragma: no cover\n        notes.append(f"AIPW fit failed: {exc}")\n        return pd.DataFrame({"analysis": ["mi_aipw_ate_risk_difference"], "error": [str(exc)]})\n\n\ndef _pool_imputation_effects(effect_rows: pd.DataFrame) -> pd.DataFrame:\n    if effect_rows.empty:\n        return pd.DataFrame()\n\n    needed = {"model", "term", "coef"}\n    if not needed.issubset(effect_rows.columns):\n        return effect_rows\n\n    rows: list[dict[str, object]] = []\n    group_cols = ["model", "term", "effect_type", "effect_scale"]\n    for keys, g in effect_rows.groupby(group_cols, dropna=False):\n        model, term, effect_type, effect_scale = keys\n        g = g.loc[pd.to_numeric(g["coef"], errors="coerce").notna()].copy()\n        if g.empty:\n            continue\n        m = int(len(g))\n        coef = pd.to_numeric(g["coef"], errors="coerce")\n        qbar = float(coef.mean())\n        se_series = (\n            pd.to_numeric(g["std_error"], errors="coerce")\n            if "std_error" in g.columns\n            else pd.Series(np.nan, index=g.index)\n        )\n        if se_series.notna().any():\n            ubar = float(np.nanmean(np.square(se_series)))\n            b = float(np.nanvar(coef, ddof=1)) if m > 1 else 0.0\n            tvar = ubar + ((1.0 + (1.0 / m)) * b)\n            se_pool = float(np.sqrt(max(tvar, 0.0)))\n        else:\n            se_pool = np.nan\n\n        if np.isfinite(se_pool) and se_pool > 0:\n            z = qbar / se_pool\n            ci_low_coef = qbar - 1.96 * se_pool\n            ci_high_coef = qbar + 1.96 * se_pool\n            p_value = _two_sided_p_from_z(z)\n        else:\n            ci_low_coef = np.nan\n            ci_high_coef = np.nan\n            p_value = np.nan\n\n        row: dict[str, object] = {\n            "model": model,\n            "term": term,\n            "coef": qbar,\n            "std_error": se_pool,\n            "p_value": p_value,\n            "effect_type": effect_type,\n            "effect_scale": effect_scale,\n            "imputations": m,\n            "n_total": float(pd.to_numeric(g.get("n_total", np.nan), errors="coerce").mean()),\n            "events": float(pd.to_numeric(g.get("events", np.nan), errors="coerce").mean()),\n        }\n        if effect_scale == "log":\n            row["ci_low"] = float(np.exp(ci_low_coef)) if np.isfinite(ci_low_coef) else np.nan\n            row["ci_high"] = float(np.exp(ci_high_coef)) if np.isfinite(ci_high_coef) else np.nan\n            if effect_type == "OR":\n                row["or"] = float(np.exp(qbar))\n            elif effect_type == "HR":\n                row["hr"] = float(np.exp(qbar))\n            row["estimate"] = float(np.exp(qbar))\n        else:\n            row["ci_low"] = ci_low_coef\n            row["ci_high"] = ci_high_coef\n            row["estimate"] = qbar\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\ndef _pool_balance_tables(balance_frames: list[pd.DataFrame], target_abs_smd: float) -> pd.DataFrame:\n    if not balance_frames:\n        return pd.DataFrame()\n    bal = pd.concat(balance_frames, ignore_index=True, sort=False)\n    if bal.empty:\n        return bal\n    pooled = (\n        bal.groupby(["covariate", "weighting_label"], dropna=False)\n        .agg(\n            smd_unweighted=("smd_unweighted", "mean"),\n            smd_weighted=("smd_weighted", "mean"),\n            abs_smd_unweighted=("abs_smd_unweighted", "mean"),\n            abs_smd_weighted=("abs_smd_weighted", "mean"),\n            n_treated=("n_treated", "mean"),\n            n_control=("n_control", "mean"),\n        )\n        .reset_index()\n    )\n    summary_rows: list[dict[str, object]] = []\n    for label, g in pooled.groupby("weighting_label", dropna=False):\n        max_abs = float(pd.to_numeric(g["abs_smd_weighted"], errors="coerce").max())\n        summary_rows.append(\n            {\n                "covariate": "__summary_max_abs_smd__",\n                "weighting_label": label,\n                "smd_unweighted": np.nan,\n                "smd_weighted": np.nan,\n                "abs_smd_unweighted": np.nan,\n                "abs_smd_weighted": max_abs,\n                "n_treated": float(pd.to_numeric(g["n_treated"], errors="coerce").mean()),\n                "n_control": float(pd.to_numeric(g["n_control"], errors="coerce").mean()),\n                "balance_target_abs_smd": target_abs_smd,\n                "balance_pass": bool(max_abs < target_abs_smd) if np.isfinite(max_abs) else False,\n            }\n        )\n    if summary_rows:\n        pooled = pd.concat([pooled, pd.DataFrame(summary_rows)], ignore_index=True, sort=False)\n    return pooled\n\n\ndef build_ps_and_outcomes(\n    df: pd.DataFrame,\n    config: dict,\n    threshold: int,\n    notes: list[str],\n    model_store: dict[str, object] | None = None,\n) -> tuple[pd.DataFrame, pd.DataFrame]:\n    control_label = str(config.get("reference_levels", {}).get("exposure_group", "metformin_only"))\n    treated_label = "glp1_only"\n    contrast_label = f"{treated_label}_vs_{control_label}"\n    target_abs_smd = float(config.get("ps_balance_target_abs_smd", 0.10))\n\n    ps_df = df.loc[df["exposure_group"].isin([control_label, treated_label])].copy()\n    if bool(config["require_730d_for_binary_models"]):\n        ps_df = ps_df.loc[ps_df["has_min_followup_2y"] == 1].copy()\n    if ps_df.empty:\n        return _create_policy_note_df("propensity_score_results"), _create_policy_note_df("balance_diagnostics")\n\n    ps_df["treat_glp1"] = (ps_df["exposure_group"] == treated_label).astype(int)\n    needed = [\n        "incident_spine_2y",\n        "time_to_event_or_censor_days",\n        "event_full_followup",\n        "treat_glp1",\n        "exposure_group",\n        *set(_ps_numeric_covariates(ps_df)),\n        *set(_ps_categorical_covariates(ps_df)),\n    ]\n    needed = [c for c in dict.fromkeys(needed) if c in ps_df.columns]\n    ps_df = ps_df[needed].copy()\n\n    for cat in _ps_categorical_covariates(ps_df) + (["exposure_group"] if "exposure_group" in ps_df.columns else []):\n        ps_df[cat] = ps_df[cat].astype("category")\n\n    n = int(len(ps_df))\n    events = int(pd.to_numeric(ps_df.get("incident_spine_2y", 0), errors="coerce").fillna(0).sum())\n    compliant, note = model_is_policy_compliant(n, events, threshold)\n    if not compliant:\n        notes.append(f"propensity_score_results: {note}")\n        return _create_policy_note_df("propensity_score_results"), _create_policy_note_df("balance_diagnostics")\n\n    missingness_weight = _compute_missingness_weights(ps_df, config, notes=notes)\n    imputed_sets = _build_imputed_ps_datasets(ps_df, config, notes=notes)\n\n    effect_rows: list[pd.DataFrame] = []\n    diagnostics_rows: list[pd.DataFrame] = []\n    balance_frames: list[pd.DataFrame] = []\n\n    strategies = [str(x) for x in config.get("ps_weighting_strategies", ["overlap", "iptw", "att"])]\n    truncation_options = config.get("ps_weight_truncation_options")\n    if not truncation_options:\n        truncation_options = [config.get("ps_weight_truncation_quantiles", (0.01, 0.99))]\n    truncation_options = [(float(x[0]), float(x[1])) for x in truncation_options]\n\n    for imp_idx, imp_df in enumerate(imputed_sets, start=1):\n        work = imp_df.copy()\n        for col in _ps_numeric_covariates(work) + [\n            "incident_spine_2y",\n            "event_full_followup",\n            "time_to_event_or_censor_days",\n            "treat_glp1",\n        ]:\n            if col in work.columns:\n                work[col] = pd.to_numeric(work[col], errors="coerce")\n                if work[col].isna().any():\n                    work[col] = work[col].fillna(work[col].median())\n\n        ps_candidates: dict[str, pd.Series] = {}\n        formula, _ = _build_ps_formula(work, config)\n        try:\n            ps_fit = smf.glm(formula=formula, data=work, family=sm.families.Binomial()).fit()\n            if model_store is not None:\n                model_store[f"propensity_model_{contrast_label}_imp{imp_idx}_glm"] = ps_fit\n            ps_tab = _glm_or_table(ps_fit, f"propensity_model_{contrast_label}_glm")\n            ps_tab["imputation_id"] = imp_idx\n            ps_tab["n_total"] = len(work)\n            ps_tab["treated_n"] = int(work["treat_glp1"].sum())\n            ps_tab["control_n"] = int((1 - work["treat_glp1"]).sum())\n            effect_rows.append(ps_tab)\n            ps_candidates["formula_glm"] = pd.Series(ps_fit.predict(work), index=work.index)\n        except Exception as exc_glm:  # pragma: no cover\n            notes.append(f"Imputation {imp_idx}: PS GLM failed ({exc_glm}); trying logit.")\n            try:\n                ps_fit = smf.logit(formula=formula, data=work).fit(disp=False)\n                if model_store is not None:\n                    model_store[f"propensity_model_{contrast_label}_imp{imp_idx}_logit"] = ps_fit\n                ps_tab = _logit_or_table(ps_fit, f"propensity_model_{contrast_label}_logit")\n                ps_tab["imputation_id"] = imp_idx\n                ps_tab["n_total"] = len(work)\n                ps_tab["treated_n"] = int(work["treat_glp1"].sum())\n                ps_tab["control_n"] = int((1 - work["treat_glp1"]).sum())\n                effect_rows.append(ps_tab)\n                ps_candidates["formula_logit"] = pd.Series(ps_fit.predict(work), index=work.index)\n            except Exception as exc_logit:  # pragma: no cover\n                notes.append(f"Imputation {imp_idx}: PS logit failed ({exc_logit}).")\n\n        if bool(config.get("ps_use_ml_if_available", False)) and GradientBoostingClassifier is not None:\n            try:\n                ml_covars = [c for c in _ps_numeric_covariates(work) if c in work.columns]\n                x = pd.get_dummies(work[[*ml_covars, *_ps_categorical_covariates(work)]], dummy_na=True)\n                y = work["treat_glp1"].astype(int)\n                gbt = GradientBoostingClassifier(\n                    n_estimators=int(config.get("ps_ml_n_estimators", 300)),\n                    learning_rate=float(config.get("ps_ml_learning_rate", 0.05)),\n                    subsample=float(config.get("ps_ml_subsample", 0.8)),\n                    random_state=int(config.get("random_seed", 42)) + imp_idx,\n                )\n                gbt.fit(x, y)\n                ps_ml = pd.Series(gbt.predict_proba(x)[:, 1], index=work.index)\n                ps_candidates["gradient_boosting"] = ps_ml\n            except Exception as exc_ml:  # pragma: no cover\n                notes.append(f"Imputation {imp_idx}: ML PS candidate skipped ({exc_ml}).")\n\n        if not ps_candidates:\n            continue\n\n        best: dict[str, object] | None = None\n        min_ess_ratio = float(config.get("ps_min_ess_ratio", 0.30))\n        min_ess = float(len(work)) * min_ess_ratio\n        for ps_name, ps_pred in ps_candidates.items():\n            ps_clean = pd.to_numeric(ps_pred, errors="coerce").clip(1e-4, 1 - 1e-4)\n            for strategy in strategies:\n                raw_w = _compute_treatment_weights(\n                    work["treat_glp1"],\n                    ps_clean,\n                    strategy=strategy,\n                    stabilized=bool(config.get("ps_stabilized_weights", True)),\n                )\n                for q_lo, q_hi in truncation_options:\n                    w_treat = _truncate_weights(raw_w, q_lo, q_hi)\n                    label = f"{ps_name}|{strategy}|q{q_lo:.3f}-{q_hi:.3f}"\n                    trial = work.copy()\n                    trial["w_treat"] = w_treat\n                    bal = _compute_balance_table(trial, weight_col="w_treat", weighting_label=label)\n                    if bal.empty:\n                        continue\n                    max_abs = float(pd.to_numeric(bal["abs_smd_weighted"], errors="coerce").max())\n                    ess = _effective_sample_size(w_treat)\n                    if not np.isfinite(ess) or ess < min_ess:\n                        continue\n                    if best is None or (\n                        max_abs < float(best["max_abs_smd"])\n                        or (\n                            np.isclose(max_abs, float(best["max_abs_smd"]))\n                            and ess > float(best["ess"])\n                        )\n                    ):\n                        best = {\n                            "ps_name": ps_name,\n                            "strategy": strategy,\n                            "label": label,\n                            "ps": ps_clean,\n                            "w_treat": w_treat,\n                            "max_abs_smd": max_abs,\n                            "ess": ess,\n                            "balance": bal,\n                        }\n\n        if best is None:\n            notes.append(\n                f"Imputation {imp_idx}: no PS candidate met ESS floor ({min_ess_ratio:.2f}); relaxing to best SMD."\n            )\n            for ps_name, ps_pred in ps_candidates.items():\n                ps_clean = pd.to_numeric(ps_pred, errors="coerce").clip(1e-4, 1 - 1e-4)\n                for strategy in strategies:\n                    raw_w = _compute_treatment_weights(\n                        work["treat_glp1"],\n                        ps_clean,\n                        strategy=strategy,\n                        stabilized=bool(config.get("ps_stabilized_weights", True)),\n                    )\n                    for q_lo, q_hi in truncation_options:\n                        w_treat = _truncate_weights(raw_w, q_lo, q_hi)\n                        label = f"{ps_name}|{strategy}|q{q_lo:.3f}-{q_hi:.3f}"\n                        trial = work.copy()\n                        trial["w_treat"] = w_treat\n                        bal = _compute_balance_table(trial, weight_col="w_treat", weighting_label=label)\n                        if bal.empty:\n                            continue\n                        max_abs = float(pd.to_numeric(bal["abs_smd_weighted"], errors="coerce").max())\n                        ess = _effective_sample_size(w_treat)\n                        if best is None or max_abs < float(best["max_abs_smd"]):\n                            best = {\n                                "ps_name": ps_name,\n                                "strategy": strategy,\n                                "label": label,\n                                "ps": ps_clean,\n                                "w_treat": w_treat,\n                                "max_abs_smd": max_abs,\n                                "ess": ess,\n                                "balance": bal,\n                            }\n        if best is None:\n            continue\n\n        work["ps_selected"] = pd.to_numeric(best["ps"], errors="coerce").clip(1e-4, 1 - 1e-4)\n        work["w_treat"] = pd.to_numeric(best["w_treat"], errors="coerce").fillna(0.0)\n        work["w_missing"] = pd.to_numeric(missingness_weight.reindex(work.index), errors="coerce").fillna(1.0)\n        work["w_double"] = work["w_treat"] * work["w_missing"]\n\n        if model_store is not None:\n            model_store[f"ps_selected_weights_imp{imp_idx}"] = {\n                "ps_source": best["ps_name"],\n                "strategy": best["strategy"],\n                "label": best["label"],\n                "max_abs_smd": float(best["max_abs_smd"]),\n                "ess": float(best["ess"]),\n            }\n\n        bal_primary = _compute_balance_table(work, weight_col="w_treat", weighting_label=f"{best[\'label\']}|primary")\n        bal_primary["imputation_id"] = imp_idx\n        balance_frames.append(bal_primary)\n        bal_double = _compute_balance_table(work, weight_col="w_double", weighting_label=f"{best[\'label\']}|double")\n        bal_double["imputation_id"] = imp_idx\n        balance_frames.append(bal_double)\n\n        diagnostics_rows.append(\n            _weight_diagnostics_rows(\n                work["w_treat"],\n                label=f"{best[\'label\']}|primary",\n                n_total=len(work),\n                treat_n=int(work["treat_glp1"].sum()),\n                control_n=int((1 - work["treat_glp1"]).sum()),\n            ).assign(imputation_id=imp_idx)\n        )\n        diagnostics_rows.append(\n            _weight_diagnostics_rows(\n                work["w_double"],\n                label=f"{best[\'label\']}|double",\n                n_total=len(work),\n                treat_n=int(work["treat_glp1"].sum()),\n                control_n=int((1 - work["treat_glp1"]).sum()),\n            ).assign(imputation_id=imp_idx)\n        )\n\n        # Primary weighted logistic model.\n        try:\n            out_fit = smf.glm(\n                "incident_spine_2y ~ treat_glp1",\n                data=work,\n                family=sm.families.Binomial(),\n                freq_weights=work["w_treat"],\n            ).fit(cov_type="HC3")\n            if model_store is not None:\n                model_store[f"mi_weighted_logistic_primary_imp{imp_idx}"] = out_fit\n            out_tab = _glm_or_table(out_fit, "mi_weighted_logistic_primary")\n            out_tab["imputation_id"] = imp_idx\n            out_tab["n_total"] = len(work)\n            out_tab["events"] = int(work["incident_spine_2y"].sum())\n            out_tab["weighting_label"] = f"{best[\'label\']}|primary"\n            effect_rows.append(out_tab)\n        except Exception as exc:  # pragma: no cover\n            notes.append(f"Imputation {imp_idx}: primary weighted logistic failed ({exc}).")\n\n        # Doubly robust weighted logistic.\n        dr_terms = [\n            c\n            for c in [\n                "age_at_index",\n                "bmi",\n                "hba1c_recent",\n                "hba1c_mean_year",\n                "diabetes_duration_days",\n                "insulin_use_baseline",\n                "microvascular_any_baseline",\n                "baseline_condition_count",\n                "baseline_outpatient_visits",\n                "baseline_spine_imaging",\n                "baseline_back_pain_flag",\n            ]\n            if c in work.columns\n        ]\n        dr_terms.extend(_reference_term(c, config) for c in _ps_categorical_covariates(work))\n        dr_rhs = " + ".join(dr_terms)\n        dr_formula = "incident_spine_2y ~ treat_glp1" + (f" + {dr_rhs}" if dr_rhs else "")\n        try:\n            dr_fit = smf.glm(\n                dr_formula,\n                data=work,\n                family=sm.families.Binomial(),\n                freq_weights=work["w_treat"],\n            ).fit(cov_type="HC3")\n            if model_store is not None:\n                model_store[f"mi_doubly_robust_weighted_logit_imp{imp_idx}"] = dr_fit\n            dr_tab = _glm_or_table(dr_fit, "mi_doubly_robust_weighted_logit")\n            dr_tab["imputation_id"] = imp_idx\n            dr_tab["n_total"] = len(work)\n            dr_tab["events"] = int(work["incident_spine_2y"].sum())\n            dr_tab["weighting_label"] = f"{best[\'label\']}|primary"\n            effect_rows.append(dr_tab)\n        except Exception as exc:  # pragma: no cover\n            notes.append(f"Imputation {imp_idx}: doubly robust weighted logit failed ({exc}).")\n\n        # Double-weighting sensitivity.\n        try:\n            dbl_fit = smf.glm(\n                "incident_spine_2y ~ treat_glp1",\n                data=work,\n                family=sm.families.Binomial(),\n                freq_weights=work["w_double"],\n            ).fit(cov_type="HC3")\n            if model_store is not None:\n                model_store[f"mi_weighted_logistic_double_imp{imp_idx}"] = dbl_fit\n            dbl_tab = _glm_or_table(dbl_fit, "mi_weighted_logistic_double_weighted")\n            dbl_tab["imputation_id"] = imp_idx\n            dbl_tab["n_total"] = len(work)\n            dbl_tab["events"] = int(work["incident_spine_2y"].sum())\n            dbl_tab["weighting_label"] = f"{best[\'label\']}|double"\n            effect_rows.append(dbl_tab)\n        except Exception as exc:  # pragma: no cover\n            notes.append(f"Imputation {imp_idx}: double-weighted logistic failed ({exc}).")\n\n        aipw_tab = _fit_aipw_binary(work, ps_col="ps_selected", config=config, notes=notes)\n        if not aipw_tab.empty and "coef" in aipw_tab.columns:\n            aipw_tab["imputation_id"] = imp_idx\n            aipw_tab["n_total"] = len(work)\n            aipw_tab["events"] = int(work["incident_spine_2y"].sum())\n            aipw_tab["weighting_label"] = f"{best[\'label\']}|primary"\n            effect_rows.append(aipw_tab)\n\n        if CoxPHFitter is not None:\n            cox_df = work[\n                ["time_to_event_or_censor_days", "event_full_followup", "treat_glp1", "w_treat"]\n            ].copy()\n            cox_df = cox_df.loc[\n                cox_df["time_to_event_or_censor_days"].notna() & (cox_df["time_to_event_or_censor_days"] > 0)\n            ]\n            cox_events = int(cox_df["event_full_followup"].sum()) if not cox_df.empty else 0\n            if len(cox_df) >= threshold and cox_events >= threshold:\n                try:\n                    cph = CoxPHFitter(penalizer=float(config.get("cox_penalizer", 0.0)))\n                    cph.fit(\n                        cox_df,\n                        duration_col="time_to_event_or_censor_days",\n                        event_col="event_full_followup",\n                        weights_col="w_treat",\n                        robust=True,\n                    )\n                    if model_store is not None:\n                        model_store[f"mi_weighted_cox_imp{imp_idx}"] = cph\n                    csum = cph.summary.reset_index()\n                    ctab = pd.DataFrame(\n                        {\n                            "term": csum["covariate"],\n                            "coef": csum["coef"],\n                            "std_error": csum.get("se(coef)", np.nan),\n                            "hr": csum["exp(coef)"],\n                            "ci_low": csum["exp(coef) lower 95%"],\n                            "ci_high": csum["exp(coef) upper 95%"],\n                            "p_value": csum["p"],\n                            "model": "mi_weighted_cox_primary",\n                            "effect_type": "HR",\n                            "effect_scale": "log",\n                            "imputation_id": imp_idx,\n                            "n_total": len(cox_df),\n                            "events": cox_events,\n                            "weighting_label": f"{best[\'label\']}|primary",\n                        }\n                    )\n                    effect_rows.append(ctab)\n                except Exception as exc:  # pragma: no cover\n                    notes.append(f"Imputation {imp_idx}: weighted Cox failed ({exc}).")\n        else:\n            notes.append("lifelines not available; MI weighted Cox skipped.")\n\n    if effect_rows:\n        effect_df = pd.concat(effect_rows, ignore_index=True, sort=False)\n        pooled_effects = _pool_imputation_effects(effect_df)\n    else:\n        pooled_effects = pd.DataFrame()\n\n    if diagnostics_rows:\n        diag_df = pd.concat(diagnostics_rows, ignore_index=True, sort=False)\n        diag_summary = (\n            diag_df.groupby(["model", "term", "weighting_label"], dropna=False)\n            .agg(\n                value=("value", "mean"),\n                value_max=("value", "max"),\n                value_min=("value", "min"),\n                imputations=("imputation_id", "nunique"),\n                n_total=("n_total", "mean"),\n                treated_n=("treated_n", "mean"),\n                control_n=("control_n", "mean"),\n            )\n            .reset_index()\n        )\n        diag_summary["analysis"] = "weight_diagnostics"\n    else:\n        diag_summary = pd.DataFrame()\n\n    if not pooled_effects.empty and not diag_summary.empty:\n        ps_results = pd.concat([pooled_effects, diag_summary], ignore_index=True, sort=False)\n    elif not pooled_effects.empty:\n        ps_results = pooled_effects\n    elif not diag_summary.empty:\n        ps_results = diag_summary\n    else:\n        ps_results = _create_policy_note_df("propensity_score_results")\n\n    balance = _pool_balance_tables(balance_frames, target_abs_smd=target_abs_smd)\n    if balance.empty:\n        balance = _create_policy_note_df("balance_diagnostics")\n\n    summary_max = pd.to_numeric(\n        balance.loc[balance["covariate"] == "__summary_max_abs_smd__", "abs_smd_weighted"], errors="coerce"\n    )\n    if summary_max.notna().any():\n        best_smd = float(summary_max.min())\n        notes.append(\n            f"Best post-weight max |SMD| across MI sets: {best_smd:.3f} (target < {target_abs_smd:.2f})."\n        )\n    return ps_results, balance\n\n\ndef utilization_bias_analysis(\n    df: pd.DataFrame,\n    config: dict,\n    threshold: int,\n    notes: list[str],\n    model_store: dict[str, object] | None = None,\n    diagnostics_store: list[pd.DataFrame] | None = None,\n) -> pd.DataFrame:\n    rows: list[pd.DataFrame] = []\n\n    util_summary = (\n        df.groupby("exposure_group", dropna=False, observed=False)\n        .agg(\n            n=("person_id", "size"),\n            baseline_outpatient_visits_mean=("baseline_outpatient_visits", "mean"),\n            baseline_spine_imaging_mean=("baseline_spine_imaging", "mean"),\n            baseline_endocrinology_visits_mean=("baseline_endocrinology_visits", "mean"),\n            baseline_orthopedics_visits_mean=("baseline_orthopedics_visits", "mean"),\n        )\n        .reset_index()\n    )\n    util_summary["analysis"] = "utilization_descriptives"\n    rows.append(util_summary)\n\n    model_df = df.loc[\n        (df["has_min_followup_2y"] == 1)\n        & df["age_at_index"].notna()\n        & df["bmi_c"].notna()\n        & df["exposure_group"].notna()\n    ].copy()\n    exposure_term = _reference_term("exposure_group", config)\n    shared_terms = _base_logit_terms(config, include_exposure=False)\n    formula = "incident_spine_2y ~ " + " + ".join(\n        [\n            exposure_term,\n            *shared_terms,\n            "bmi_c",\n            "baseline_outpatient_visits",\n            "baseline_spine_imaging",\n            "baseline_endocrinology_visits",\n            "baseline_orthopedics_visits",\n        ]\n    )\n\n    util_model = _fit_logit_if_allowed(\n        model_df,\n        formula,\n        label="utilization_adjusted_logit",\n        event_col="incident_spine_2y",\n        threshold=threshold,\n        notes=notes,\n        config=config,\n        model_store=model_store,\n        diagnostics_store=diagnostics_store,\n    )\n    rows.append(util_model)\n\n    # Sensitivity: restrict within utilization tertiles.\n    tertile_df = model_df.copy()\n    try:\n        tertile_df["utilization_tertile"] = pd.qcut(\n            tertile_df["baseline_outpatient_visits"].rank(method="first"),\n            q=3,\n            labels=["low", "mid", "high"],\n        )\n        for tertile, sub in tertile_df.groupby("utilization_tertile", dropna=False, observed=False):\n            n = int(len(sub))\n            events = int(sub["incident_spine_2y"].sum())\n            compliant, note = model_is_policy_compliant(n, events, threshold)\n            if not compliant:\n                rows.append(pd.DataFrame({"analysis": [f"utilization_tertile_{tertile}"], "policy_note": [note]}))\n                continue\n            tertile_formula = "incident_spine_2y ~ " + " + ".join([exposure_term, *shared_terms, "bmi_c"])\n            tab = _fit_logit_if_allowed(\n                sub,\n                tertile_formula,\n                label=f"utilization_tertile_{tertile}",\n                event_col="incident_spine_2y",\n                threshold=threshold,\n                notes=notes,\n                config=config,\n                model_store=model_store,\n                diagnostics_store=diagnostics_store,\n            )\n            rows.append(tab)\n    except Exception as exc:  # pragma: no cover\n        notes.append(f"Utilization tertile sensitivity failed: {exc}")\n        rows.append(pd.DataFrame({"analysis": ["utilization_tertile_sensitivity"], "error": [str(exc)]}))\n\n    return pd.concat(rows, ignore_index=True, sort=False)\n\n\ndef _closest_measurement_per_window(\n    person_df: pd.DataFrame,\n    target_days: list[int],\n    tolerance_days: int,\n) -> dict[str, float | None]:\n    out: dict[str, float | None] = {}\n    if person_df.empty:\n        for day in target_days:\n            out[f"bmi_{day}d"] = np.nan\n        return out\n\n    value_col = "bmi"\n    if value_col not in person_df.columns:\n        if "bmi_post" in person_df.columns:\n            value_col = "bmi_post"\n        else:\n            for day in target_days:\n                out[f"bmi_{day}d"] = np.nan\n            return out\n\n    for day in target_days:\n        target_date = person_df["index_date"].iloc[0] + pd.Timedelta(days=day)\n        tmp = person_df.copy()\n        tmp["abs_diff"] = (tmp["measurement_date"] - target_date).abs().dt.days\n        tmp = tmp.loc[tmp["abs_diff"] <= tolerance_days]\n        if tmp.empty:\n            out[f"bmi_{day}d"] = np.nan\n        else:\n            out[f"bmi_{day}d"] = float(tmp.sort_values("abs_diff").iloc[0][value_col])\n    return out\n\n\ndef weight_change_analysis(\n    baseline_df: pd.DataFrame,\n    post_index_bmi_df: pd.DataFrame,\n    config: dict,\n    threshold: int,\n    notes: list[str],\n    model_store: dict[str, object] | None = None,\n) -> pd.DataFrame:\n    if post_index_bmi_df.empty:\n        notes.append("Weight-change analysis skipped: no post-index BMI rows returned.")\n        return _create_policy_note_df("weight_change_analysis")\n\n    target_days = list(config["weight_change_target_days"])\n    tolerance = int(config["weight_change_window_tolerance_days"])\n\n    merged = post_index_bmi_df.merge(\n        baseline_df[["person_id", "index_date", "exposure_group", "bmi", "incident_spine_2y"]],\n        on=["person_id", "index_date", "exposure_group"],\n        how="inner",\n        suffixes=("_post", "_baseline"),\n    )\n    if merged.empty:\n        notes.append("Weight-change analysis skipped: no overlap between baseline cohort and post-index BMI measurements.")\n        return _create_policy_note_df("weight_change_analysis")\n\n    rows: list[dict[str, object]] = []\n    for person_id, g in merged.groupby("person_id", dropna=False):\n        window_vals = _closest_measurement_per_window(g, target_days, tolerance)\n        record: dict[str, object] = {\n            "person_id": person_id,\n            "exposure_group": g["exposure_group"].iloc[0],\n            "index_date": g["index_date"].iloc[0],\n            "bmi_baseline": g["bmi_baseline"].iloc[0],\n            "incident_spine_2y": g["incident_spine_2y"].iloc[0],\n        }\n        record.update(window_vals)\n\n        # Slope if at least 2 points.\n        points = []\n        for day in target_days:\n            val = record.get(f"bmi_{day}d")\n            if pd.notna(val):\n                points.append((day, float(val)))\n        if len(points) >= 2:\n            xs = np.array([p[0] for p in points], dtype=float)\n            ys = np.array([p[1] for p in points], dtype=float)\n            slope = np.polyfit(xs, ys, deg=1)[0]\n            record["bmi_slope_per_day"] = float(slope)\n        else:\n            record["bmi_slope_per_day"] = np.nan\n\n        rows.append(record)\n\n    traj = pd.DataFrame(rows)\n    for day in target_days:\n        col = f"bmi_{day}d"\n        traj[f"delta_{day}d"] = traj[col] - traj["bmi_baseline"]\n\n    summary_rows: list[pd.DataFrame] = []\n    agg_spec: dict[str, tuple[str, str]] = {\n        "n": ("person_id", "size"),\n        "bmi_slope_per_day_mean": ("bmi_slope_per_day", "mean"),\n    }\n    for day in target_days:\n        dcol = f"delta_{day}d"\n        if dcol in traj.columns:\n            agg_spec[f"{dcol}_mean"] = (dcol, "mean")\n\n    summary = traj.groupby("exposure_group", dropna=False).agg(**agg_spec).reset_index()\n    summary["analysis"] = "weight_change_descriptives"\n    summary_rows.append(summary)\n\n    # Exploratory mediation-ish model: outcome on treatment + delta BMI at 12 months.\n    preferred_delta = "delta_365d" if "delta_365d" in traj.columns else f"delta_{target_days[min(1, len(target_days)-1)]}d"\n    model_df = traj.loc[\n        traj[preferred_delta].notna() & traj["exposure_group"].notna() & traj["incident_spine_2y"].notna()\n    ].copy()\n    n = int(len(model_df))\n    events = int(model_df["incident_spine_2y"].sum()) if not model_df.empty else 0\n    compliant, note = model_is_policy_compliant(n, events, threshold)\n    if compliant:\n        try:\n            exposure_term = _reference_term("exposure_group", config)\n            fit = smf.logit(\n                f"incident_spine_2y ~ {exposure_term} + {preferred_delta}",\n                data=model_df,\n            ).fit(disp=False, cov_type="HC3")\n            if model_store is not None:\n                model_store[f"weight_change_{preferred_delta}_logit"] = fit\n            summary_rows.append(_logit_or_table(fit, f"weight_change_{preferred_delta}_logit"))\n        except Exception as exc:  # pragma: no cover\n            notes.append(f"Weight-change exploratory model failed: {exc}")\n            summary_rows.append(pd.DataFrame({"analysis": [f"weight_change_{preferred_delta}_logit"], "error": [str(exc)]}))\n    else:\n        summary_rows.append(_create_policy_note_df(f"weight_change_{preferred_delta}_logit"))\n\n    return pd.concat(summary_rows, ignore_index=True, sort=False)\n\n\ndef build_forest_ready(\n    logistic_main: pd.DataFrame,\n    cox_results: pd.DataFrame,\n    ps_results: pd.DataFrame,\n    utilization_results: pd.DataFrame,\n) -> pd.DataFrame:\n    frames: list[pd.DataFrame] = []\n\n    if not logistic_main.empty and {"term", "or", "ci_low", "ci_high", "model"}.issubset(logistic_main.columns):\n        tmp = logistic_main[["term", "or", "ci_low", "ci_high", "model"]].copy()\n        tmp = tmp.rename(columns={"or": "estimate"})\n        tmp["effect_type"] = "OR"\n        frames.append(tmp)\n\n    if not cox_results.empty and {"term", "hr", "ci_low", "ci_high", "model"}.issubset(cox_results.columns):\n        tmp = cox_results[["term", "hr", "ci_low", "ci_high", "model"]].copy()\n        tmp = tmp.rename(columns={"hr": "estimate"})\n        tmp["effect_type"] = "HR"\n        frames.append(tmp)\n\n    if not ps_results.empty and "model" in ps_results.columns:\n        if {"term", "or", "ci_low", "ci_high"}.issubset(ps_results.columns):\n            tmp = ps_results[["term", "or", "ci_low", "ci_high", "model"]].copy()\n            tmp = tmp.rename(columns={"or": "estimate"})\n            tmp["effect_type"] = "OR"\n            frames.append(tmp)\n        if {"term", "hr", "ci_low", "ci_high"}.issubset(ps_results.columns):\n            tmp = ps_results[["term", "hr", "ci_low", "ci_high", "model"]].copy()\n            tmp = tmp.rename(columns={"hr": "estimate"})\n            tmp["effect_type"] = "HR"\n            frames.append(tmp)\n\n    if not utilization_results.empty and {"term", "or", "ci_low", "ci_high", "model"}.issubset(utilization_results.columns):\n        tmp = utilization_results[["term", "or", "ci_low", "ci_high", "model"]].copy()\n        tmp = tmp.rename(columns={"or": "estimate"})\n        tmp["effect_type"] = "OR"\n        frames.append(tmp)\n\n    if not frames:\n        return pd.DataFrame()\n    return pd.concat(frames, ignore_index=True, sort=False)\n\n\ndef run_all_analyses(\n    analytic_df: pd.DataFrame,\n    post_index_bmi_df: pd.DataFrame,\n    config: dict,\n) -> AnalysisBundle:\n    threshold = int(config["small_cell_threshold"])\n    notes: list[str] = []\n    model_store: dict[str, object] = {}\n    logit_diag_chunks: list[pd.DataFrame] = []\n\n    df = prepare_analysis_df(analytic_df, config, notes=notes)\n\n    table1 = build_table1(df)\n    severity = build_severity_baseline_table(df)\n    bmi_missing = build_bmi_missingness_table(df, threshold=threshold, notes=notes)\n    complete_vs_missing, ipw_bmi_df = bmi_missingness_selection_models(\n        df,\n        config=config,\n        threshold=threshold,\n        notes=notes,\n        model_store=model_store,\n        diagnostics_store=logit_diag_chunks,\n    )\n\n    logistic_main, logistic_interaction, interaction_results = run_logistic_models(\n        df,\n        config=config,\n        threshold=threshold,\n        notes=notes,\n        model_store=model_store,\n        diagnostics_store=logit_diag_chunks,\n    )\n    extra_interaction = run_additional_interaction_models(\n        df=df,\n        config=config,\n        threshold=threshold,\n        notes=notes,\n        model_store=model_store,\n        diagnostics_store=logit_diag_chunks,\n    )\n    interaction_results = pd.concat([interaction_results, extra_interaction], ignore_index=True, sort=False)\n\n    cox_results, ph_results, cox_diagnostics, cox_model_df = run_cox_models(\n        df,\n        config=config,\n        threshold=threshold,\n        notes=notes,\n        model_store=model_store,\n    )\n    if not ph_results.empty:\n        interaction_results = pd.concat([interaction_results, ph_results], ignore_index=True, sort=False)\n\n    ps_results, balance = build_ps_and_outcomes(\n        df,\n        config=config,\n        threshold=threshold,\n        notes=notes,\n        model_store=model_store,\n    )\n    notes.append(\n        "Primary causal estimates use MI-pooled weighted/doubly-robust PS models; pattern-mixture deltas remain sensitivity analyses."\n    )\n\n    util_results = utilization_bias_analysis(\n        df,\n        config=config,\n        threshold=threshold,\n        notes=notes,\n        model_store=model_store,\n        diagnostics_store=logit_diag_chunks,\n    )\n    weight_change = weight_change_analysis(\n        baseline_df=df,\n        post_index_bmi_df=post_index_bmi_df,\n        config=config,\n        threshold=threshold,\n        notes=notes,\n        model_store=model_store,\n    )\n\n    km_curve_data = build_km_curve_data(df, config)\n    forest_ready = build_forest_ready(\n        logistic_main=logistic_main,\n        cox_results=cox_results,\n        ps_results=ps_results,\n        utilization_results=util_results,\n    )\n\n    suppression_exclusions = pd.DataFrame(columns=["file", "policy_note", "suppression_columns"])\n    logit_diagnostics = (\n        pd.concat(logit_diag_chunks, ignore_index=True, sort=False)\n        if logit_diag_chunks\n        else pd.DataFrame(columns=["analysis", "scope", "level", "n", "events", "nonevents", "event_rate"])\n    )\n\n    artifacts: dict[str, object] = {\n        "datasets": {\n            "raw_analytic_df": analytic_df,\n            "analysis_df": df,\n            "cox_model_df": cox_model_df,\n            "post_index_bmi_df": post_index_bmi_df,\n            "bmi_ipw_df": ipw_bmi_df,\n        },\n        "models": model_store,\n        "diagnostics": {\n            "logit_diagnostics": logit_diagnostics,\n            "cox_diagnostics": cox_diagnostics,\n            "balance_diagnostics": balance,\n        },\n    }\n\n    return AnalysisBundle(\n        table1=table1,\n        severity_table=severity,\n        bmi_missingness=bmi_missing,\n        complete_vs_missing=complete_vs_missing,\n        logistic_main=logistic_main,\n        logistic_interaction=logistic_interaction,\n        interaction_results=interaction_results,\n        cox_results=cox_results,\n        ps_results=ps_results,\n        balance_diagnostics=balance,\n        utilization_results=util_results,\n        weight_change_results=weight_change,\n        forest_ready=forest_ready,\n        km_curve_data=km_curve_data,\n        suppression_exclusions=suppression_exclusions,\n        logit_diagnostics=logit_diagnostics,\n        cox_diagnostics=cox_diagnostics,\n        notes=notes,\n        artifacts=artifacts,\n    )\n',
    'reporting': '"""Report generation utilities for Project 6 outputs."""\n\nfrom __future__ import annotations\n\nfrom pathlib import Path\n\nimport pandas as pd\n\n\ndef _fmt_pct(x: float | int | None) -> str:\n    if x is None or pd.isna(x):\n        return "NA"\n    return f"{100 * float(x):.1f}%"\n\n\ndef write_report(\n    *,\n    output_dir: Path,\n    change_log: list[str],\n    assumptions: list[str],\n    cohort_flow: pd.DataFrame,\n    generated_files: list[str],\n    notes: list[str],\n    suppression_log: pd.DataFrame,\n) -> Path:\n    report_path = output_dir / "REPORT.md"\n\n    lines: list[str] = []\n    lines.append("# PROJECT 6 REPORT: Degenerative Spine Disease and Diabetes Medications")\n    lines.append("")\n\n    lines.append("## Change Log")\n    for entry in change_log:\n        lines.append(f"- {entry}")\n    lines.append("")\n\n    lines.append("## Assumptions")\n    for entry in assumptions:\n        lines.append(f"- {entry}")\n    lines.append("")\n\n    lines.append("## Cohort Flow")\n    if cohort_flow.empty:\n        lines.append("- Cohort flow unavailable.")\n    else:\n        for _, row in cohort_flow.iterrows():\n            step = row.get("step", "step")\n            n = row.get("n", "NA")\n            lines.append(f"- {step}: {n}")\n    lines.append("")\n\n    lines.append("## Generated Artifacts")\n    for fp in sorted(generated_files):\n        lines.append(f"- `{fp}`")\n    lines.append("")\n\n    lines.append("## Policy Exclusions (AoU n<20)")\n    if suppression_log.empty:\n        lines.append("- No table rows were removed by suppression checks.")\n    else:\n        for _, row in suppression_log.iterrows():\n            file_name = row.get("file", "unknown")\n            policy_note = row.get("policy_note", "")\n            lines.append(f"- `{file_name}`: {policy_note}")\n    lines.append("")\n\n    lines.append("## Notes")\n    if not notes:\n        lines.append("- None.")\n    else:\n        for note in notes:\n            lines.append(f"- {note}")\n    lines.append("")\n\n    lines.append("## Interpretation Guardrails")\n    lines.append("- Binary 2-year models are secondary/replication analyses.")\n    lines.append("- 2-year-censored time-to-event models include participants with positive observed follow-up and treat shorter follow-up as censoring.")\n    lines.append("- Outputs excluded under AoU small-cell policy are not interpreted.")\n\n    report_path.write_text("\\n".join(lines), encoding="utf-8")\n    return report_path\n',
    'main': '"""Main entrypoint for Project 6 spine pipeline."""\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nimport logging\nfrom pathlib import Path\n\nimport pandas as pd\n\nfrom analysis import AnalysisBundle, run_all_analyses\nfrom bq_utils import create_bq_client, validate_dataset_id\nfrom cohort import CohortData, fetch_cohort_data\nfrom config import ASSUMPTIONS, CHANGE_LOG, CONFIG, CONCEPTS, REQUIRED_OUTPUT_FILES, ensure_output_dir, validate_config\nfrom reporting import write_report\nfrom suppression import suppress_small_cells\n\n\n@dataclass\nclass PipelineRunResult:\n    output_dir: Path\n    generated_files: list[str]\n    cohort_data: CohortData\n    analyses: AnalysisBundle\n    notes: list[str]\n    suppression_log: pd.DataFrame\n\n\ndef _configure_logging() -> None:\n    logging.basicConfig(\n        level=logging.INFO,\n        format="%(asctime)s | %(levelname)s | %(message)s",\n    )\n\n\ndef _print_df(label: str, df: pd.DataFrame, max_rows: int = 30) -> None:\n    print(f"\\n===== {label} =====")\n    if df.empty:\n        print("[empty]")\n        return\n    if len(df) > max_rows:\n        print(df.head(max_rows).to_string(index=False))\n        print(f"... ({len(df)} rows total)")\n    else:\n        print(df.to_string(index=False))\n\n\ndef _save_with_policy(\n    *,\n    file_name: str,\n    df: pd.DataFrame,\n    output_dir: Path,\n    threshold: int,\n    suppression_rows: list[pd.DataFrame],\n    count_columns: list[str] | None = None,\n    print_tables: bool = False,\n    print_max_rows: int = 30,\n) -> Path:\n    kept, excluded = suppress_small_cells(df, threshold=threshold, count_columns=count_columns)\n    out_path = output_dir / file_name\n    kept.to_csv(out_path, index=False)\n\n    if not excluded.empty:\n        tmp = excluded.copy()\n        tmp["file"] = file_name\n        suppression_rows.append(tmp)\n\n    logging.info("Saved %s (%s rows)", file_name, len(kept))\n    if logging.getLogger().isEnabledFor(logging.DEBUG) and not kept.empty:\n        logging.debug("%s preview:\\n%s", file_name, kept.head(20).to_string(index=False))\n    if print_tables:\n        _print_df(file_name, kept, max_rows=print_max_rows)\n    return out_path\n\n\ndef _verify_outputs(output_dir: Path, notes: list[str]) -> None:\n    for file_name in REQUIRED_OUTPUT_FILES:\n        if not (output_dir / file_name).exists():\n            notes.append(f"Missing expected output artifact: {file_name}")\n\n\ndef main() -> PipelineRunResult:\n    _configure_logging()\n    validate_config()\n\n    output_dir = ensure_output_dir()\n    dataset = validate_dataset_id(CONFIG["dataset"])\n    threshold = int(CONFIG["small_cell_threshold"])\n    print_tables = bool(CONFIG.get("print_tables_in_notebook", False))\n    print_max_rows = int(CONFIG.get("print_table_max_rows", 30))\n\n    logging.info("Starting Project 6 pipeline. dataset=%s", dataset)\n    logging.info("Using dataset: %s", dataset)\n    logging.info("Output directory: %s", output_dir)\n\n    client = create_bq_client(location=CONFIG["bq_location"])\n\n    cohort_data = fetch_cohort_data(client, dataset, CONFIG, CONCEPTS)\n\n    suppression_rows: list[pd.DataFrame] = []\n    generated_files: list[str] = []\n\n    cohort_flow_path = _save_with_policy(\n        file_name="cohort_flow.csv",\n        df=cohort_data.cohort_flow,\n        output_dir=output_dir,\n        threshold=threshold,\n        suppression_rows=suppression_rows,\n        count_columns=["n"],\n        print_tables=print_tables,\n        print_max_rows=print_max_rows,\n    )\n    generated_files.append(str(cohort_flow_path.name))\n\n    analyses = run_all_analyses(\n        analytic_df=cohort_data.analytic_df,\n        post_index_bmi_df=cohort_data.post_index_bmi_df,\n        config=CONFIG,\n    )\n\n    output_map: list[tuple[str, pd.DataFrame, list[str] | None]] = [\n        ("table1_baseline_by_treatment.csv", analyses.table1, ["n"]),\n        ("severity_baseline_table.csv", analyses.severity_table, ["n"]),\n        (\n            "bmi_missingness_table.csv",\n            analyses.bmi_missingness,\n            ["n_total", "n_bmi_missing", "n_bmi_present"],\n        ),\n        ("complete_vs_missing_bmi_risk.csv", analyses.complete_vs_missing, ["n", "events"]),\n        ("logistic_main_hc3.csv", analyses.logistic_main, ["n_total", "events"]),\n        ("logistic_interaction_obesity.csv", analyses.logistic_interaction, ["n_total", "events"]),\n        ("interaction_results.csv", analyses.interaction_results, ["n_total", "events"]),\n        ("logit_model_diagnostics.csv", analyses.logit_diagnostics, ["n", "events", "nonevents"]),\n        ("cox_model_diagnostics.csv", analyses.cox_diagnostics, ["n", "events", "nonevents"]),\n        ("cox_time_to_spine.csv", analyses.cox_results, ["n_total", "events", "person_time_days"]),\n        (\n            "propensity_score_results.csv",\n            analyses.ps_results,\n            ["n_total"],\n        ),\n        ("balance_diagnostics.csv", analyses.balance_diagnostics, ["n_treated", "n_control"]),\n        ("utilization_bias_results.csv", analyses.utilization_results, ["n", "events", "n_total"]),\n        ("weight_change_analysis.csv", analyses.weight_change_results, ["n", "events", "n_total"]),\n        ("forest_plot_ready.csv", analyses.forest_ready, None),\n        ("km_curve_data.csv", analyses.km_curve_data, ["n_at_risk", "n_events", "n_censored"]),\n    ]\n\n    for file_name, df, count_cols in output_map:\n        path = _save_with_policy(\n            file_name=file_name,\n            df=df,\n            output_dir=output_dir,\n            threshold=threshold,\n            suppression_rows=suppression_rows,\n            count_columns=count_cols,\n            print_tables=print_tables,\n            print_max_rows=print_max_rows,\n        )\n        generated_files.append(path.name)\n\n    suppression_log = (\n        pd.concat(suppression_rows, ignore_index=True, sort=False)\n        if suppression_rows\n        else pd.DataFrame(columns=["file", "policy_note", "suppression_columns"])\n    )\n\n    if suppression_log.empty:\n        logging.info("No rows suppressed under AoU policy threshold n<%s", threshold)\n    else:\n        logging.warning("Suppression applied to %s rows total.", len(suppression_log))\n\n    notes = list(analyses.notes)\n    if CONFIG.get("temp_dataset"):\n        notes.append(f"Intermediate cache tables were materialized in `{CONFIG[\'temp_dataset\']}`.")\n\n    _verify_outputs(output_dir, notes)\n\n    report_path = write_report(\n        output_dir=output_dir,\n        change_log=CHANGE_LOG,\n        assumptions=ASSUMPTIONS,\n        cohort_flow=cohort_data.cohort_flow,\n        generated_files=generated_files,\n        notes=notes,\n        suppression_log=suppression_log[[c for c in ["file", "policy_note", "suppression_columns"] if c in suppression_log.columns]],\n    )\n    generated_files.append(report_path.name)\n\n    logging.info("Pipeline complete. Generated files:")\n    for fp in sorted(generated_files):\n        logging.info("- %s", fp)\n\n    return PipelineRunResult(\n        output_dir=output_dir,\n        generated_files=sorted(generated_files),\n        cohort_data=cohort_data,\n        analyses=analyses,\n        notes=notes,\n        suppression_log=suppression_log,\n    )\n\n\nif __name__ == "__main__":\n    main()\n',
}


def load_module(name, source):
    mod = types.ModuleType(name)
    mod.__file__ = f"<{name}>"
    sys.modules[name] = mod
    exec(source, mod.__dict__)
    return mod

# Load in dependency order.
load_order = ["config", "suppression", "bq_utils", "cohort", "analysis", "reporting", "main"]
for _m in load_order:
    if _m in sys.modules:
        del sys.modules[_m]
for _m in load_order:
    load_module(_m, module_sources[_m])

from config import CONFIG, CONCEPTS, validate_config, ensure_output_dir
from main import main as run_pipeline

# Optional runtime overrides for notebook execution.
USER_INPUT = {
    "workspace_cdr": "fc-aou-cdr-prod-ct.C2024Q3R8",
    "bq_location": "US",
    "temp_dataset": "",
    "output_dir": "./project6_outputs",
    # Optional: override concept sets without editing module source.
    "concept_overrides": {
        # "SPINE_OUTCOME_SNOMED_CONCEPT_IDS": [137548, 198520],
        # "GLP1_INGREDIENT_NAMES": ["semaglutide", "liraglutide", "dulaglutide"],
    },
}

if USER_INPUT.get("workspace_cdr") and "<PASTE" not in USER_INPUT["workspace_cdr"]:
    CONFIG["dataset"] = USER_INPUT["workspace_cdr"].strip()
if USER_INPUT.get("bq_location"):
    CONFIG["bq_location"] = str(USER_INPUT["bq_location"]).strip()
if USER_INPUT.get("temp_dataset"):
    CONFIG["temp_dataset"] = str(USER_INPUT["temp_dataset"]).strip()
if USER_INPUT.get("output_dir"):
    CONFIG["output_dir"] = str(Path(str(USER_INPUT["output_dir"])).resolve())
else:
    CONFIG["output_dir"] = str((Path.cwd() / "project6_outputs").resolve())

for _k, _v in USER_INPUT.get("concept_overrides", {}).items():
    if _v:
        CONCEPTS[_k] = _v

# Ensure output folders exist (uses CONFIG["output_dir"]).
ensure_output_dir()

try:
    validate_config()
except Exception as exc:
    raise RuntimeError(
        "Configuration invalid. Set USER_INPUT['workspace_cdr'] or WORKSPACE_CDR env var before running."
    ) from exc

result = run_pipeline()
print(f"Pipeline finished. Output directory: {result.output_dir}")
print("Generated files:")
for fp in result.generated_files:
    print(" -", fp)

import pandas as pd
from IPython.display import display, Markdown
from pandas.errors import EmptyDataError

preview_paths = sorted(result.output_dir.glob("*.csv"))
if not preview_paths:
    print("No CSV outputs found in output directory.")

for p in preview_paths:
    display(Markdown(f"### `{p}`"))
    if not p.exists():
        print("status: missing file")
        continue
    if p.stat().st_size == 0:
        print("status: empty file (0 bytes)")
        continue
    try:
        df = pd.read_csv(p)
        print(f"status: OK | rows={len(df):,}, cols={df.shape[1]}")
        display(df.head(50))
    except EmptyDataError:
        print("status: EmptyDataError (no columns)")
