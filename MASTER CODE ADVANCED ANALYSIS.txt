"""
================================================================================
ADVANCED CARDIOMETABOLIC ANALYSIS - NOVEL RESEARCH QUESTIONS
================================================================================
Building on STATIN + METFORMIN + GLP-1 ANALYSIS

Treatment Groups (7 mutually exclusive):
1. Statin monotherapy (statin only, no metformin, no GLP-1)
2. Metformin monotherapy (metformin only, no statin, no GLP-1)
3. GLP-1 monotherapy (GLP-1 only, no metformin, no statin)
4. Statin + GLP-1 (statin and GLP-1, no metformin)
5. Statin + Metformin (statin and metformin, no GLP-1)
6. Metformin + GLP-1 (metformin and GLP-1, no statin)
7. Statin + GLP-1 + Metformin (all three medications)

NOVEL ANALYSES INCLUDED:
1. Treatment Sequencing / Intensification Patterns
2. Time-to-Glycemic Control (Survival Analysis)
3. Treatment Response Heterogeneity / Phenotyping (ML Clustering)
4. Discordant Responders Analysis
5. GLP-1 Effect in Statin-Naive Population
6. Inflammatory Trajectory as Predictor
7. Therapeutic Inertia Analysis

All results include 95% CI and SE
================================================================================
"""

import pandas
import os
import pandas as pd
import numpy as np
from scipy import stats
from itertools import combinations
import warnings
warnings.filterwarnings('ignore')

# Import centralized concept sets -- all concept IDs are defined here
from concept_sets import (
    TYPE_2_DIABETES, BMI_INCLUSION, CV_EXCLUSION, PREGNANCY_EXCLUSION,
    ETHNICITY_EXCLUSION, RACE_EXCLUSION, GENDER_EXCLUSION, SEX_AT_BIRTH_EXCLUSION,
    INSULIN_EXCLUSION,
    METFORMIN, GLP1_AGONISTS, STATINS, ALL_DRUG_CONCEPT_IDS,
    MEASUREMENT_STANDARD_CONCEPTS, MEASUREMENT_SOURCE_CONCEPTS,
    BMI_MEASUREMENT, HYPERTENSION_CONDITIONS,
    SMOKING_SURVEY_QUESTIONS,
    NEW_COHORT_MEASUREMENT_CONCEPTS, NEW_COHORT_DRUG_CONCEPTS, NEW_COHORT_CV_EXCLUSION,
    GLYCEMIC_OUTCOMES, LIPID_OUTCOMES, ALL_OUTCOMES,
    PLAUSIBLE_RANGES, TREATMENT_GROUPS,
    PRE_WINDOW_DAYS, POST_WINDOW_DAYS, MIN_PRE_POST_GAP_DAYS,
    MEASURE_SPECIFIC_MIN_GAP,
    HBA1C_TARGET, GLYCEMIC_RESPONSE_THRESHOLD,
    LIPID_RESPONSE_LDL_TARGET, LIPID_RESPONSE_LDL_CHANGE,
    AGE_RANGE,
    ids_for_sql, print_concept_set_inventory,
)

# ============================================================================
# SECTION 0: PRINT CONCEPT SET INVENTORY
# ============================================================================

print("="*100)
print("SECTION 0: CONCEPT SET INVENTORY")
print("="*100)
concept_inventory_df = print_concept_set_inventory()

# ============================================================================
# SECTION 1: SQL QUERIES (ORIGINAL WORKING QUERIES)
# ============================================================================

print("\n" + "="*100)
print("SECTION 1: RUNNING SQL QUERIES")
print("="*100)

# -----------------------------------------------------------------------------
# 1.1 Person/Demographics Query
# -----------------------------------------------------------------------------
# Build cohort filter using centralized concept sets
_cdr = os.environ["WORKSPACE_CDR"]
_age_lo, _age_hi = AGE_RANGE

# -- Reusable cohort WHERE clause (references concept_sets.py) --
_cohort_where = f"""
        cb_search_person.person_id IN (SELECT
            person_id
        FROM
            `{_cdr}.cb_search_person` p
        WHERE
            DATE_DIFF(CURRENT_DATE, dob, YEAR) - IF(EXTRACT(MONTH FROM dob)*100 + EXTRACT(DAY FROM dob) > EXTRACT(MONTH FROM CURRENT_DATE)*100 + EXTRACT(DAY FROM CURRENT_DATE), 1, 0) BETWEEN {_age_lo} AND {_age_hi}
            AND NOT EXISTS (SELECT 'x' FROM `{_cdr}.death` d WHERE d.person_id = p.person_id))
        AND cb_search_person.person_id IN (SELECT
            criteria.person_id
        FROM
            (SELECT DISTINCT person_id, entry_date, concept_id
            FROM `{_cdr}.cb_search_all_events`
            WHERE (concept_id IN(SELECT DISTINCT c.concept_id
                FROM `{_cdr}.cb_criteria` c
                JOIN (SELECT CAST(cr.id as string) AS id FROM `{_cdr}.cb_criteria` cr
                    WHERE concept_id IN ({ids_for_sql(TYPE_2_DIABETES)}) AND full_text LIKE '%_rank1]%') a
                    ON (c.path LIKE CONCAT('%.', a.id, '.%') OR c.path LIKE CONCAT('%.', a.id)
                        OR c.path LIKE CONCAT(a.id, '.%') OR c.path = a.id)
                WHERE is_standard = 1 AND is_selectable = 1) AND is_standard = 1)) criteria)
        AND cb_search_person.person_id IN (SELECT
            criteria.person_id
        FROM
            (SELECT DISTINCT person_id, entry_date, concept_id
            FROM `{_cdr}.cb_search_all_events`
            WHERE (concept_id IN ({ids_for_sql(BMI_INCLUSION)}) AND is_standard = 0 AND value_as_number >= 25.0)) criteria)
        AND cb_search_person.person_id NOT IN (SELECT
            criteria.person_id
        FROM
            (SELECT DISTINCT person_id, entry_date, concept_id
            FROM `{_cdr}.cb_search_all_events`
            WHERE (concept_id IN(SELECT DISTINCT c.concept_id
                FROM `{_cdr}.cb_criteria` c
                JOIN (SELECT CAST(cr.id as string) AS id FROM `{_cdr}.cb_criteria` cr
                    WHERE concept_id IN ({ids_for_sql(CV_EXCLUSION)})
                    AND full_text LIKE '%_rank1]%') a
                    ON (c.path LIKE CONCAT('%.', a.id, '.%') OR c.path LIKE CONCAT('%.', a.id)
                        OR c.path LIKE CONCAT(a.id, '.%') OR c.path = a.id)
                WHERE is_standard = 1 AND is_selectable = 1) AND is_standard = 1)) criteria)
        AND cb_search_person.person_id NOT IN (SELECT
            criteria.person_id
        FROM
            (SELECT DISTINCT person_id, entry_date, concept_id
            FROM `{_cdr}.cb_search_all_events`
            WHERE (concept_id IN(SELECT DISTINCT c.concept_id
                FROM `{_cdr}.cb_criteria` c
                JOIN (SELECT CAST(cr.id as string) AS id FROM `{_cdr}.cb_criteria` cr
                    WHERE concept_id IN ({ids_for_sql(PREGNANCY_EXCLUSION)}) AND full_text LIKE '%_rank1]%') a
                    ON (c.path LIKE CONCAT('%.', a.id, '.%') OR c.path LIKE CONCAT('%.', a.id)
                        OR c.path LIKE CONCAT(a.id, '.%') OR c.path = a.id)
                WHERE is_standard = 1 AND is_selectable = 1) AND is_standard = 1)) criteria)
        AND cb_search_person.person_id NOT IN (
            SELECT person_id FROM `{_cdr}.person` p
            WHERE ethnicity_concept_id IN ({ids_for_sql(ETHNICITY_EXCLUSION)})
            UNION DISTINCT
            SELECT person_id FROM `{_cdr}.person` p
            WHERE race_concept_id IN ({ids_for_sql(RACE_EXCLUSION)})
            UNION DISTINCT
            SELECT person_id FROM `{_cdr}.person` p
            WHERE gender_concept_id IN ({ids_for_sql(GENDER_EXCLUSION)})
            UNION DISTINCT
            SELECT person_id FROM `{_cdr}.person` p
            WHERE sex_at_birth_concept_id IN ({ids_for_sql(SEX_AT_BIRTH_EXCLUSION)}))
        AND cb_search_person.person_id NOT IN (
            SELECT DISTINCT person_id
            FROM `{_cdr}.drug_exposure`
            WHERE drug_concept_id IN (
                SELECT DISTINCT ca.descendant_id
                FROM `{_cdr}.cb_criteria_ancestor` ca
                WHERE ca.ancestor_id IN ({ids_for_sql(INSULIN_EXCLUSION)})))"""

dataset_28107006_person_sql = f"""
    SELECT
        person.person_id,
        person.gender_concept_id,
        p_gender_concept.concept_name as gender,
        person.birth_datetime as date_of_birth,
        person.race_concept_id,
        p_race_concept.concept_name as race,
        person.ethnicity_concept_id,
        p_ethnicity_concept.concept_name as ethnicity,
        person.sex_at_birth_concept_id,
        p_sex_at_birth_concept.concept_name as sex_at_birth
    FROM
        `{_cdr}.person` person
    LEFT JOIN
        `{_cdr}.concept` p_gender_concept
            ON person.gender_concept_id = p_gender_concept.concept_id
    LEFT JOIN
        `{_cdr}.concept` p_race_concept
            ON person.race_concept_id = p_race_concept.concept_id
    LEFT JOIN
        `{_cdr}.concept` p_ethnicity_concept
            ON person.ethnicity_concept_id = p_ethnicity_concept.concept_id
    LEFT JOIN
        `{_cdr}.concept` p_sex_at_birth_concept
            ON person.sex_at_birth_concept_id = p_sex_at_birth_concept.concept_id
    WHERE
        person.PERSON_ID IN (SELECT
            distinct person_id
        FROM
            `{_cdr}.cb_search_person` cb_search_person
        WHERE
            {_cohort_where})"""

dataset_28107006_person_df = pandas.read_gbq(
    dataset_28107006_person_sql,
    dialect="standard",
    use_bqstorage_api=("BIGQUERY_STORAGE_API_ENABLED" in os.environ),
    progress_bar_type="tqdm_notebook")

print(f"Person count: {len(dataset_28107006_person_df)}")

# -----------------------------------------------------------------------------
# 1.2 Measurement Query
# -----------------------------------------------------------------------------
dataset_28107006_measurement_sql = f"""
    SELECT
        measurement.person_id,
        measurement.measurement_concept_id,
        m_standard_concept.concept_name as standard_concept_name,
        measurement.measurement_datetime,
        measurement.value_as_number,
        measurement.unit_concept_id,
        m_unit.concept_name as unit_concept_name
    FROM
        ( SELECT *
        FROM `{_cdr}.measurement` measurement
        WHERE
            (measurement_concept_id IN (SELECT DISTINCT c.concept_id
                FROM `{_cdr}.cb_criteria` c
                JOIN (SELECT CAST(cr.id as string) AS id FROM `{_cdr}.cb_criteria` cr
                    WHERE concept_id IN ({ids_for_sql(MEASUREMENT_STANDARD_CONCEPTS)})
                    AND full_text LIKE '%_rank1]%') a
                    ON (c.path LIKE CONCAT('%.', a.id, '.%') OR c.path LIKE CONCAT('%.', a.id)
                        OR c.path LIKE CONCAT(a.id, '.%') OR c.path = a.id)
                WHERE is_standard = 1 AND is_selectable = 1)
            OR measurement_source_concept_id IN (SELECT DISTINCT c.concept_id
                FROM `{_cdr}.cb_criteria` c
                JOIN (SELECT CAST(cr.id as string) AS id FROM `{_cdr}.cb_criteria` cr
                    WHERE concept_id IN ({ids_for_sql(MEASUREMENT_SOURCE_CONCEPTS)}) AND full_text LIKE '%_rank1]%') a
                    ON (c.path LIKE CONCAT('%.', a.id, '.%') OR c.path LIKE CONCAT('%.', a.id)
                        OR c.path LIKE CONCAT(a.id, '.%') OR c.path = a.id)
                WHERE is_standard = 0 AND is_selectable = 1))
            AND measurement.PERSON_ID IN (SELECT distinct person_id
                FROM `{_cdr}.cb_search_person` cb_search_person
                WHERE {_cohort_where})) measurement
        LEFT JOIN `{_cdr}.concept` m_standard_concept
            ON measurement.measurement_concept_id = m_standard_concept.concept_id
        LEFT JOIN `{_cdr}.concept` m_unit
            ON measurement.unit_concept_id = m_unit.concept_id"""

dataset_28107006_measurement_df = pandas.read_gbq(
    dataset_28107006_measurement_sql,
    dialect="standard",
    use_bqstorage_api=("BIGQUERY_STORAGE_API_ENABLED" in os.environ),
    progress_bar_type="tqdm_notebook")

print(f"Measurement count: {len(dataset_28107006_measurement_df)}")

# -----------------------------------------------------------------------------
# 1.3 Drug Exposure Query - Statins, Metformin, and GLP-1
# -----------------------------------------------------------------------------
dataset_28107006_drug_sql = f"""
    SELECT
        d_exposure.person_id,
        d_exposure.drug_concept_id,
        d_standard_concept.concept_name as standard_concept_name,
        d_exposure.drug_exposure_start_datetime
    FROM
        ( SELECT *
        FROM `{_cdr}.drug_exposure` d_exposure
        WHERE
            (drug_concept_id IN (SELECT DISTINCT ca.descendant_id
                FROM `{_cdr}.cb_criteria_ancestor` ca
                JOIN (SELECT DISTINCT c.concept_id
                    FROM `{_cdr}.cb_criteria` c
                    JOIN (SELECT CAST(cr.id as string) AS id FROM `{_cdr}.cb_criteria` cr
                        WHERE concept_id IN (
                            {ids_for_sql(ALL_DRUG_CONCEPT_IDS)}
                        ) AND full_text LIKE '%_rank1]%') a
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') OR c.path LIKE CONCAT('%.', a.id)
                            OR c.path LIKE CONCAT(a.id, '.%') OR c.path = a.id)
                    WHERE is_standard = 1 AND is_selectable = 1) b
                    ON (ca.ancestor_id = b.concept_id)))
            AND d_exposure.PERSON_ID IN (SELECT distinct person_id
                FROM `{_cdr}.cb_search_person` cb_search_person
                WHERE {_cohort_where})) d_exposure
        LEFT JOIN `{_cdr}.concept` d_standard_concept
            ON d_exposure.drug_concept_id = d_standard_concept.concept_id"""

dataset_28107006_drug_df = pandas.read_gbq(
    dataset_28107006_drug_sql,
    dialect="standard",
    use_bqstorage_api=("BIGQUERY_STORAGE_API_ENABLED" in os.environ),
    progress_bar_type="tqdm_notebook")

print(f"Drug exposure count: {len(dataset_28107006_drug_df)}")

# -----------------------------------------------------------------------------
# 1.4 Survey Query -- Smoking status from AoU PPI surveys
# Uses SMOKING_SURVEY_QUESTIONS from concept_sets.py
# Questions: 1585857 (100 cigarettes ever), 1585860 (current frequency),
#            1586177 (cigarette status); 1585855 (parent tobacco hierarchy)
# -----------------------------------------------------------------------------

_smoking_direct_ids = ids_for_sql((1585857, 1585860, 1586177))
_smoking_parent_id = 1585855

dataset_28107006_survey_sql = f"""
    SELECT
        answer.person_id,
        answer.survey_datetime,
        answer.survey,
        answer.question_concept_id,
        answer.question,
        answer.answer_concept_id,
        answer.answer,
        answer.survey_version_concept_id,
        answer.survey_version_name
    FROM
        `{_cdr}.ds_survey` answer
    WHERE
        (
            question_concept_id IN ({_smoking_direct_ids})
            OR question_concept_id IN (SELECT
                DISTINCT concept_id
            FROM
                `{_cdr}.cb_criteria` c
            JOIN
                (SELECT
                    CAST(cr.id as string) AS id
                FROM
                    `{_cdr}.cb_criteria` cr
                WHERE
                    concept_id IN ({_smoking_parent_id})
                    AND domain_id = 'SURVEY') a
                ON (c.path like CONCAT('%', a.id, '.%'))
            WHERE
                domain_id = 'SURVEY'
                AND type = 'PPI'
                AND subtype = 'QUESTION')
        )
        AND (
            answer.PERSON_ID IN (SELECT
                distinct person_id
            FROM
                `{_cdr}.cb_search_person` cb_search_person
            WHERE
                {_cohort_where})
        )"""

dataset_28107006_survey_df = pandas.read_gbq(
    dataset_28107006_survey_sql,
    dialect="standard",
    use_bqstorage_api=("BIGQUERY_STORAGE_API_ENABLED" in os.environ),
    progress_bar_type="tqdm_notebook")

print(f"Survey (smoking) rows: {len(dataset_28107006_survey_df)}")

# ============================================================================
# SECTION 2: DATA PROCESSING
# ============================================================================

print("\n" + "="*100)
print("SECTION 2: DATA PROCESSING AND TREATMENT GROUP CREATION")
print("="*100)

# Categorize measurements (excluding inflammatory biomarkers per study protocol)
def categorize_measurement(name):
    """Map measurement concept names to outcome categories.

    NOTE: Inflammatory biomarkers (CRP, ESR, Fibrinogen) are intentionally
    excluded from categorization. They will be classified as 'Other' and
    filtered out of downstream analyses.
    """
    if pd.isna(name):
        return 'Other'
    name_lower = str(name).lower()

    if 'hemoglobin a1c' in name_lower or 'hba1c' in name_lower:
        return 'HbA1c'
    elif 'glucose' in name_lower:
        return 'Glucose'
    elif 'triglyceride' in name_lower:
        return 'Triglycerides'
    elif 'hdl' in name_lower:
        return 'HDL'
    elif 'ldl' in name_lower:
        return 'LDL'
    elif 'cholesterol' in name_lower and 'vldl' not in name_lower:
        return 'Total_Cholesterol'
    elif 'body mass' in name_lower or 'bmi' in name_lower:
        return 'BMI'
    else:
        return 'Other'

dataset_28107006_measurement_df['measurement_category'] = dataset_28107006_measurement_df['standard_concept_name'].apply(categorize_measurement)

# Data cleaning -- plausible ranges from concept_sets.py (no inflammatory markers)
n_before = len(dataset_28107006_measurement_df)
for category, (min_val, max_val) in PLAUSIBLE_RANGES.items():
    mask = dataset_28107006_measurement_df['measurement_category'] == category
    out_of_range = mask & (
        (dataset_28107006_measurement_df['value_as_number'] < min_val) |
        (dataset_28107006_measurement_df['value_as_number'] > max_val) |
        (dataset_28107006_measurement_df['value_as_number'].isna())
    )
    dataset_28107006_measurement_df = dataset_28107006_measurement_df[~out_of_range]

print(f"Cleaned measurements: {len(dataset_28107006_measurement_df)} (removed {n_before - len(dataset_28107006_measurement_df)})")

# ============================================================================
# SECTION 2A: BMI EXTRACTION CONSISTENCY (Requirement B)
# ============================================================================
# Strategy: BMI is extracted from the measurement table using:
#   1. measurement_concept_id matching BMI_MEASUREMENT concept IDs
#   2. String pattern match on standard_concept_name ('body mass index'/'bmi')
#   3. Categorized as 'BMI' by categorize_measurement()
# Validation: unrealistic bounds [12, 80], unit mismatch flags, % missing by group
# ============================================================================

print("\n" + "-"*80)
print("SECTION 2A: BMI EXTRACTION CONSISTENCY AUDIT")
print("-"*80)

# Identify BMI rows
bmi_mask = (
    dataset_28107006_measurement_df['measurement_category'] == 'BMI'
) | (
    dataset_28107006_measurement_df['measurement_concept_id'].isin(BMI_MEASUREMENT['concept_ids'])
) | (
    dataset_28107006_measurement_df['standard_concept_name'].astype(str).str.contains(
        BMI_MEASUREMENT['string_pattern'], case=False, na=False
    )
)

# Ensure all BMI rows are categorized consistently
dataset_28107006_measurement_df.loc[bmi_mask, 'measurement_category'] = 'BMI'

bmi_rows = dataset_28107006_measurement_df[
    dataset_28107006_measurement_df['measurement_category'] == 'BMI'
].copy()

print(f"\nBMI extraction summary:")
print(f"  Total BMI rows (after categorization): {len(bmi_rows)}")
print(f"  Unique patients with BMI:              {bmi_rows['person_id'].nunique()}")

# Plausible range enforcement (already applied above via PLAUSIBLE_RANGES)
bmi_min, bmi_max = PLAUSIBLE_RANGES['BMI']
bmi_out_of_range = bmi_rows[
    (bmi_rows['value_as_number'] < bmi_min) | (bmi_rows['value_as_number'] > bmi_max)
]
print(f"  BMI values outside [{bmi_min}, {bmi_max}]: {len(bmi_out_of_range)} (already removed)")

# Unit mismatch flag
# Expected unit: kg/m2.  Flag rows where unit_concept_name is present but not kg/m2
if 'unit_concept_name' in bmi_rows.columns:
    bmi_unit_check = bmi_rows[bmi_rows['unit_concept_name'].notna()].copy()
    bmi_unit_mismatch = bmi_unit_check[
        ~bmi_unit_check['unit_concept_name'].str.contains('kg/m2|kilogram per square meter', case=False, na=True)
    ]
    print(f"  BMI unit mismatches (unit != kg/m2): {len(bmi_unit_mismatch)}")
    if len(bmi_unit_mismatch) > 0:
        print(f"    Mismatched units: {bmi_unit_mismatch['unit_concept_name'].value_counts().to_dict()}")
else:
    print(f"  BMI unit column not available for audit")

# Missing BMI by source
print(f"\n  BMI by source concept_id:")
bmi_source_counts = bmi_rows['measurement_concept_id'].value_counts()
for cid, cnt in bmi_source_counts.items():
    concept_name = bmi_rows[bmi_rows['measurement_concept_id'] == cid]['standard_concept_name'].iloc[0] if cnt > 0 else 'Unknown'
    print(f"    concept_id={cid} ({concept_name}): {cnt} rows")

print("\n  BMI derivation strategy: STANDARDIZED")
print(f"    Source: measurement table, concept_ids={BMI_MEASUREMENT['concept_ids']}")
print(f"    String fallback: '{BMI_MEASUREMENT['string_pattern']}'")
print(f"    Plausible range: [{bmi_min}, {bmi_max}] kg/m2")
print(f"    Expected unit: {BMI_MEASUREMENT['expected_unit']}")

# ============================================================================
# SECTION 2B: SMOKING VARIABLE -- Binary Derivation (Requirement C)
# ============================================================================
# Rule:
#   "Smoker"     = ANY smoking-related survey answer is NOT in the non_smoker set
#   "Non-smoker" = ALL smoking-related answers are explicitly "No" / "Not at all"
#   "Unknown"    = No smoking survey data, or all answers are skip/prefer-not-to-answer
#
# Survey questions used (from concept_sets.py SMOKING_SURVEY_QUESTIONS):
#   1585857 = "Have you smoked at least 100 cigarettes in your entire life?"
#   1585860 = "Do you now smoke cigarettes every day, some days, or not at all?"
#   1586177 = "Cigarette Smoking Status"
#   1585855 = parent hierarchy for tobacco use sub-questions
# ============================================================================

print("\n" + "-"*80)
print("SECTION 2B: SMOKING VARIABLE (BINARY) DERIVATION")
print("-"*80)

_non_smoker_answers = set(a.lower() for a in SMOKING_SURVEY_QUESTIONS['non_smoker_answers'])
_unknown_skip_answers = set(a.lower() for a in SMOKING_SURVEY_QUESTIONS['unknown_skip_answers'])

def derive_smoking_status(person_survey_df):
    """
    Derive binary smoking status for one person from their survey responses.

    Returns: 'Smoker', 'Non_Smoker', or 'Unknown'
    """
    if person_survey_df is None or len(person_survey_df) == 0:
        return 'Unknown'

    answers = person_survey_df['answer'].dropna().astype(str).str.strip()
    if len(answers) == 0:
        return 'Unknown'

    answers_lower = set(answers.str.lower())

    # If ALL answers are skip/unknown, classify as Unknown
    if answers_lower.issubset(_unknown_skip_answers):
        return 'Unknown'

    # If ANY answer is NOT in the non-smoker or skip set -> Smoker
    non_smoker_or_skip = _non_smoker_answers | _unknown_skip_answers
    for ans in answers_lower:
        if ans not in non_smoker_or_skip:
            return 'Smoker'

    # All meaningful answers are in the non-smoker set
    if answers_lower & _non_smoker_answers:
        return 'Non_Smoker'

    return 'Unknown'

# Apply to all cohort patients
smoking_records = []
cohort_person_ids = set(dataset_28107006_person_df['person_id'].unique())

for pid in cohort_person_ids:
    person_surveys = dataset_28107006_survey_df[dataset_28107006_survey_df['person_id'] == pid]
    status = derive_smoking_status(person_surveys)
    smoking_records.append({'person_id': pid, 'smoking_status': status})

smoking_df = pd.DataFrame(smoking_records)

print(f"\nSmoking variable derivation complete:")
print(f"  Total cohort patients: {len(smoking_df)}")
print(f"\n  Smoking Status Distribution:")
for status in ['Smoker', 'Non_Smoker', 'Unknown']:
    n = (smoking_df['smoking_status'] == status).sum()
    pct = n / len(smoking_df) * 100 if len(smoking_df) > 0 else 0
    print(f"    {status:<15}: {n:>6} ({pct:>5.1f}%)")

print(f"\n  Survey questions used:")
print(f"    concept_ids: {SMOKING_SURVEY_QUESTIONS['concept_ids']}")
print(f"    Description: {SMOKING_SURVEY_QUESTIONS['description']}")
print(f"\n  Classification rules:")
print(f"    Smoker:     ANY answer NOT in {SMOKING_SURVEY_QUESTIONS['non_smoker_answers']}")
print(f"    Non_Smoker: ALL answers explicitly in {SMOKING_SURVEY_QUESTIONS['non_smoker_answers']}")
print(f"    Unknown:    No survey data OR all answers are skip/prefer-not-to-answer")

# -----------------------------------------------------------------------------
# IDENTIFY DRUG USERS: Statins, Metformin, and GLP-1
# -----------------------------------------------------------------------------

# Drug classification patterns (from concept_sets.py)
metformin_pattern = METFORMIN['string_pattern']
glp1_pattern = GLP1_AGONISTS['string_pattern']
statin_pattern = STATINS['string_pattern']

# Metformin users
metformin_users = set(dataset_28107006_drug_df[
    dataset_28107006_drug_df['standard_concept_name'].str.contains(metformin_pattern, case=False, na=False)
]['person_id'].unique())

# GLP-1 agonist users
glp1_users = set(dataset_28107006_drug_df[
    dataset_28107006_drug_df['standard_concept_name'].str.contains(glp1_pattern, case=False, na=False)
]['person_id'].unique())

# Statin users (HMG-CoA reductase inhibitors)
statin_users = set(dataset_28107006_drug_df[
    dataset_28107006_drug_df['standard_concept_name'].str.contains(statin_pattern, case=False, na=False)
]['person_id'].unique())

print(f"\nTotal users identified:")
print(f"  - Statin users:    {len(statin_users):>6}")
print(f"  - Metformin users: {len(metformin_users):>6}")
print(f"  - GLP-1 users:     {len(glp1_users):>6}")

# -----------------------------------------------------------------------------
# CREATE 7 MUTUALLY EXCLUSIVE TREATMENT GROUPS
# -----------------------------------------------------------------------------

# Monotherapy groups (only one drug class)
statin_mono = statin_users - metformin_users - glp1_users
metformin_mono = metformin_users - statin_users - glp1_users
glp1_mono = glp1_users - statin_users - metformin_users

# Dual therapy groups (exactly two drug classes)
statin_glp1 = (statin_users & glp1_users) - metformin_users
statin_metformin = (statin_users & metformin_users) - glp1_users
metformin_glp1 = (metformin_users & glp1_users) - statin_users

# Triple therapy group (all three drug classes)
statin_glp1_metformin = statin_users & glp1_users & metformin_users

print(f"\nTreatment Groups (Mutually Exclusive):")
print(f"  MONOTHERAPY:")
print(f"    1. Statin only:           {len(statin_mono):>6}")
print(f"    2. Metformin only:        {len(metformin_mono):>6}")
print(f"    3. GLP-1 only:            {len(glp1_mono):>6}")
print(f"  DUAL THERAPY:")
print(f"    4. Statin + GLP-1:        {len(statin_glp1):>6}")
print(f"    5. Statin + Metformin:    {len(statin_metformin):>6}")
print(f"    6. Metformin + GLP-1:     {len(metformin_glp1):>6}")
print(f"  TRIPLE THERAPY:")
print(f"    7. Statin + GLP-1 + Met:  {len(statin_glp1_metformin):>6}")

# Verify groups are mutually exclusive
all_patients = (statin_mono | metformin_mono | glp1_mono |
                statin_glp1 | statin_metformin | metformin_glp1 |
                statin_glp1_metformin)
print(f"\n  Total unique patients:      {len(all_patients):>6}")

# -----------------------------------------------------------------------------
# GET INDEX DATES FOR EACH GROUP (WITH DETAILED DRUG START DATES)
# -----------------------------------------------------------------------------

def get_drug_start_date(person_id, drug_df, drug_pattern):
    """Get first start date for a specific drug pattern for a person."""
    person_drugs = drug_df[
        (drug_df['person_id'] == person_id) &
        (drug_df['standard_concept_name'].str.contains(drug_pattern, case=False, na=False))
    ]
    if len(person_drugs) > 0:
        return person_drugs['drug_exposure_start_datetime'].min()
    return pd.NaT

def get_index_dates(person_ids, drug_df, drug_pattern, group_name):
    dates = drug_df[
        (drug_df['person_id'].isin(person_ids)) &
        (drug_df['standard_concept_name'].str.contains(drug_pattern, case=False, na=False))
    ].groupby('person_id')['drug_exposure_start_datetime'].min().reset_index()
    dates.columns = ['person_id', 'index_date']
    dates['group'] = group_name
    return dates

# For monotherapy groups, use the respective drug as index date
statin_mono_dates = get_index_dates(statin_mono, dataset_28107006_drug_df, statin_pattern, 'Statin_mono')
metformin_mono_dates = get_index_dates(metformin_mono, dataset_28107006_drug_df, metformin_pattern, 'Metformin_mono')
glp1_mono_dates = get_index_dates(glp1_mono, dataset_28107006_drug_df, glp1_pattern, 'GLP1_mono')

# For dual therapy groups, use the later of the two drug start dates
def get_dual_index_dates(person_ids, drug_df, pattern1, pattern2, group_name):
    dates_list = []
    for pid in person_ids:
        person_drugs = drug_df[drug_df['person_id'] == pid]
        drug1_start = person_drugs[
            person_drugs['standard_concept_name'].str.contains(pattern1, case=False, na=False)
        ]['drug_exposure_start_datetime'].min()
        drug2_start = person_drugs[
            person_drugs['standard_concept_name'].str.contains(pattern2, case=False, na=False)
        ]['drug_exposure_start_datetime'].min()
        if pd.notna(drug1_start) and pd.notna(drug2_start):
            index_date = max(drug1_start, drug2_start)
            dates_list.append({
                'person_id': pid,
                'index_date': index_date,
                'group': group_name,
                'drug1_start': drug1_start,
                'drug2_start': drug2_start
            })
    return pd.DataFrame(dates_list)

statin_glp1_dates = get_dual_index_dates(statin_glp1, dataset_28107006_drug_df, statin_pattern, glp1_pattern, 'Statin_GLP1')
statin_metformin_dates = get_dual_index_dates(statin_metformin, dataset_28107006_drug_df, statin_pattern, metformin_pattern, 'Statin_Metformin')
metformin_glp1_dates = get_dual_index_dates(metformin_glp1, dataset_28107006_drug_df, metformin_pattern, glp1_pattern, 'Metformin_GLP1')

# For triple therapy, use when all three started (max of all three)
def get_triple_index_dates(person_ids, drug_df, pattern1, pattern2, pattern3, group_name):
    dates_list = []
    for pid in person_ids:
        person_drugs = drug_df[drug_df['person_id'] == pid]
        drug1_start = person_drugs[
            person_drugs['standard_concept_name'].str.contains(pattern1, case=False, na=False)
        ]['drug_exposure_start_datetime'].min()
        drug2_start = person_drugs[
            person_drugs['standard_concept_name'].str.contains(pattern2, case=False, na=False)
        ]['drug_exposure_start_datetime'].min()
        drug3_start = person_drugs[
            person_drugs['standard_concept_name'].str.contains(pattern3, case=False, na=False)
        ]['drug_exposure_start_datetime'].min()
        if pd.notna(drug1_start) and pd.notna(drug2_start) and pd.notna(drug3_start):
            index_date = max(drug1_start, drug2_start, drug3_start)
            dates_list.append({
                'person_id': pid,
                'index_date': index_date,
                'group': group_name,
                'statin_start': drug1_start,
                'metformin_start': drug2_start,
                'glp1_start': drug3_start
            })
    return pd.DataFrame(dates_list)

statin_glp1_met_dates = get_triple_index_dates(statin_glp1_metformin, dataset_28107006_drug_df,
                                                 statin_pattern, metformin_pattern, glp1_pattern,
                                                 'Statin_GLP1_Metformin')

# Combine all groups
all_groups = pd.concat([
    statin_mono_dates,
    metformin_mono_dates,
    glp1_mono_dates,
    statin_glp1_dates[['person_id', 'index_date', 'group']],
    statin_metformin_dates[['person_id', 'index_date', 'group']],
    metformin_glp1_dates[['person_id', 'index_date', 'group']],
    statin_glp1_met_dates[['person_id', 'index_date', 'group']]
], ignore_index=True)

print(f"\nIndex dates computed for {len(all_groups)} patients")

all_groups = all_groups.merge(
    dataset_28107006_person_df[['person_id', 'sex_at_birth', 'race', 'ethnicity', 'date_of_birth']],
    on='person_id', how='left'
)

all_groups['index_date'] = pd.to_datetime(all_groups['index_date'])
all_groups['date_of_birth'] = pd.to_datetime(all_groups['date_of_birth'])
all_groups['age_at_index'] = (all_groups['index_date'] - all_groups['date_of_birth']).dt.days / 365.25

# Merge measurements and time-restrict
measurements_groups = dataset_28107006_measurement_df.merge(all_groups, on='person_id', how='inner')
measurements_groups['measurement_datetime'] = pd.to_datetime(measurements_groups['measurement_datetime'])
measurements_groups['days_from_index'] = (measurements_groups['measurement_datetime'] - measurements_groups['index_date']).dt.days

# Time windows from concept_sets.py
# PRE_WINDOW_DAYS = 365, POST_WINDOW_DAYS = 365 (imported)

measurements_restricted = measurements_groups[
    ((measurements_groups['days_from_index'] >= -PRE_WINDOW_DAYS) & (measurements_groups['days_from_index'] < 0)) |
    ((measurements_groups['days_from_index'] > 0) & (measurements_groups['days_from_index'] <= POST_WINDOW_DAYS))
].copy()
measurements_restricted['timing'] = measurements_restricted['days_from_index'].apply(lambda x: 'pre' if x < 0 else 'post')

# Merge smoking status into all_groups for downstream use as covariate
all_groups = all_groups.merge(smoking_df, on='person_id', how='left')
all_groups['smoking_status'] = all_groups['smoking_status'].fillna('Unknown')

# ============================================================================
# SECTION 3: HELPER FUNCTIONS
# ============================================================================

def bootstrap_ci(data, n_bootstrap=1000, ci=95, func=np.median):
    data = np.array(data)
    data = data[~np.isnan(data)]
    if len(data) < 2:
        return np.nan, np.nan
    boot_stats = [func(np.random.choice(data, size=len(data), replace=True)) for _ in range(n_bootstrap)]
    alpha = (100 - ci) / 2
    return np.percentile(boot_stats, alpha), np.percentile(boot_stats, 100 - alpha)

def calculate_se_median(data):
    data = np.array(data)
    data = data[~np.isnan(data)]
    if len(data) < 2:
        return np.nan
    boot_medians = [np.median(np.random.choice(data, size=len(data), replace=True)) for _ in range(1000)]
    return np.std(boot_medians)

def calculate_stats(data):
    data = np.array(data)
    data = data[~np.isnan(data)]
    n = len(data)
    if n == 0:
        return {'n': 0, 'mean': np.nan, 'median': np.nan, 'sd': np.nan,
                'se_mean': np.nan, 'se_median': np.nan, 'ci_lower': np.nan, 'ci_upper': np.nan}
    mean, median = np.mean(data), np.median(data)
    sd = np.std(data, ddof=1) if n > 1 else np.nan
    se_mean = sd / np.sqrt(n) if n > 1 else np.nan
    se_median = calculate_se_median(data)
    ci_lower, ci_upper = bootstrap_ci(data)
    return {'n': n, 'mean': mean, 'median': median, 'sd': sd,
            'se_mean': se_mean, 'se_median': se_median, 'ci_lower': ci_lower, 'ci_upper': ci_upper}

def bootstrap_difference_ci(data1, data2, n_bootstrap=1000, ci=95):
    data1, data2 = np.array(data1), np.array(data2)
    data1, data2 = data1[~np.isnan(data1)], data2[~np.isnan(data2)]
    if len(data1) < 2 or len(data2) < 2:
        return (np.nan, np.nan)
    boot_diffs = [np.median(np.random.choice(data1, len(data1), replace=True)) -
                  np.median(np.random.choice(data2, len(data2), replace=True)) for _ in range(n_bootstrap)]
    alpha = (100 - ci) / 2
    return (np.percentile(boot_diffs, alpha), np.percentile(boot_diffs, 100 - alpha))

# ============================================================================
# SECTION 4: CREATE PRE/POST DATASET
# ============================================================================

print("\n" + "="*100)
print("SECTION 4: CREATING PRE/POST DATASET")
print("="*100)

# GLYCEMIC_OUTCOMES, LIPID_OUTCOMES, ALL_OUTCOMES imported from concept_sets.py
# NOTE: Inflammatory markers (CRP, ESR, Fibrinogen) are excluded per study protocol.
# TREATMENT_GROUPS imported from concept_sets.py
GROUPS = TREATMENT_GROUPS

# ============================================================================
# SECTION 4A: MEASUREMENT TIMING RULES (Requirement D)
# ============================================================================
# Rule: Pre and post measurements must be separated by at least
# MIN_PRE_POST_GAP_DAYS (default 30 days, or measure-specific override).
#
# For each person x measure, the closest-to-index pre measurement and
# the closest-to-index post measurement are identified. If the gap between
# them is < the required minimum, the pair is excluded.
#
# Measure-specific overrides (from concept_sets.py MEASURE_SPECIFIC_MIN_GAP):
#   HbA1c: 90 days (reflects ~3-month average)
#   Others: 30 days (default)
# ============================================================================

print("\n" + "-"*80)
print("SECTION 4A: MEASUREMENT TIMING RULES (Pre/Post Gap Enforcement)")
print("-"*80)

print(f"\n  Default minimum pre-post gap: {MIN_PRE_POST_GAP_DAYS} days")
print(f"  Measure-specific overrides:")
for measure, gap in MEASURE_SPECIFIC_MIN_GAP.items():
    print(f"    {measure:<25}: {gap} days")

# For each person x measure, find closest pre and closest post to index date
# then compute gap. Exclude if gap < required minimum.
timing_exclusion_records = []
timing_gap_distribution = []

pre_measurements = measurements_restricted[measurements_restricted['timing'] == 'pre'].copy()
post_measurements = measurements_restricted[measurements_restricted['timing'] == 'post'].copy()

# Get closest pre (max days_from_index, i.e., closest to 0 but negative)
closest_pre = pre_measurements.sort_values('days_from_index', ascending=False).groupby(
    ['person_id', 'measurement_category']
).first().reset_index()[['person_id', 'measurement_category', 'days_from_index', 'value_as_number']]
closest_pre.columns = ['person_id', 'measurement_category', 'pre_days_from_index', 'pre_value']

# Get closest post (min days_from_index, i.e., closest to 0 but positive)
closest_post = post_measurements.sort_values('days_from_index').groupby(
    ['person_id', 'measurement_category']
).first().reset_index()[['person_id', 'measurement_category', 'days_from_index', 'value_as_number']]
closest_post.columns = ['person_id', 'measurement_category', 'post_days_from_index', 'post_value']

timing_pairs = closest_pre.merge(closest_post, on=['person_id', 'measurement_category'], how='inner')
timing_pairs['gap_days'] = timing_pairs['post_days_from_index'] - timing_pairs['pre_days_from_index']

# Apply measure-specific minimum gap
timing_pairs['required_gap'] = timing_pairs['measurement_category'].map(
    MEASURE_SPECIFIC_MIN_GAP
).fillna(MIN_PRE_POST_GAP_DAYS)

timing_pairs['meets_timing_rule'] = timing_pairs['gap_days'] >= timing_pairs['required_gap']

# Summary table
print(f"\n  Timing Rule Summary:")
print(f"  {'Measure':<25} {'Total Pairs':>12} {'Excluded':>10} {'% Excluded':>12} {'Median Gap':>12} {'Min Gap':>10}")
print("  " + "-"*90)

for measure in ALL_OUTCOMES + ['BMI']:
    m_pairs = timing_pairs[timing_pairs['measurement_category'] == measure]
    if len(m_pairs) == 0:
        continue
    n_total = len(m_pairs)
    n_excluded = (~m_pairs['meets_timing_rule']).sum()
    pct_excluded = n_excluded / n_total * 100 if n_total > 0 else 0
    median_gap = m_pairs['gap_days'].median()
    min_gap_required = MEASURE_SPECIFIC_MIN_GAP.get(measure, MIN_PRE_POST_GAP_DAYS)

    print(f"  {measure:<25} {n_total:>12} {n_excluded:>10} {pct_excluded:>11.1f}% {median_gap:>12.0f} {min_gap_required:>10}")

    timing_exclusion_records.append({
        'measure': measure,
        'n_total_pairs': n_total,
        'n_excluded': n_excluded,
        'pct_excluded': pct_excluded,
        'median_gap_days': median_gap,
        'required_gap_days': min_gap_required,
    })

timing_exclusion_df = pd.DataFrame(timing_exclusion_records)

# Distribution of measurement gaps
print(f"\n  Gap Distribution (days) by Measure:")
print(f"  {'Measure':<25} {'P10':>8} {'P25':>8} {'Median':>8} {'P75':>8} {'P90':>8}")
print("  " + "-"*60)
for measure in ALL_OUTCOMES + ['BMI']:
    m_pairs = timing_pairs[timing_pairs['measurement_category'] == measure]
    if len(m_pairs) >= 5:
        gaps = m_pairs['gap_days']
        print(f"  {measure:<25} {gaps.quantile(0.10):>8.0f} {gaps.quantile(0.25):>8.0f} "
              f"{gaps.median():>8.0f} {gaps.quantile(0.75):>8.0f} {gaps.quantile(0.90):>8.0f}")

# Build set of (person_id, measurement_category) pairs that fail the timing rule
failed_timing = set(
    zip(
        timing_pairs[~timing_pairs['meets_timing_rule']]['person_id'],
        timing_pairs[~timing_pairs['meets_timing_rule']]['measurement_category']
    )
)

# ============================================================================
# Build pre/post dataset (with timing rule enforcement)
# ============================================================================

pre_post_data = measurements_restricted[
    measurements_restricted['measurement_category'].isin(ALL_OUTCOMES)
].groupby(
    ['person_id', 'group', 'measurement_category', 'timing', 'sex_at_birth', 'race', 'ethnicity', 'age_at_index']
)['value_as_number'].median().reset_index()

pre_post_wide = pre_post_data.pivot_table(
    index=['person_id', 'group', 'measurement_category', 'sex_at_birth', 'race', 'ethnicity', 'age_at_index'],
    columns='timing',
    values='value_as_number'
).reset_index()

pre_post_wide['change'] = pre_post_wide['post'] - pre_post_wide['pre']
pre_post_complete = pre_post_wide.dropna(subset=['pre', 'post'])

# Enforce timing rule: remove pairs that do not meet minimum gap
n_before_timing = len(pre_post_complete)
pre_post_complete['_timing_key'] = list(zip(pre_post_complete['person_id'], pre_post_complete['measurement_category']))
pre_post_complete = pre_post_complete[~pre_post_complete['_timing_key'].isin(failed_timing)].copy()
pre_post_complete.drop(columns=['_timing_key'], inplace=True)
n_after_timing = len(pre_post_complete)

print(f"\n  Pre/post pairs removed by timing rule: {n_before_timing - n_after_timing}")
print(f"  Pre/post pairs remaining: {n_after_timing}")

np.random.seed(42)

print(f"\nComplete pre/post rows: {len(pre_post_complete)}")
print(f"Unique patients with >=1 complete pre/post outcome: {pre_post_complete['person_id'].nunique()}")

print(f"\nPatients per group with complete pre/post data:")
for group in GROUPS:
    n = pre_post_complete[pre_post_complete['group'] == group]['person_id'].nunique()
    print(f"  {group:<25}: {n:>6}")

# ============================================================================
# SECTION 4B: SMOKING QA TABLE BY EXPOSURE GROUP (Requirement C continued)
# ============================================================================

print("\n" + "-"*80)
print("SECTION 4B: SMOKING QA TABLE BY EXPOSURE GROUP")
print("-"*80)

# Merge smoking status into pre_post dataset for downstream analyses
pre_post_complete = pre_post_complete.merge(
    smoking_df, on='person_id', how='left'
)
pre_post_complete['smoking_status'] = pre_post_complete['smoking_status'].fillna('Unknown')

print(f"\n{'Group':<25} {'Smoker':>10} {'Non_Smoker':>12} {'Unknown':>10} {'Total':>8} {'%Smoker':>10}")
print("-"*80)

for group in GROUPS:
    group_data = all_groups[all_groups['group'] == group]
    g_smoke = group_data.merge(smoking_df, on='person_id', how='left')
    g_smoke['smoking_status'] = g_smoke['smoking_status'].fillna('Unknown')

    n_smoker = (g_smoke['smoking_status'] == 'Smoker').sum()
    n_non = (g_smoke['smoking_status'] == 'Non_Smoker').sum()
    n_unk = (g_smoke['smoking_status'] == 'Unknown').sum()
    n_total = len(g_smoke)
    pct_smoker = n_smoker / n_total * 100 if n_total > 0 else 0

    print(f"{group:<25} {n_smoker:>10} {n_non:>12} {n_unk:>10} {n_total:>8} {pct_smoker:>9.1f}%")

# BMI missing by exposure group
print(f"\n{'Group':<25} {'BMI Available':>15} {'BMI Missing':>12} {'%Missing':>10}")
print("-"*65)

for group in GROUPS:
    group_pids = all_groups[all_groups['group'] == group]['person_id']
    bmi_available = bmi_rows[bmi_rows['person_id'].isin(group_pids)]['person_id'].nunique()
    bmi_missing = len(group_pids) - bmi_available
    pct_missing = bmi_missing / len(group_pids) * 100 if len(group_pids) > 0 else 0
    print(f"{group:<25} {bmi_available:>15} {bmi_missing:>12} {pct_missing:>9.1f}%")

# ============================================================================
# ============================================================================
# NOVEL ANALYSIS 1: TREATMENT SEQUENCING / INTENSIFICATION PATTERNS
# ============================================================================
# ============================================================================
# Question: Does the order of adding medications matter?
# - Compare: Metformin -> add GLP-1 vs GLP-1 -> add Metformin
# - Compare: Early triple therapy vs stepwise escalation
# Why novel: Guidelines say "start metformin first" but real-world sequencing varies
# ============================================================================

print("\n" + "="*100)
print("NOVEL ANALYSIS 1: TREATMENT SEQUENCING / INTENSIFICATION PATTERNS")
print("="*100)
print("\nResearch Question: Does the order of adding medications matter?")
print("Clinical Relevance: Guidelines say 'start metformin first' but real-world sequencing varies.")
print("No one has shown whether early GLP-1 initiation leads to better outcomes.\n")

# -----------------------------------------------------------------------------
# 1A. Create detailed drug sequencing data for combination therapy patients
# -----------------------------------------------------------------------------

print("-"*80)
print("1A. EXTRACTING DETAILED DRUG SEQUENCING DATA")
print("-"*80)

# For Metformin + GLP-1 patients: determine which came first
sequencing_met_glp1 = []

for pid in metformin_glp1:
    person_drugs = dataset_28107006_drug_df[dataset_28107006_drug_df['person_id'] == pid]

    met_start = person_drugs[
        person_drugs['standard_concept_name'].str.contains(metformin_pattern, case=False, na=False)
    ]['drug_exposure_start_datetime'].min()

    glp1_start = person_drugs[
        person_drugs['standard_concept_name'].str.contains(glp1_pattern, case=False, na=False)
    ]['drug_exposure_start_datetime'].min()

    if pd.notna(met_start) and pd.notna(glp1_start):
        days_between = (glp1_start - met_start).days

        if days_between > 30:  # GLP-1 added after metformin (>30 days gap)
            sequence = 'Metformin_First'
            first_drug = 'Metformin'
            intensification_date = glp1_start
            time_to_intensification = days_between
        elif days_between < -30:  # Metformin added after GLP-1 (>30 days gap)
            sequence = 'GLP1_First'
            first_drug = 'GLP-1'
            intensification_date = met_start
            time_to_intensification = abs(days_between)
        else:  # Started within 30 days of each other
            sequence = 'Concurrent'
            first_drug = 'Both'
            intensification_date = max(met_start, glp1_start)
            time_to_intensification = 0

        sequencing_met_glp1.append({
            'person_id': pid,
            'metformin_start': met_start,
            'glp1_start': glp1_start,
            'days_between': days_between,
            'sequence': sequence,
            'first_drug': first_drug,
            'intensification_date': intensification_date,
            'time_to_intensification_days': time_to_intensification
        })

sequencing_met_glp1_df = pd.DataFrame(sequencing_met_glp1)

if len(sequencing_met_glp1_df) > 0:
    print(f"\nMetformin + GLP-1 Combination ({len(sequencing_met_glp1_df)} patients):")
    print(f"\n  Sequencing Pattern Distribution:")
    seq_counts = sequencing_met_glp1_df['sequence'].value_counts()
    for seq, count in seq_counts.items():
        pct = count / len(sequencing_met_glp1_df) * 100
        print(f"    {seq:<20}: {count:>5} ({pct:>5.1f}%)")

    print(f"\n  Time to Intensification (days) by Sequence:")
    for seq in ['Metformin_First', 'GLP1_First']:
        seq_data = sequencing_met_glp1_df[sequencing_met_glp1_df['sequence'] == seq]['time_to_intensification_days']
        if len(seq_data) >= 5:
            print(f"    {seq:<20}: Median = {seq_data.median():.0f}, IQR = [{seq_data.quantile(0.25):.0f}, {seq_data.quantile(0.75):.0f}]")

# For Triple Therapy patients: determine sequencing pattern
sequencing_triple = []

for pid in statin_glp1_metformin:
    person_drugs = dataset_28107006_drug_df[dataset_28107006_drug_df['person_id'] == pid]

    statin_start = person_drugs[
        person_drugs['standard_concept_name'].str.contains(statin_pattern, case=False, na=False)
    ]['drug_exposure_start_datetime'].min()

    met_start = person_drugs[
        person_drugs['standard_concept_name'].str.contains(metformin_pattern, case=False, na=False)
    ]['drug_exposure_start_datetime'].min()

    glp1_start = person_drugs[
        person_drugs['standard_concept_name'].str.contains(glp1_pattern, case=False, na=False)
    ]['drug_exposure_start_datetime'].min()

    if pd.notna(statin_start) and pd.notna(met_start) and pd.notna(glp1_start):
        # Determine order
        starts = {'Statin': statin_start, 'Metformin': met_start, 'GLP1': glp1_start}
        sorted_starts = sorted(starts.items(), key=lambda x: x[1])

        first_drug = sorted_starts[0][0]
        second_drug = sorted_starts[1][0]
        third_drug = sorted_starts[2][0]

        first_date = sorted_starts[0][1]
        second_date = sorted_starts[1][1]
        third_date = sorted_starts[2][1]

        days_first_to_second = (second_date - first_date).days
        days_second_to_third = (third_date - second_date).days
        days_first_to_third = (third_date - first_date).days

        # Classify pattern
        if days_first_to_third <= 90:  # All within 90 days
            pattern = 'Early_Triple'
        else:
            pattern = 'Stepwise_Escalation'

        # Determine if metformin was first (guideline-concordant)
        guideline_concordant = first_drug == 'Metformin'

        sequencing_triple.append({
            'person_id': pid,
            'statin_start': statin_start,
            'metformin_start': met_start,
            'glp1_start': glp1_start,
            'first_drug': first_drug,
            'second_drug': second_drug,
            'third_drug': third_drug,
            'sequence_order': f"{first_drug}->{second_drug}->{third_drug}",
            'days_first_to_second': days_first_to_second,
            'days_second_to_third': days_second_to_third,
            'days_first_to_third': days_first_to_third,
            'pattern': pattern,
            'guideline_concordant': guideline_concordant
        })

sequencing_triple_df = pd.DataFrame(sequencing_triple)

if len(sequencing_triple_df) > 0:
    print(f"\nTriple Therapy ({len(sequencing_triple_df)} patients):")
    print(f"\n  Escalation Pattern Distribution:")
    pattern_counts = sequencing_triple_df['pattern'].value_counts()
    for pattern, count in pattern_counts.items():
        pct = count / len(sequencing_triple_df) * 100
        print(f"    {pattern:<25}: {count:>5} ({pct:>5.1f}%)")

    print(f"\n  First Drug Distribution:")
    first_counts = sequencing_triple_df['first_drug'].value_counts()
    for drug, count in first_counts.items():
        pct = count / len(sequencing_triple_df) * 100
        print(f"    {drug:<25}: {count:>5} ({pct:>5.1f}%)")

    print(f"\n  Guideline Concordance (Metformin First):")
    concordant = sequencing_triple_df['guideline_concordant'].sum()
    pct_concordant = concordant / len(sequencing_triple_df) * 100
    print(f"    Concordant:     {concordant:>5} ({pct_concordant:.1f}%)")
    print(f"    Non-concordant: {len(sequencing_triple_df) - concordant:>5} ({100 - pct_concordant:.1f}%)")

    print(f"\n  Most Common Sequence Orders:")
    seq_order_counts = sequencing_triple_df['sequence_order'].value_counts().head(10)
    for order, count in seq_order_counts.items():
        pct = count / len(sequencing_triple_df) * 100
        print(f"    {order:<30}: {count:>5} ({pct:>5.1f}%)")

# -----------------------------------------------------------------------------
# 1B. Outcomes by Treatment Sequencing (Metformin + GLP-1)
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("1B. OUTCOMES BY TREATMENT SEQUENCING (Metformin + GLP-1 Group)")
print("-"*80)

if len(sequencing_met_glp1_df) > 0:
    # Merge sequencing data with pre/post outcomes
    met_glp1_outcomes = pre_post_complete[
        pre_post_complete['group'] == 'Metformin_GLP1'
    ].merge(sequencing_met_glp1_df[['person_id', 'sequence']], on='person_id', how='inner')

    sequencing_results = []

    print(f"\nComparing outcomes by drug sequence:")
    print(f"{'Outcome':<20} {'Sequence':<20} {'n':>6} {'Pre':>10} {'Post':>10} {'Delta':>10} {'95% CI':>22} {'p-value':>10}")
    print("-"*115)

    for outcome in ALL_OUTCOMES:
        outcome_data = met_glp1_outcomes[met_glp1_outcomes['measurement_category'] == outcome]

        if len(outcome_data) < 10:
            continue

        for sequence in ['Metformin_First', 'GLP1_First', 'Concurrent']:
            seq_data = outcome_data[outcome_data['sequence'] == sequence]

            if len(seq_data) >= 5:
                pre_vals = seq_data['pre'].dropna().values
                post_vals = seq_data['post'].dropna().values
                change_vals = seq_data['change'].dropna().values

                change_stats = calculate_stats(change_vals)

                try:
                    _, pval = stats.wilcoxon(pre_vals, post_vals)
                    p_str = f"{pval:.4f}" if pval >= 0.0001 else "<0.0001"
                    sig = "*" if pval < 0.05 else ""
                except:
                    pval, p_str, sig = np.nan, "N/A", ""

                ci_str = f"[{change_stats['ci_lower']:+.2f}, {change_stats['ci_upper']:+.2f}]"

                print(f"{outcome:<20} {sequence:<20} {change_stats['n']:>6} {np.median(pre_vals):>10.2f} "
                      f"{np.median(post_vals):>10.2f} {change_stats['median']:>+10.2f} {ci_str:>22} {p_str:>9}{sig}")

                sequencing_results.append({
                    'comparison': 'Met_GLP1_Sequencing',
                    'outcome': outcome,
                    'sequence': sequence,
                    'n': change_stats['n'],
                    'pre_median': np.median(pre_vals),
                    'post_median': np.median(post_vals),
                    'change_median': change_stats['median'],
                    'ci_lower': change_stats['ci_lower'],
                    'ci_upper': change_stats['ci_upper'],
                    'p_value': pval
                })

    # Statistical comparison between sequences
    print(f"\n  Between-Sequence Comparisons (Metformin_First vs GLP1_First):")
    print(f"  {'Outcome':<20} {'n_MetFirst':>10} {'n_GLP1First':>11} {'Delta_Diff':>12} {'95% CI':>22} {'p-value':>10}")
    print("  " + "-"*95)

    for outcome in ALL_OUTCOMES:
        outcome_data = met_glp1_outcomes[met_glp1_outcomes['measurement_category'] == outcome]

        met_first = outcome_data[outcome_data['sequence'] == 'Metformin_First']['change'].dropna().values
        glp1_first = outcome_data[outcome_data['sequence'] == 'GLP1_First']['change'].dropna().values

        if len(met_first) >= 5 and len(glp1_first) >= 5:
            try:
                _, p = stats.mannwhitneyu(met_first, glp1_first, alternative='two-sided')
                diff = np.median(met_first) - np.median(glp1_first)
                ci_low, ci_high = bootstrap_difference_ci(met_first, glp1_first)

                p_str = f"{p:.4f}" if p >= 0.0001 else "<0.0001"
                sig = "*" if p < 0.05 else ""

                print(f"  {outcome:<20} {len(met_first):>10} {len(glp1_first):>11} {diff:>+12.2f} "
                      f"[{ci_low:+.2f}, {ci_high:+.2f}] {p_str:>9}{sig}")
            except:
                pass

    sequencing_results_df = pd.DataFrame(sequencing_results)

# -----------------------------------------------------------------------------
# 1C. Outcomes by Escalation Pattern (Triple Therapy)
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("1C. OUTCOMES BY ESCALATION PATTERN (Triple Therapy)")
print("-"*80)

if len(sequencing_triple_df) > 0:
    # Merge with outcomes
    triple_outcomes = pre_post_complete[
        pre_post_complete['group'] == 'Statin_GLP1_Metformin'
    ].merge(sequencing_triple_df[['person_id', 'pattern', 'guideline_concordant', 'first_drug']],
            on='person_id', how='inner')

    escalation_results = []

    # A) Early Triple vs Stepwise Escalation
    print(f"\nA) Early Triple Therapy vs Stepwise Escalation:")
    print(f"{'Outcome':<20} {'Pattern':<25} {'n':>6} {'Pre':>10} {'Post':>10} {'Delta':>10} {'95% CI':>22} {'p-value':>10}")
    print("-"*120)

    for outcome in ALL_OUTCOMES:
        outcome_data = triple_outcomes[triple_outcomes['measurement_category'] == outcome]

        if len(outcome_data) < 10:
            continue

        for pattern in ['Early_Triple', 'Stepwise_Escalation']:
            pattern_data = outcome_data[outcome_data['pattern'] == pattern]

            if len(pattern_data) >= 5:
                pre_vals = pattern_data['pre'].dropna().values
                post_vals = pattern_data['post'].dropna().values
                change_vals = pattern_data['change'].dropna().values

                change_stats = calculate_stats(change_vals)

                try:
                    _, pval = stats.wilcoxon(pre_vals, post_vals)
                    p_str = f"{pval:.4f}" if pval >= 0.0001 else "<0.0001"
                    sig = "*" if pval < 0.05 else ""
                except:
                    pval, p_str, sig = np.nan, "N/A", ""

                ci_str = f"[{change_stats['ci_lower']:+.2f}, {change_stats['ci_upper']:+.2f}]"

                print(f"{outcome:<20} {pattern:<25} {change_stats['n']:>6} {np.median(pre_vals):>10.2f} "
                      f"{np.median(post_vals):>10.2f} {change_stats['median']:>+10.2f} {ci_str:>22} {p_str:>9}{sig}")

                escalation_results.append({
                    'comparison': 'Escalation_Pattern',
                    'outcome': outcome,
                    'pattern': pattern,
                    'n': change_stats['n'],
                    'pre_median': np.median(pre_vals),
                    'post_median': np.median(post_vals),
                    'change_median': change_stats['median'],
                    'ci_lower': change_stats['ci_lower'],
                    'ci_upper': change_stats['ci_upper'],
                    'p_value': pval
                })

    # B) Guideline Concordance (Metformin First vs Not)
    print(f"\nB) Guideline Concordance (Metformin First):")
    print(f"{'Outcome':<20} {'Concordant':>12} {'n':>6} {'Pre':>10} {'Post':>10} {'Delta':>10} {'95% CI':>22} {'p-value':>10}")
    print("-"*115)

    for outcome in ALL_OUTCOMES:
        outcome_data = triple_outcomes[triple_outcomes['measurement_category'] == outcome]

        if len(outcome_data) < 10:
            continue

        for concordant in [True, False]:
            conc_data = outcome_data[outcome_data['guideline_concordant'] == concordant]
            conc_label = "Yes" if concordant else "No"

            if len(conc_data) >= 5:
                pre_vals = conc_data['pre'].dropna().values
                post_vals = conc_data['post'].dropna().values
                change_vals = conc_data['change'].dropna().values

                change_stats = calculate_stats(change_vals)

                try:
                    _, pval = stats.wilcoxon(pre_vals, post_vals)
                    p_str = f"{pval:.4f}" if pval >= 0.0001 else "<0.0001"
                    sig = "*" if pval < 0.05 else ""
                except:
                    pval, p_str, sig = np.nan, "N/A", ""

                ci_str = f"[{change_stats['ci_lower']:+.2f}, {change_stats['ci_upper']:+.2f}]"

                print(f"{outcome:<20} {conc_label:>12} {change_stats['n']:>6} {np.median(pre_vals):>10.2f} "
                      f"{np.median(post_vals):>10.2f} {change_stats['median']:>+10.2f} {ci_str:>22} {p_str:>9}{sig}")

    escalation_results_df = pd.DataFrame(escalation_results)

# -----------------------------------------------------------------------------
# 1D. Summary Statistics for Sequencing Analysis
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("1D. SUMMARY: TREATMENT SEQUENCING PATTERNS")
print("-"*80)

print("\n" + "="*80)
print("KEY FINDINGS - TREATMENT SEQUENCING ANALYSIS")
print("="*80)
print("""
This analysis examines whether the ORDER of starting medications matters:

1. Metformin + GLP-1 Dual Therapy:
   - Compares patients who started Metformin first vs GLP-1 first vs concurrent
   - Guidelines recommend metformin as first-line, but real-world varies
   - Novel insight: Does early GLP-1 lead to better glycemic/metabolic outcomes?

2. Triple Therapy Escalation:
   - Early Triple (all 3 within 90 days) vs Stepwise Escalation
   - Guideline concordance: Was metformin started first?
   - Novel insight: Is aggressive early combination better than stepwise?

Clinical Implications:
- If GLP-1 first shows better outcomes, may challenge guidelines
- If early triple shows benefits, supports more aggressive initial therapy
- Identifies real-world prescribing patterns vs guideline recommendations
""")

# ============================================================================
# ============================================================================
# NOVEL ANALYSIS 2: TIME-TO-GLYCEMIC CONTROL (SURVIVAL ANALYSIS)
# ============================================================================
# ============================================================================
# Question: How quickly do patients achieve HbA1c <7% across treatment groups?
# Method: Kaplan-Meier survival analysis, Cox proportional hazards regression
# Why novel: Most studies report magnitude of change, not SPEED of response
# Clinically relevant for patient counseling ("how long until I see results?")
# ============================================================================

print("\n" + "="*100)
print("NOVEL ANALYSIS 2: TIME-TO-GLYCEMIC CONTROL (SURVIVAL ANALYSIS)")
print("="*100)
print("\nResearch Question: How quickly do patients achieve HbA1c <7% across treatment groups?")
print("Clinical Relevance: Most studies report magnitude of change, not speed.")
print("This answers: 'How long until I see results?' - critical for patient counseling.\n")

# -----------------------------------------------------------------------------
# 2A. Prepare Time-to-Event Data for HbA1c Control
# -----------------------------------------------------------------------------

print("-"*80)
print("2A. PREPARING TIME-TO-EVENT DATA")
print("-"*80)

# Get all HbA1c measurements for patients in our cohort
hba1c_measurements = measurements_groups[
    measurements_groups['measurement_category'] == 'HbA1c'
].copy()

# Target: HbA1c < 7.0% (HBA1C_TARGET imported from concept_sets.py)

# Create time-to-event dataset
time_to_control_data = []

for person_id in all_groups['person_id'].unique():
    person_info = all_groups[all_groups['person_id'] == person_id].iloc[0]
    group = person_info['group']
    index_date = person_info['index_date']

    # Get all HbA1c measurements AFTER index date
    person_hba1c = hba1c_measurements[
        (hba1c_measurements['person_id'] == person_id) &
        (hba1c_measurements['measurement_datetime'] > index_date)
    ].sort_values('measurement_datetime')

    if len(person_hba1c) == 0:
        continue

    # Get baseline HbA1c (closest before or on index date within 365 days)
    baseline_hba1c = hba1c_measurements[
        (hba1c_measurements['person_id'] == person_id) &
        (hba1c_measurements['measurement_datetime'] <= index_date) &
        (hba1c_measurements['measurement_datetime'] >= index_date - pd.Timedelta(days=365))
    ]

    if len(baseline_hba1c) == 0:
        continue

    baseline_value = baseline_hba1c.sort_values('measurement_datetime', ascending=False).iloc[0]['value_as_number']

    # Only include patients with baseline HbA1c >= 7% (uncontrolled at start)
    if baseline_value < HBA1C_TARGET:
        continue

    # Find first time HbA1c < 7%
    controlled_measurements = person_hba1c[person_hba1c['value_as_number'] < HBA1C_TARGET]

    if len(controlled_measurements) > 0:
        # Event occurred - achieved control
        event_date = controlled_measurements.iloc[0]['measurement_datetime']
        time_to_event = (event_date - index_date).days
        event = 1
    else:
        # Censored - did not achieve control during follow-up
        last_measurement = person_hba1c.iloc[-1]['measurement_datetime']
        time_to_event = (last_measurement - index_date).days
        event = 0

    # Exclude if time_to_event is negative or too short
    if time_to_event < 30:
        continue

    # Cap at 2 years for analysis
    if time_to_event > 730:
        time_to_event = 730
        if event == 1:
            # Check if event occurred within 2 years
            controlled_within_2yr = person_hba1c[
                (person_hba1c['value_as_number'] < HBA1C_TARGET) &
                ((person_hba1c['measurement_datetime'] - index_date).dt.days <= 730)
            ]
            if len(controlled_within_2yr) == 0:
                event = 0

    time_to_control_data.append({
        'person_id': person_id,
        'group': group,
        'baseline_hba1c': baseline_value,
        'time_to_event_days': time_to_event,
        'time_to_event_months': time_to_event / 30.44,
        'event': event,  # 1 = achieved HbA1c <7%, 0 = censored
        'sex_at_birth': person_info['sex_at_birth'],
        'race': person_info['race'],
        'age_at_index': person_info['age_at_index']
    })

time_to_control_df = pd.DataFrame(time_to_control_data)

print(f"\nPatients with baseline HbA1c >= 7% and follow-up data: {len(time_to_control_df)}")
print(f"\nDistribution by treatment group:")
print(f"{'Group':<25} {'N':>6} {'Events':>8} {'Event Rate':>12} {'Median Time (mo)':>18}")
print("-"*75)

for group in GROUPS:
    group_data = time_to_control_df[time_to_control_df['group'] == group]
    n = len(group_data)
    if n >= 5:
        events = group_data['event'].sum()
        event_rate = events / n * 100
        median_time = group_data[group_data['event'] == 1]['time_to_event_months'].median() if events > 0 else np.nan
        median_str = f"{median_time:.1f}" if not np.isnan(median_time) else "N/A"
        print(f"{group:<25} {n:>6} {events:>8} {event_rate:>11.1f}% {median_str:>18}")

# -----------------------------------------------------------------------------
# 2B. Kaplan-Meier Survival Analysis
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("2B. KAPLAN-MEIER SURVIVAL ANALYSIS")
print("-"*80)

# Try to use lifelines for proper survival analysis, fallback to manual calculation
try:
    from lifelines import KaplanMeierFitter
    from lifelines.statistics import logrank_test
    LIFELINES_AVAILABLE = True
    print("\nUsing lifelines package for survival analysis")
except ImportError:
    LIFELINES_AVAILABLE = False
    print("\nlifelines not available - using manual Kaplan-Meier estimation")

km_results = []

if LIFELINES_AVAILABLE:
    kmf = KaplanMeierFitter()

    print(f"\nKaplan-Meier Estimates - Time to HbA1c <7%:")
    print(f"{'Group':<25} {'N':>6} {'Events':>8} {'Median (mo)':>12} {'95% CI':>20} {'6mo Prob':>10} {'12mo Prob':>11}")
    print("-"*100)

    for group in GROUPS:
        group_data = time_to_control_df[time_to_control_df['group'] == group]

        if len(group_data) >= 10:
            T = group_data['time_to_event_months'].values
            E = group_data['event'].values

            kmf.fit(T, event_observed=E, label=group)

            # Median survival time
            median_time = kmf.median_survival_time_
            ci = kmf.confidence_interval_median_survival_time_

            # Probability of achieving control at 6 and 12 months
            try:
                prob_6mo = 1 - kmf.survival_function_at_times(6).values[0]
                prob_12mo = 1 - kmf.survival_function_at_times(12).values[0]
            except:
                prob_6mo, prob_12mo = np.nan, np.nan

            median_str = f"{median_time:.1f}" if not np.isinf(median_time) and not np.isnan(median_time) else "NR"

            if hasattr(ci, 'iloc'):
                ci_low = ci.iloc[0, 0] if not np.isnan(ci.iloc[0, 0]) else "NR"
                ci_high = ci.iloc[0, 1] if not np.isnan(ci.iloc[0, 1]) else "NR"
                ci_str = f"[{ci_low}, {ci_high}]" if ci_low != "NR" else "[NR, NR]"
            else:
                ci_str = "[NR, NR]"

            print(f"{group:<25} {len(group_data):>6} {int(E.sum()):>8} {median_str:>12} {ci_str:>20} "
                  f"{prob_6mo*100:>9.1f}% {prob_12mo*100:>10.1f}%")

            km_results.append({
                'group': group,
                'n': len(group_data),
                'events': int(E.sum()),
                'median_months': median_time if not np.isinf(median_time) else np.nan,
                'prob_6mo': prob_6mo,
                'prob_12mo': prob_12mo
            })

    # Log-rank tests comparing groups
    print(f"\nLog-Rank Test Comparisons (vs Metformin_mono as reference):")
    print(f"{'Comparison':<45} {'Chi-sq':>10} {'p-value':>12}")
    print("-"*70)

    ref_data = time_to_control_df[time_to_control_df['group'] == 'Metformin_mono']
    if len(ref_data) >= 10:
        for group in GROUPS:
            if group != 'Metformin_mono':
                group_data = time_to_control_df[time_to_control_df['group'] == group]
                if len(group_data) >= 10:
                    try:
                        result = logrank_test(
                            ref_data['time_to_event_months'],
                            group_data['time_to_event_months'],
                            ref_data['event'],
                            group_data['event']
                        )
                        p_str = f"{result.p_value:.4f}" if result.p_value >= 0.0001 else "<0.0001"
                        sig = "*" if result.p_value < 0.05 else ""
                        print(f"{group} vs Metformin_mono{'':<20} {result.test_statistic:>10.2f} {p_str:>11}{sig}")
                    except:
                        pass

else:
    # Manual Kaplan-Meier estimation
    print(f"\nManual Kaplan-Meier Estimates - Time to HbA1c <7%:")
    print(f"{'Group':<25} {'N':>6} {'Events':>8} {'Event Rate':>12} {'Median Time (mo)':>18}")
    print("-"*75)

    for group in GROUPS:
        group_data = time_to_control_df[time_to_control_df['group'] == group]

        if len(group_data) >= 10:
            events = group_data['event'].sum()
            event_rate = events / len(group_data) * 100
            event_times = group_data[group_data['event'] == 1]['time_to_event_months']
            median_time = event_times.median() if len(event_times) > 0 else np.nan

            median_str = f"{median_time:.1f}" if not np.isnan(median_time) else "N/A"
            print(f"{group:<25} {len(group_data):>6} {events:>8} {event_rate:>11.1f}% {median_str:>18}")

            km_results.append({
                'group': group,
                'n': len(group_data),
                'events': events,
                'event_rate': event_rate,
                'median_months': median_time
            })

km_results_df = pd.DataFrame(km_results)

# -----------------------------------------------------------------------------
# 2C. Cox Proportional Hazards Regression
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("2C. COX PROPORTIONAL HAZARDS REGRESSION")
print("-"*80)

try:
    from lifelines import CoxPHFitter
    COX_AVAILABLE = True
except ImportError:
    COX_AVAILABLE = False

cox_results = []

if COX_AVAILABLE and len(time_to_control_df) >= 50:
    print("\nCox Regression: Hazard Ratios for Achieving HbA1c <7%")
    print("(Higher HR = faster time to control)")
    print("\nReference: Metformin_mono")

    # Prepare data for Cox regression
    cox_data = time_to_control_df.copy()

    # Create dummy variables for groups
    for group in GROUPS:
        if group != 'Metformin_mono':
            cox_data[f'group_{group}'] = (cox_data['group'] == group).astype(int)

    # Create dummy for sex
    cox_data['female'] = (cox_data['sex_at_birth'] == 'Female').astype(int)

    # Create race dummies
    cox_data['race_black'] = cox_data['race'].astype(str).str.contains('Black|African', case=False, na=False).astype(int)
    cox_data['race_white'] = cox_data['race'].astype(str).str.contains('White', case=False, na=False).astype(int)

    # Standardize continuous variables
    cox_data['age_std'] = (cox_data['age_at_index'] - cox_data['age_at_index'].mean()) / cox_data['age_at_index'].std()
    cox_data['baseline_hba1c_std'] = (cox_data['baseline_hba1c'] - cox_data['baseline_hba1c'].mean()) / cox_data['baseline_hba1c'].std()

    # Select columns for Cox model
    group_cols = [f'group_{g}' for g in GROUPS if g != 'Metformin_mono']
    covariate_cols = ['age_std', 'female', 'race_black', 'baseline_hba1c_std']
    model_cols = ['time_to_event_months', 'event'] + group_cols + covariate_cols

    cox_model_data = cox_data[model_cols].dropna()

    if len(cox_model_data) >= 50:
        cph = CoxPHFitter()
        try:
            cph.fit(cox_model_data, duration_col='time_to_event_months', event_col='event')

            print(f"\n{'Variable':<30} {'HR':>8} {'95% CI':>20} {'p-value':>12}")
            print("-"*75)

            for var in cph.params_.index:
                hr = np.exp(cph.params_[var])
                ci_low = np.exp(cph.confidence_intervals_.loc[var, '95% lower-bound'])
                ci_high = np.exp(cph.confidence_intervals_.loc[var, '95% upper-bound'])
                p_val = cph.summary['p'].loc[var]

                p_str = f"{p_val:.4f}" if p_val >= 0.0001 else "<0.0001"
                sig = "*" if p_val < 0.05 else ""

                # Clean up variable name for display
                display_name = var.replace('group_', '').replace('_std', '')

                print(f"{display_name:<30} {hr:>8.2f} [{ci_low:>6.2f}, {ci_high:>6.2f}] {p_str:>11}{sig}")

                cox_results.append({
                    'variable': display_name,
                    'HR': hr,
                    'HR_CI_lower': ci_low,
                    'HR_CI_upper': ci_high,
                    'p_value': p_val
                })

            print(f"\nModel Concordance Index: {cph.concordance_index_:.3f}")
            print(f"N = {len(cox_model_data)}, Events = {int(cox_model_data['event'].sum())}")

        except Exception as e:
            print(f"Cox regression failed: {e}")
    else:
        print("Insufficient data for Cox regression")
else:
    print("\nlifelines not available or insufficient data - skipping Cox regression")
    print("To enable: pip install lifelines")

cox_results_df = pd.DataFrame(cox_results)

# -----------------------------------------------------------------------------
# 2D. Time-to-Control by Sex (Stratified)
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("2D. TIME-TO-GLYCEMIC CONTROL BY SEX")
print("-"*80)

print(f"\n{'Group':<25} {'Sex':<10} {'N':>6} {'Events':>8} {'Event Rate':>12} {'Median (mo)':>14}")
print("-"*85)

for group in GROUPS:
    group_data = time_to_control_df[time_to_control_df['group'] == group]

    for sex in ['Male', 'Female']:
        sex_data = group_data[group_data['sex_at_birth'] == sex]

        if len(sex_data) >= 5:
            events = sex_data['event'].sum()
            event_rate = events / len(sex_data) * 100
            event_times = sex_data[sex_data['event'] == 1]['time_to_event_months']
            median_time = event_times.median() if len(event_times) > 0 else np.nan
            median_str = f"{median_time:.1f}" if not np.isnan(median_time) else "N/A"

            print(f"{group:<25} {sex:<10} {len(sex_data):>6} {events:>8} {event_rate:>11.1f}% {median_str:>14}")

# -----------------------------------------------------------------------------
# 2E. Summary
# -----------------------------------------------------------------------------

print("\n" + "="*80)
print("KEY FINDINGS - TIME-TO-GLYCEMIC CONTROL ANALYSIS")
print("="*80)
print("""
This survival analysis answers: "How quickly do patients achieve HbA1c <7%?"

Key Metrics:
- Median time to control: Time at which 50% of patients achieve HbA1c <7%
- Event rate: Proportion achieving control during follow-up
- Hazard Ratio: Relative speed of achieving control vs reference group
  (HR > 1 = faster control, HR < 1 = slower control)

Clinical Implications:
- Helps set patient expectations ("you may see results in X months")
- Identifies which treatment strategies lead to faster glycemic control
- May influence treatment choice for patients needing rapid improvement
- Sex-stratified results show if response speed differs by sex

Novel Contribution:
- Unlike typical studies reporting final HbA1c change, this shows TRAJECTORY
- Time-to-event analysis handles patients who don't reach goal (censoring)
- Provides actionable timeline information for clinical counseling
""")

# ============================================================================
# ============================================================================
# NOVEL ANALYSIS 3: TREATMENT RESPONSE HETEROGENEITY / PHENOTYPING
# ============================================================================
# ============================================================================
# Question: Can we identify patient phenotypes that respond better to GLP-1 vs Metformin?
# Method: K-means clustering on baseline characteristics, then compare treatment responses
# Why novel: Precision medicine approach - "Who benefits most from GLP-1?"
# ============================================================================

print("\n" + "="*100)
print("NOVEL ANALYSIS 3: TREATMENT RESPONSE HETEROGENEITY / PHENOTYPING")
print("="*100)
print("\nResearch Question: Can we identify patient phenotypes that respond differently to treatments?")
print("Clinical Relevance: Precision medicine - 'Who benefits most from GLP-1?'")
print("Method: Cluster patients by baseline characteristics, compare treatment responses.\n")

# -----------------------------------------------------------------------------
# 3A. Prepare Baseline Feature Matrix
# -----------------------------------------------------------------------------

print("-"*80)
print("3A. PREPARING BASELINE FEATURE MATRIX FOR CLUSTERING")
print("-"*80)

# Get baseline values for clustering features
# Features: age, baseline HbA1c, baseline LDL, baseline Triglycerides, baseline CRP, baseline BMI

# Create baseline dataset per patient
baseline_features = []

for person_id in all_groups['person_id'].unique():
    person_info = all_groups[all_groups['person_id'] == person_id].iloc[0]
    group = person_info['group']
    index_date = person_info['index_date']
    sex = person_info['sex_at_birth']
    race = person_info['race']
    age = person_info['age_at_index']

    # Get pre-treatment measurements (within 365 days before index)
    person_measurements = measurements_groups[
        (measurements_groups['person_id'] == person_id) &
        (measurements_groups['days_from_index'] >= -365) &
        (measurements_groups['days_from_index'] < 0)
    ]

    features = {
        'person_id': person_id,
        'group': group,
        'sex_at_birth': sex,
        'race': race,
        'age_at_index': age
    }

    # Get baseline values for each measure
    for measure in ['HbA1c', 'LDL', 'Triglycerides', 'HDL', 'Glucose']:
        measure_data = person_measurements[person_measurements['measurement_category'] == measure]
        if len(measure_data) > 0:
            # Use closest to index date
            features[f'baseline_{measure}'] = measure_data.sort_values('days_from_index', ascending=False).iloc[0]['value_as_number']
        else:
            features[f'baseline_{measure}'] = np.nan

    baseline_features.append(features)

baseline_features_df = pd.DataFrame(baseline_features)

# Get BMI for baseline (using centralized BMI_MEASUREMENT concept set)
bmi_data = dataset_28107006_measurement_df[
    (dataset_28107006_measurement_df['measurement_concept_id'].isin(BMI_MEASUREMENT['concept_ids'])) |
    (dataset_28107006_measurement_df['standard_concept_name'].astype(str).str.contains(
        BMI_MEASUREMENT['string_pattern'], case=False, na=False))
].copy()

bmi_baseline = []
for person_id in baseline_features_df['person_id'].unique():
    person_info = all_groups[all_groups['person_id'] == person_id].iloc[0]
    index_date = person_info['index_date']

    person_bmi = bmi_data[bmi_data['person_id'] == person_id].copy()
    if len(person_bmi) > 0:
        person_bmi['measurement_datetime'] = pd.to_datetime(person_bmi['measurement_datetime'])
        person_bmi['days_to_index'] = (person_bmi['measurement_datetime'] - index_date).dt.days

        baseline = person_bmi[
            (person_bmi['days_to_index'] <= 0) &
            (person_bmi['days_to_index'] >= -365)
        ].sort_values('days_to_index', ascending=False)

        if len(baseline) > 0:
            bmi_baseline.append({'person_id': person_id, 'baseline_BMI': baseline.iloc[0]['value_as_number']})

bmi_baseline_df = pd.DataFrame(bmi_baseline)
baseline_features_df = baseline_features_df.merge(bmi_baseline_df, on='person_id', how='left')

print(f"\nBaseline features extracted for {len(baseline_features_df)} patients")
print(f"\nFeature availability:")
for col in ['baseline_HbA1c', 'baseline_LDL', 'baseline_Triglycerides', 'baseline_HDL',
            'baseline_Glucose', 'baseline_BMI']:
    n_available = baseline_features_df[col].notna().sum()
    pct = n_available / len(baseline_features_df) * 100
    print(f"  {col:<25}: {n_available:>6} ({pct:>5.1f}%)")

# -----------------------------------------------------------------------------
# 3B. K-Means Clustering
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("3B. K-MEANS CLUSTERING OF PATIENT PHENOTYPES")
print("-"*80)

try:
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    from sklearn.impute import SimpleImputer
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

clustering_results = []
cluster_treatment_response = []

if SKLEARN_AVAILABLE:
    # Select features for clustering
    # NOTE: baseline_CRP removed -- inflammatory biomarkers excluded per protocol
    cluster_features = ['age_at_index', 'baseline_HbA1c', 'baseline_LDL',
                        'baseline_Triglycerides', 'baseline_BMI']

    # Prepare data - require at least 4 of 6 features
    cluster_data = baseline_features_df.copy()
    cluster_data['n_features'] = cluster_data[cluster_features].notna().sum(axis=1)
    cluster_data_complete = cluster_data[cluster_data['n_features'] >= 4].copy()

    print(f"\nPatients with >= 4 baseline features for clustering: {len(cluster_data_complete)}")

    if len(cluster_data_complete) >= 100:
        # Impute missing values
        X = cluster_data_complete[cluster_features].values
        imputer = SimpleImputer(strategy='median')
        X_imputed = imputer.fit_transform(X)

        # Standardize
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_imputed)

        # Determine optimal k using elbow method (simplified - just use k=3 or 4)
        # For clinical interpretability, 3-4 clusters is ideal
        n_clusters = 3

        # Fit K-means
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_data_complete['cluster'] = kmeans.fit_predict(X_scaled)

        print(f"\nK-Means clustering with k={n_clusters} clusters")
        print(f"\nCluster Distribution:")
        for cluster in range(n_clusters):
            n = (cluster_data_complete['cluster'] == cluster).sum()
            pct = n / len(cluster_data_complete) * 100
            print(f"  Cluster {cluster}: {n:>6} ({pct:>5.1f}%)")

        # Characterize clusters
        print(f"\nCluster Characteristics (Median values):")
        print(f"{'Cluster':<10} {'Age':>8} {'HbA1c':>8} {'LDL':>8} {'TG':>8} {'BMI':>8}")
        print("-"*55)

        cluster_profiles = []
        for cluster in range(n_clusters):
            cluster_subset = cluster_data_complete[cluster_data_complete['cluster'] == cluster]
            profile = {
                'cluster': cluster,
                'n': len(cluster_subset),
                'age': cluster_subset['age_at_index'].median(),
                'hba1c': cluster_subset['baseline_HbA1c'].median(),
                'ldl': cluster_subset['baseline_LDL'].median(),
                'tg': cluster_subset['baseline_Triglycerides'].median(),
                'bmi': cluster_subset['baseline_BMI'].median()
            }
            cluster_profiles.append(profile)

            print(f"  {cluster:<10} {profile['age']:>8.1f} {profile['hba1c']:>8.1f} {profile['ldl']:>8.1f} "
                  f"{profile['tg']:>8.1f} {profile['bmi']:>8.1f}")

        # Name clusters based on profiles
        cluster_profiles_df = pd.DataFrame(cluster_profiles)
        print("\n  Cluster Phenotype Interpretation:")

        for i, profile in enumerate(cluster_profiles):
            description = []
            if profile['hba1c'] > cluster_profiles_df['hba1c'].median():
                description.append("High HbA1c")
            else:
                description.append("Lower HbA1c")

            if profile['bmi'] > cluster_profiles_df['bmi'].median():
                description.append("Higher BMI")
            else:
                description.append("Lower BMI")

            if profile['ldl'] > cluster_profiles_df['ldl'].median():
                description.append("Higher LDL")
            else:
                description.append("Lower LDL")

            print(f"    Cluster {i}: {', '.join(description)}")

        # -----------------------------------------------------------------------------
        # 3C. Treatment Response by Cluster
        # -----------------------------------------------------------------------------

        print("\n" + "-"*80)
        print("3C. TREATMENT RESPONSE BY PATIENT PHENOTYPE CLUSTER")
        print("-"*80)

        # Merge cluster assignments with outcomes
        cluster_outcomes = pre_post_complete.merge(
            cluster_data_complete[['person_id', 'cluster']],
            on='person_id',
            how='inner'
        )

        print(f"\nPatients with cluster assignment and outcome data: {cluster_outcomes['person_id'].nunique()}")

        # Analyze HbA1c response by cluster and treatment
        print(f"\nHbA1c Response by Cluster and Treatment Group:")
        print(f"{'Cluster':<10} {'Group':<25} {'n':>6} {'Pre':>8} {'Post':>8} {'Delta':>8} {'p-value':>10}")
        print("-"*85)

        for cluster in range(n_clusters):
            cluster_hba1c = cluster_outcomes[
                (cluster_outcomes['cluster'] == cluster) &
                (cluster_outcomes['measurement_category'] == 'HbA1c')
            ]

            for group in GROUPS:
                group_data = cluster_hba1c[cluster_hba1c['group'] == group]

                if len(group_data) >= 5:
                    pre_vals = group_data['pre'].dropna().values
                    post_vals = group_data['post'].dropna().values
                    change_vals = group_data['change'].dropna().values

                    try:
                        _, pval = stats.wilcoxon(pre_vals, post_vals)
                        p_str = f"{pval:.4f}" if pval >= 0.0001 else "<0.0001"
                        sig = "*" if pval < 0.05 else ""
                    except:
                        pval, p_str, sig = np.nan, "N/A", ""

                    print(f"{cluster:<10} {group:<25} {len(group_data):>6} {np.median(pre_vals):>8.2f} "
                          f"{np.median(post_vals):>8.2f} {np.median(change_vals):>+8.2f} {p_str:>9}{sig}")

                    cluster_treatment_response.append({
                        'cluster': cluster,
                        'group': group,
                        'outcome': 'HbA1c',
                        'n': len(group_data),
                        'pre_median': np.median(pre_vals),
                        'post_median': np.median(post_vals),
                        'change_median': np.median(change_vals),
                        'p_value': pval
                    })

        # -----------------------------------------------------------------------------
        # 3D. Identify Best Treatment by Cluster
        # -----------------------------------------------------------------------------

        print("\n" + "-"*80)
        print("3D. OPTIMAL TREATMENT BY PATIENT PHENOTYPE")
        print("-"*80)

        cluster_response_df = pd.DataFrame(cluster_treatment_response)

        if len(cluster_response_df) > 0:
            print("\nHbA1c Reduction by Treatment Group within Each Cluster:")
            print("(More negative = better glycemic improvement)\n")

            for cluster in range(n_clusters):
                cluster_resp = cluster_response_df[
                    (cluster_response_df['cluster'] == cluster) &
                    (cluster_response_df['outcome'] == 'HbA1c')
                ].sort_values('change_median')

                if len(cluster_resp) > 0:
                    print(f"  Cluster {cluster} ({cluster_profiles[cluster]['n']} patients):")
                    print(f"  {'Rank':<6} {'Treatment':<25} {'HbA1c Change':>15} {'n':>8}")
                    print("  " + "-"*60)

                    for rank, (_, row) in enumerate(cluster_resp.iterrows(), 1):
                        print(f"  {rank:<6} {row['group']:<25} {row['change_median']:>+15.2f} {row['n']:>8}")

                    # Best treatment for this cluster
                    best = cluster_resp.iloc[0]
                    print(f"\n  --> Best treatment for Cluster {cluster}: {best['group']} (Delta HbA1c = {best['change_median']:+.2f})")
                    print()

        # Interaction test: Does cluster modify treatment effect?
        print("\nCluster x Treatment Interaction Test:")
        print("Testing if treatment effects differ significantly across clusters\n")

        for group1, group2 in [('GLP1_mono', 'Metformin_mono'), ('Metformin_GLP1', 'Metformin_mono')]:
            print(f"  Comparison: {group1} vs {group2}")
            print(f"  {'Cluster':<10} {'n_G1':>8} {'n_G2':>8} {'Delta_G1':>10} {'Delta_G2':>10} {'Diff':>10} {'p-value':>10}")
            print("  " + "-"*75)

            for cluster in range(n_clusters):
                g1_data = cluster_outcomes[
                    (cluster_outcomes['cluster'] == cluster) &
                    (cluster_outcomes['group'] == group1) &
                    (cluster_outcomes['measurement_category'] == 'HbA1c')
                ]['change'].dropna().values

                g2_data = cluster_outcomes[
                    (cluster_outcomes['cluster'] == cluster) &
                    (cluster_outcomes['group'] == group2) &
                    (cluster_outcomes['measurement_category'] == 'HbA1c')
                ]['change'].dropna().values

                if len(g1_data) >= 5 and len(g2_data) >= 5:
                    try:
                        _, p = stats.mannwhitneyu(g1_data, g2_data, alternative='two-sided')
                        diff = np.median(g1_data) - np.median(g2_data)
                        p_str = f"{p:.4f}" if p >= 0.0001 else "<0.0001"
                        sig = "*" if p < 0.05 else ""

                        print(f"  {cluster:<10} {len(g1_data):>8} {len(g2_data):>8} {np.median(g1_data):>+10.2f} "
                              f"{np.median(g2_data):>+10.2f} {diff:>+10.2f} {p_str:>9}{sig}")
                    except:
                        pass
            print()

else:
    print("\nscikit-learn not available - skipping clustering analysis")
    print("To enable: pip install scikit-learn")

cluster_treatment_response_df = pd.DataFrame(cluster_treatment_response)

# -----------------------------------------------------------------------------
# 3E. Summary
# -----------------------------------------------------------------------------

print("\n" + "="*80)
print("KEY FINDINGS - TREATMENT RESPONSE HETEROGENEITY ANALYSIS")
print("="*80)
print("""
This precision medicine analysis identifies patient phenotypes with differential treatment responses:

Methodology:
- K-means clustering on baseline characteristics (age, HbA1c, BMI, LDL, Triglycerides)
- Compared treatment response across identified phenotype clusters
- Tested for cluster x treatment interaction effects
- Note: Inflammatory biomarkers excluded per study protocol

Clinical Implications:
- Identifies which patients benefit most from GLP-1 vs Metformin
- Supports precision medicine approach to T2DM treatment selection
- Could inform clinical decision tools for personalized therapy

Novel Contribution:
- Most GLP-1 trials report average effects; this shows WHO responds best
- Phenotype-based analysis is a gap in real-world diabetes research
- Actionable insights: "Patients with profile X should consider treatment Y"
""")

# ============================================================================
# ============================================================================
# NOVEL ANALYSIS 4: DISCORDANT RESPONDERS ANALYSIS
# ============================================================================
# ============================================================================
# Question: What characterizes patients who achieve glycemic control but NOT lipid goals (or vice versa)?
# Define: "Glycemic responder" (HbA1c decrease >=0.5%) vs "Lipid responder" (LDL <100 or decrease >=20)
# Identify discordant groups and compare characteristics
# Why novel: Challenges assumption that metabolic improvements are uniform
# ============================================================================

print("\n" + "="*100)
print("NOVEL ANALYSIS 4: DISCORDANT RESPONDERS ANALYSIS")
print("="*100)
print("\nResearch Question: What characterizes patients with discordant glycemic vs lipid responses?")
print("Clinical Relevance: Challenges assumption that metabolic improvements are uniform.")
print("Implication: Some patients may need additional targeted therapy.\n")

# -----------------------------------------------------------------------------
# 4A. Define Response Categories
# -----------------------------------------------------------------------------

print("-"*80)
print("4A. DEFINING RESPONSE CATEGORIES")
print("-"*80)

# Response definitions (imported from concept_sets.py)

print(f"\nResponse Definitions:")
print(f"  Glycemic Responder: HbA1c change <= {GLYCEMIC_RESPONSE_THRESHOLD}% (decrease of >= 0.5%)")
print(f"  Lipid Responder: Post-LDL < {LIPID_RESPONSE_LDL_TARGET} mg/dL OR LDL change <= {LIPID_RESPONSE_LDL_CHANGE} mg/dL")

# Get HbA1c responses
hba1c_response = pre_post_complete[
    pre_post_complete['measurement_category'] == 'HbA1c'
][['person_id', 'group', 'pre', 'post', 'change', 'sex_at_birth', 'race', 'age_at_index']].copy()
hba1c_response.columns = ['person_id', 'group', 'hba1c_pre', 'hba1c_post', 'hba1c_change', 'sex_at_birth', 'race', 'age_at_index']
hba1c_response['glycemic_responder'] = hba1c_response['hba1c_change'] <= GLYCEMIC_RESPONSE_THRESHOLD

# Get LDL responses
ldl_response = pre_post_complete[
    pre_post_complete['measurement_category'] == 'LDL'
][['person_id', 'pre', 'post', 'change']].copy()
ldl_response.columns = ['person_id', 'ldl_pre', 'ldl_post', 'ldl_change']
ldl_response['lipid_responder'] = (ldl_response['ldl_post'] < LIPID_RESPONSE_LDL_TARGET) | \
                                   (ldl_response['ldl_change'] <= LIPID_RESPONSE_LDL_CHANGE)

# Merge to get patients with both HbA1c and LDL data
discordant_data = hba1c_response.merge(ldl_response, on='person_id', how='inner')

# Create response categories
def categorize_response(row):
    if row['glycemic_responder'] and row['lipid_responder']:
        return 'Dual_Responder'
    elif row['glycemic_responder'] and not row['lipid_responder']:
        return 'Glycemic_Only'
    elif not row['glycemic_responder'] and row['lipid_responder']:
        return 'Lipid_Only'
    else:
        return 'Non_Responder'

discordant_data['response_category'] = discordant_data.apply(categorize_response, axis=1)

print(f"\nPatients with both HbA1c and LDL pre/post data: {len(discordant_data)}")

# Distribution of response categories
print(f"\nResponse Category Distribution:")
print(f"{'Category':<20} {'N':>8} {'%':>8}")
print("-"*40)

response_counts = discordant_data['response_category'].value_counts()
for category in ['Dual_Responder', 'Glycemic_Only', 'Lipid_Only', 'Non_Responder']:
    if category in response_counts.index:
        n = response_counts[category]
        pct = n / len(discordant_data) * 100
        print(f"{category:<20} {n:>8} {pct:>7.1f}%")

# -----------------------------------------------------------------------------
# 4B. Characteristics of Discordant Responders
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("4B. CHARACTERISTICS OF RESPONSE CATEGORIES")
print("-"*80)

# Merge with baseline features
if 'baseline_features_df' in dir():
    discordant_with_features = discordant_data.merge(
        baseline_features_df[['person_id', 'baseline_HbA1c', 'baseline_LDL', 'baseline_Triglycerides',
                              'baseline_BMI']],
        on='person_id',
        how='left'
    )
else:
    discordant_with_features = discordant_data.copy()

print(f"\n{'Category':<20} {'N':>6} {'Age':>8} {'%Female':>10} {'BL HbA1c':>10} {'BL LDL':>10} {'BL BMI':>10}")
print("-"*80)

response_characteristics = []

for category in ['Dual_Responder', 'Glycemic_Only', 'Lipid_Only', 'Non_Responder']:
    cat_data = discordant_with_features[discordant_with_features['response_category'] == category]

    if len(cat_data) >= 5:
        n = len(cat_data)
        age = cat_data['age_at_index'].median()
        female_pct = (cat_data['sex_at_birth'] == 'Female').sum() / n * 100

        bl_hba1c = cat_data['baseline_HbA1c'].median() if 'baseline_HbA1c' in cat_data.columns else cat_data['hba1c_pre'].median()
        bl_ldl = cat_data['baseline_LDL'].median() if 'baseline_LDL' in cat_data.columns else cat_data['ldl_pre'].median()
        bl_bmi = cat_data['baseline_BMI'].median() if 'baseline_BMI' in cat_data.columns else np.nan

        bl_hba1c_str = f"{bl_hba1c:.1f}" if not np.isnan(bl_hba1c) else "N/A"
        bl_ldl_str = f"{bl_ldl:.1f}" if not np.isnan(bl_ldl) else "N/A"
        bl_bmi_str = f"{bl_bmi:.1f}" if not np.isnan(bl_bmi) else "N/A"

        print(f"{category:<20} {n:>6} {age:>8.1f} {female_pct:>9.1f}% {bl_hba1c_str:>10} {bl_ldl_str:>10} {bl_bmi_str:>10}")

        response_characteristics.append({
            'category': category,
            'n': n,
            'age_median': age,
            'female_pct': female_pct,
            'baseline_hba1c': bl_hba1c,
            'baseline_ldl': bl_ldl,
            'baseline_bmi': bl_bmi
        })

# -----------------------------------------------------------------------------
# 4C. Discordant Responders by Treatment Group
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("4C. DISCORDANT RESPONDERS BY TREATMENT GROUP")
print("-"*80)

print(f"\n{'Group':<25} {'N':>6} {'Dual':>8} {'Glyc_Only':>10} {'Lipid_Only':>11} {'Non_Resp':>10}")
print("-"*85)

discordant_by_group = []

for group in GROUPS:
    group_data = discordant_data[discordant_data['group'] == group]

    if len(group_data) >= 10:
        n = len(group_data)
        dual = (group_data['response_category'] == 'Dual_Responder').sum()
        glyc_only = (group_data['response_category'] == 'Glycemic_Only').sum()
        lipid_only = (group_data['response_category'] == 'Lipid_Only').sum()
        non_resp = (group_data['response_category'] == 'Non_Responder').sum()

        print(f"{group:<25} {n:>6} {dual:>5} ({dual/n*100:>4.1f}%) {glyc_only:>5} ({glyc_only/n*100:>4.1f}%) "
              f"{lipid_only:>5} ({lipid_only/n*100:>4.1f}%) {non_resp:>5} ({non_resp/n*100:>4.1f}%)")

        discordant_by_group.append({
            'group': group,
            'n': n,
            'dual_responder_n': dual,
            'dual_responder_pct': dual/n*100,
            'glycemic_only_n': glyc_only,
            'glycemic_only_pct': glyc_only/n*100,
            'lipid_only_n': lipid_only,
            'lipid_only_pct': lipid_only/n*100,
            'non_responder_n': non_resp,
            'non_responder_pct': non_resp/n*100
        })

discordant_by_group_df = pd.DataFrame(discordant_by_group)

# -----------------------------------------------------------------------------
# 4D. Statistical Comparison of Discordant Groups
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("4D. STATISTICAL COMPARISON OF DISCORDANT GROUPS")
print("-"*80)

# Compare characteristics between Glycemic_Only vs Lipid_Only
glyc_only = discordant_with_features[discordant_with_features['response_category'] == 'Glycemic_Only']
lipid_only = discordant_with_features[discordant_with_features['response_category'] == 'Lipid_Only']

if len(glyc_only) >= 10 and len(lipid_only) >= 10:
    print(f"\nComparing Glycemic-Only vs Lipid-Only Responders:")
    print(f"  N Glycemic-Only: {len(glyc_only)}")
    print(f"  N Lipid-Only:    {len(lipid_only)}")

    print(f"\n{'Characteristic':<25} {'Glyc_Only':>12} {'Lipid_Only':>12} {'p-value':>12}")
    print("-"*65)

    # Age comparison
    try:
        _, p_age = stats.mannwhitneyu(glyc_only['age_at_index'].dropna(),
                                       lipid_only['age_at_index'].dropna())
        p_str = f"{p_age:.4f}" if p_age >= 0.0001 else "<0.0001"
        sig = "*" if p_age < 0.05 else ""
        print(f"{'Age (median)':<25} {glyc_only['age_at_index'].median():>12.1f} "
              f"{lipid_only['age_at_index'].median():>12.1f} {p_str:>11}{sig}")
    except:
        pass

    # Baseline HbA1c
    try:
        _, p_hba1c = stats.mannwhitneyu(glyc_only['hba1c_pre'].dropna(),
                                         lipid_only['hba1c_pre'].dropna())
        p_str = f"{p_hba1c:.4f}" if p_hba1c >= 0.0001 else "<0.0001"
        sig = "*" if p_hba1c < 0.05 else ""
        print(f"{'Baseline HbA1c (median)':<25} {glyc_only['hba1c_pre'].median():>12.1f} "
              f"{lipid_only['hba1c_pre'].median():>12.1f} {p_str:>11}{sig}")
    except:
        pass

    # Baseline LDL
    try:
        _, p_ldl = stats.mannwhitneyu(glyc_only['ldl_pre'].dropna(),
                                       lipid_only['ldl_pre'].dropna())
        p_str = f"{p_ldl:.4f}" if p_ldl >= 0.0001 else "<0.0001"
        sig = "*" if p_ldl < 0.05 else ""
        print(f"{'Baseline LDL (median)':<25} {glyc_only['ldl_pre'].median():>12.1f} "
              f"{lipid_only['ldl_pre'].median():>12.1f} {p_str:>11}{sig}")
    except:
        pass

    # Sex comparison (chi-square)
    try:
        glyc_female = (glyc_only['sex_at_birth'] == 'Female').sum()
        lipid_female = (lipid_only['sex_at_birth'] == 'Female').sum()
        contingency = [[glyc_female, len(glyc_only) - glyc_female],
                       [lipid_female, len(lipid_only) - lipid_female]]
        _, p_sex, _, _ = stats.chi2_contingency(contingency)
        p_str = f"{p_sex:.4f}" if p_sex >= 0.0001 else "<0.0001"
        sig = "*" if p_sex < 0.05 else ""
        print(f"{'% Female':<25} {glyc_female/len(glyc_only)*100:>11.1f}% "
              f"{lipid_female/len(lipid_only)*100:>11.1f}% {p_str:>11}{sig}")
    except:
        pass

# -----------------------------------------------------------------------------
# 4E. Predictors of Discordant Response (Logistic Regression)
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("4E. PREDICTORS OF DISCORDANT RESPONSE")
print("-"*80)

try:
    import statsmodels.api as sm
    STATSMODELS_AVAILABLE = True
except ImportError:
    STATSMODELS_AVAILABLE = False

if STATSMODELS_AVAILABLE and len(discordant_with_features) >= 100:
    print("\nLogistic Regression: Predictors of Glycemic-Only Response (vs Dual Responder)")
    print("(Among responders to at least one domain)")

    # Subset to responders only
    responders = discordant_with_features[
        discordant_with_features['response_category'].isin(['Dual_Responder', 'Glycemic_Only'])
    ].copy()

    if len(responders) >= 50:
        # Outcome: 1 = Glycemic Only (discordant), 0 = Dual Responder (concordant)
        responders['discordant'] = (responders['response_category'] == 'Glycemic_Only').astype(int)

        # Predictors
        responders['female'] = (responders['sex_at_birth'] == 'Female').astype(int)
        responders['race_black'] = responders['race'].astype(str).str.contains('Black|African', case=False, na=False).astype(int)

        # Use available baseline values
        responders['age_std'] = (responders['age_at_index'] - responders['age_at_index'].mean()) / responders['age_at_index'].std()
        responders['bl_hba1c_std'] = (responders['hba1c_pre'] - responders['hba1c_pre'].mean()) / responders['hba1c_pre'].std()
        responders['bl_ldl_std'] = (responders['ldl_pre'] - responders['ldl_pre'].mean()) / responders['ldl_pre'].std()

        # GLP-1 containing regimen
        responders['has_glp1'] = responders['group'].str.contains('GLP1', case=False).astype(int)

        model_data = responders[['discordant', 'age_std', 'female', 'race_black',
                                  'bl_hba1c_std', 'bl_ldl_std', 'has_glp1']].dropna()

        if len(model_data) >= 50:
            try:
                X = model_data[['age_std', 'female', 'race_black', 'bl_hba1c_std', 'bl_ldl_std', 'has_glp1']]
                X = sm.add_constant(X)
                y = model_data['discordant']

                model = sm.Logit(y, X).fit(disp=0)

                print(f"\n{'Variable':<25} {'OR':>8} {'95% CI':>18} {'p-value':>12}")
                print("-"*70)

                for var in ['age_std', 'female', 'race_black', 'bl_hba1c_std', 'bl_ldl_std', 'has_glp1']:
                    if var in model.params.index:
                        coef = model.params[var]
                        se = model.bse[var]
                        or_val = np.exp(coef)
                        ci_low = np.exp(coef - 1.96 * se)
                        ci_high = np.exp(coef + 1.96 * se)
                        p_val = model.pvalues[var]

                        p_str = f"{p_val:.4f}" if p_val >= 0.0001 else "<0.0001"
                        sig = "*" if p_val < 0.05 else ""

                        var_name = var.replace('_std', '').replace('bl_', 'Baseline ')
                        print(f"{var_name:<25} {or_val:>8.2f} [{ci_low:>6.2f}, {ci_high:>6.2f}] {p_str:>11}{sig}")

                print(f"\nModel N = {len(model_data)}, Pseudo R-squared = {model.prsquared:.3f}")

            except Exception as e:
                print(f"Logistic regression failed: {e}")
else:
    print("\nstatsmodels not available or insufficient data for logistic regression")

# -----------------------------------------------------------------------------
# 4F. Summary
# -----------------------------------------------------------------------------

print("\n" + "="*80)
print("KEY FINDINGS - DISCORDANT RESPONDERS ANALYSIS")
print("="*80)
print("""
This analysis identifies patients with discordant glycemic vs lipid responses:

Response Categories:
- Dual Responder: Improved both HbA1c (>=0.5% decrease) AND LDL (reached goal)
- Glycemic Only: Improved HbA1c but NOT LDL - may need lipid-focused therapy
- Lipid Only: Improved LDL but NOT HbA1c - may need glycemic intensification
- Non-Responder: Neither improved - consider alternative strategies

Clinical Implications:
- Identifies patients who may benefit from additional targeted therapy
- Glycemic-only responders: Consider adding/intensifying statin therapy
- Lipid-only responders: Consider GLP-1 addition or glycemic intensification
- Characterization helps predict who will have discordant responses

Novel Contribution:
- Challenges assumption that metabolic improvements occur uniformly
- Real-world evidence for multi-dimensional treatment response
- Supports individualized approach to cardiometabolic management
- Identifies gaps between glycemic and lipid control in clinical practice
""")

# ============================================================================
# ============================================================================
# NOVEL ANALYSIS 5: GLP-1 EFFECT IN STATIN-NAIVE POPULATION
# ============================================================================
# ============================================================================
# Question: In patients NOT on statins, what are the independent lipid effects of GLP-1?
# Frame: "GLP-1 lipid effects in statin-naive T2DM patients"
# Compare to statin-containing groups to show the added GLP-1 contribution
# Why novel: Most GLP-1 trials have heavy statin background use - isolating GLP-1's
#            independent lipid effect is a gap in the literature
# ============================================================================

print("\n" + "="*100)
print("NOVEL ANALYSIS 5: GLP-1 EFFECT IN STATIN-NAIVE POPULATION")
print("="*100)
print("\nResearch Question: What are the independent lipid effects of GLP-1 in statin-naive patients?")
print("Clinical Relevance: Most GLP-1 trials have heavy statin background use.")
print("This isolates GLP-1's independent lipid contribution - a literature gap.\n")

# -----------------------------------------------------------------------------
# 5A. Define Statin-Naive vs Statin-Exposed Groups
# -----------------------------------------------------------------------------

print("-"*80)
print("5A. DEFINING STATIN-NAIVE vs STATIN-EXPOSED GROUPS")
print("-"*80)

# Groups WITHOUT statins (statin-naive)
statin_naive_groups = ['Metformin_mono', 'GLP1_mono', 'Metformin_GLP1']

# Groups WITH statins (statin-exposed)
statin_exposed_groups = ['Statin_mono', 'Statin_GLP1', 'Statin_Metformin', 'Statin_GLP1_Metformin']

# For lipid analysis, focus on patients with lipid data
lipid_data = pre_post_complete[
    pre_post_complete['measurement_category'].isin(['LDL', 'HDL', 'Triglycerides', 'Total_Cholesterol'])
].copy()

lipid_data['statin_status'] = lipid_data['group'].apply(
    lambda x: 'Statin_Naive' if x in statin_naive_groups else 'Statin_Exposed'
)

lipid_data['has_glp1'] = lipid_data['group'].str.contains('GLP1', case=False)

print(f"\nLipid data available:")
print(f"  Total patients: {lipid_data['person_id'].nunique()}")

print(f"\n{'Group':<25} {'Statin Status':<15} {'Has GLP-1':>10} {'N (lipid data)':>15}")
print("-"*70)

for group in GROUPS:
    group_data = lipid_data[lipid_data['group'] == group]
    n = group_data['person_id'].nunique()
    if n > 0:
        statin_status = 'Naive' if group in statin_naive_groups else 'Exposed'
        has_glp1 = 'Yes' if 'GLP1' in group else 'No'
        print(f"{group:<25} {statin_status:<15} {has_glp1:>10} {n:>15}")

# -----------------------------------------------------------------------------
# 5B. Lipid Outcomes in Statin-Naive GLP-1 Users
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("5B. LIPID OUTCOMES IN STATIN-NAIVE PATIENTS BY GLP-1 STATUS")
print("-"*80)

statin_naive_results = []

print(f"\nLipid Changes in Statin-Naive Patients:")
print(f"{'Outcome':<20} {'GLP-1 Status':<15} {'n':>6} {'Pre':>10} {'Post':>10} {'Delta':>10} {'95% CI':>22} {'p-value':>10}")
print("-"*110)

for outcome in ['LDL', 'HDL', 'Triglycerides', 'Total_Cholesterol']:
    # Statin-naive without GLP-1 (Metformin_mono)
    outcome_met_only = lipid_data[
        (lipid_data['group'] == 'Metformin_mono') &
        (lipid_data['measurement_category'] == outcome)
    ]

    # Statin-naive with GLP-1 (GLP1_mono and Metformin_GLP1)
    outcome_glp1_naive = lipid_data[
        (lipid_data['group'].isin(['GLP1_mono', 'Metformin_GLP1'])) &
        (lipid_data['measurement_category'] == outcome)
    ]

    for label, data in [('No GLP-1', outcome_met_only), ('With GLP-1', outcome_glp1_naive)]:
        if len(data) >= 5:
            pre_vals = data['pre'].dropna().values
            post_vals = data['post'].dropna().values
            change_vals = data['change'].dropna().values

            change_stats = calculate_stats(change_vals)

            try:
                _, pval = stats.wilcoxon(pre_vals, post_vals)
                p_str = f"{pval:.4f}" if pval >= 0.0001 else "<0.0001"
                sig = "*" if pval < 0.05 else ""
            except:
                pval, p_str, sig = np.nan, "N/A", ""

            ci_str = f"[{change_stats['ci_lower']:+.2f}, {change_stats['ci_upper']:+.2f}]"

            print(f"{outcome:<20} {label:<15} {change_stats['n']:>6} {np.median(pre_vals):>10.2f} "
                  f"{np.median(post_vals):>10.2f} {change_stats['median']:>+10.2f} {ci_str:>22} {p_str:>9}{sig}")

            statin_naive_results.append({
                'analysis': 'Statin_Naive_GLP1_Effect',
                'outcome': outcome,
                'glp1_status': label,
                'n': change_stats['n'],
                'pre_median': np.median(pre_vals),
                'post_median': np.median(post_vals),
                'change_median': change_stats['median'],
                'ci_lower': change_stats['ci_lower'],
                'ci_upper': change_stats['ci_upper'],
                'p_value': pval
            })

    # Compare GLP-1 vs No GLP-1 (statin-naive)
    if len(outcome_met_only) >= 5 and len(outcome_glp1_naive) >= 5:
        try:
            _, p_between = stats.mannwhitneyu(
                outcome_glp1_naive['change'].dropna(),
                outcome_met_only['change'].dropna(),
                alternative='two-sided'
            )
            diff = np.median(outcome_glp1_naive['change'].dropna()) - np.median(outcome_met_only['change'].dropna())
            ci_low, ci_high = bootstrap_difference_ci(
                outcome_glp1_naive['change'].dropna().values,
                outcome_met_only['change'].dropna().values
            )
            p_str = f"{p_between:.4f}" if p_between >= 0.0001 else "<0.0001"
            sig = "*" if p_between < 0.05 else ""
            print(f"  --> GLP-1 vs No GLP-1: Diff = {diff:+.2f} [{ci_low:+.2f}, {ci_high:+.2f}], p = {p_str}{sig}")
        except:
            pass

    print()

statin_naive_results_df = pd.DataFrame(statin_naive_results)

# -----------------------------------------------------------------------------
# 5C. Compare GLP-1 Lipid Effects: Statin-Naive vs Statin-Exposed
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("5C. GLP-1 LIPID EFFECTS: STATIN-NAIVE vs STATIN-EXPOSED")
print("-"*80)

print(f"\nComparing GLP-1 lipid effects by statin background:")
print(f"{'Outcome':<15} {'Population':<20} {'n':>6} {'Delta':>10} {'95% CI':>22} {'p-value':>10}")
print("-"*90)

comparison_results = []

for outcome in ['LDL', 'HDL', 'Triglycerides', 'Total_Cholesterol']:
    # GLP-1 users WITHOUT statins
    glp1_no_statin = lipid_data[
        (lipid_data['group'].isin(['GLP1_mono', 'Metformin_GLP1'])) &
        (lipid_data['measurement_category'] == outcome)
    ]

    # GLP-1 users WITH statins
    glp1_with_statin = lipid_data[
        (lipid_data['group'].isin(['Statin_GLP1', 'Statin_GLP1_Metformin'])) &
        (lipid_data['measurement_category'] == outcome)
    ]

    for label, data in [('GLP-1, No Statin', glp1_no_statin), ('GLP-1 + Statin', glp1_with_statin)]:
        if len(data) >= 5:
            change_vals = data['change'].dropna().values
            change_stats = calculate_stats(change_vals)

            try:
                pre_vals = data['pre'].dropna().values
                post_vals = data['post'].dropna().values
                _, pval = stats.wilcoxon(pre_vals, post_vals)
                p_str = f"{pval:.4f}" if pval >= 0.0001 else "<0.0001"
                sig = "*" if pval < 0.05 else ""
            except:
                pval, p_str, sig = np.nan, "N/A", ""

            ci_str = f"[{change_stats['ci_lower']:+.2f}, {change_stats['ci_upper']:+.2f}]"

            print(f"{outcome:<15} {label:<20} {change_stats['n']:>6} {change_stats['median']:>+10.2f} {ci_str:>22} {p_str:>9}{sig}")

            comparison_results.append({
                'outcome': outcome,
                'population': label,
                'n': change_stats['n'],
                'change_median': change_stats['median'],
                'ci_lower': change_stats['ci_lower'],
                'ci_upper': change_stats['ci_upper'],
                'p_value': pval
            })

    # Compare statin-naive vs statin-exposed GLP-1 users
    if len(glp1_no_statin) >= 5 and len(glp1_with_statin) >= 5:
        try:
            _, p = stats.mannwhitneyu(
                glp1_no_statin['change'].dropna(),
                glp1_with_statin['change'].dropna(),
                alternative='two-sided'
            )
            diff = np.median(glp1_no_statin['change'].dropna()) - np.median(glp1_with_statin['change'].dropna())
            p_str = f"{p:.4f}" if p >= 0.0001 else "<0.0001"
            sig = "*" if p < 0.05 else ""
            print(f"  --> Difference (No Statin - With Statin): {diff:+.2f}, p = {p_str}{sig}")
        except:
            pass

    print()

comparison_results_df = pd.DataFrame(comparison_results)

# -----------------------------------------------------------------------------
# 5D. Isolate GLP-1's Independent Lipid Contribution
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("5D. ESTIMATING GLP-1's INDEPENDENT LIPID CONTRIBUTION")
print("-"*80)

print("""
Method: Compare lipid changes between groups to estimate GLP-1's independent effect:
  - Statin alone effect: Statin_mono vs Metformin_mono
  - GLP-1 alone effect (statin-naive): GLP-1 containing vs Metformin_mono
  - Combination effect: Statin + GLP-1 vs Statin alone
""")

print(f"\n{'Comparison':<45} {'Outcome':<15} {'Diff':>10} {'95% CI':>22} {'p-value':>10}")
print("-"*110)

for outcome in ['LDL', 'Triglycerides']:
    # Get data for each group
    met_mono = lipid_data[(lipid_data['group'] == 'Metformin_mono') &
                           (lipid_data['measurement_category'] == outcome)]['change'].dropna().values
    statin_mono = lipid_data[(lipid_data['group'] == 'Statin_mono') &
                              (lipid_data['measurement_category'] == outcome)]['change'].dropna().values
    glp1_mono = lipid_data[(lipid_data['group'] == 'GLP1_mono') &
                            (lipid_data['measurement_category'] == outcome)]['change'].dropna().values
    met_glp1 = lipid_data[(lipid_data['group'] == 'Metformin_GLP1') &
                           (lipid_data['measurement_category'] == outcome)]['change'].dropna().values
    statin_glp1 = lipid_data[(lipid_data['group'] == 'Statin_GLP1') &
                              (lipid_data['measurement_category'] == outcome)]['change'].dropna().values

    comparisons = [
        ('Statin alone effect (vs Met)', statin_mono, met_mono),
        ('GLP-1 alone effect (vs Met)', glp1_mono, met_mono),
        ('Met+GLP-1 effect (vs Met)', met_glp1, met_mono),
        ('Statin+GLP-1 vs Statin alone', statin_glp1, statin_mono),
    ]

    for comp_name, group1, group2 in comparisons:
        if len(group1) >= 5 and len(group2) >= 5:
            try:
                _, p = stats.mannwhitneyu(group1, group2, alternative='two-sided')
                diff = np.median(group1) - np.median(group2)
                ci_low, ci_high = bootstrap_difference_ci(group1, group2)
                p_str = f"{p:.4f}" if p >= 0.0001 else "<0.0001"
                sig = "*" if p < 0.05 else ""

                print(f"{comp_name:<45} {outcome:<15} {diff:>+10.2f} [{ci_low:+.2f}, {ci_high:+.2f}] {p_str:>9}{sig}")
            except:
                pass

    print()

# -----------------------------------------------------------------------------
# 5E. Summary
# -----------------------------------------------------------------------------

print("\n" + "="*80)
print("KEY FINDINGS - GLP-1 INDEPENDENT LIPID EFFECTS")
print("="*80)
print("""
This analysis isolates GLP-1's independent lipid effects in statin-naive patients:

Key Findings Framework:
1. GLP-1 effect in statin-naive patients:
   - Compares GLP-1 users (without statins) to metformin-only users
   - Shows what lipid changes occur from GLP-1 alone

2. Comparison to statin background:
   - GLP-1 + statin vs GLP-1 alone shows added statin benefit
   - Statin + GLP-1 vs statin alone shows added GLP-1 benefit

3. Independent contribution estimates:
   - Quantifies GLP-1's lipid effect separate from statin effects
   - Important for patients who cannot tolerate statins

Clinical Implications:
- For statin-intolerant patients: GLP-1 may provide some lipid benefit
- GLP-1's triglyceride-lowering effect may be independent of statins
- Combination therapy (statin + GLP-1) may provide additive benefits
- Supports GLP-1 use in patients with residual dyslipidemia on statins

Novel Contribution:
- Most GLP-1 trials include patients on background statins
- This isolates GLP-1's independent lipid contribution
- Fills literature gap on GLP-1 lipid effects without statin confounding
- Clinically relevant for statin-intolerant/statin-naive populations
""")

# ============================================================================
# ============================================================================
# NOVEL ANALYSIS 6: REMOVED -- INFLAMMATORY BIOMARKERS EXCLUDED
# ============================================================================
# ============================================================================
# NOTE: This analysis (Inflammatory Trajectory as Predictor) has been removed
# because inflammatory biomarkers (CRP, ESR, Fibrinogen) are excluded from
# the concept set inventory per study protocol.
# The original analysis examined whether baseline CRP predicts treatment response
# Stratify by baseline CRP (high vs low inflammation)
# Why novel: Ties into "inflammaging" and cardiometabolic inflammation literature
# ============================================================================

print("\n" + "="*100)
print("NOVEL ANALYSIS 6: SKIPPED -- INFLAMMATORY BIOMARKERS EXCLUDED")
print("="*100)
print("\nThis analysis has been removed because inflammatory biomarkers (CRP, ESR, Fibrinogen)")
print("are excluded from the concept set inventory per study protocol.")
print("The original analysis stratified patients by baseline CRP and tested whether")
print("GLP-1's anti-inflammatory properties benefit high-CRP patients more.\n")

# --- All Analysis 6 code has been removed (inflammatory biomarkers excluded) ---

# ============================================================================
# ============================================================================
# NOVEL ANALYSIS 7: THERAPEUTIC INERTIA ANALYSIS
# ============================================================================
# ============================================================================
# Question: How long do patients stay on suboptimal therapy before intensification?
# Among patients who eventually got triple therapy, how long were they on mono/dual
# therapy with HbA1c >7%?
# Identify predictors of delayed intensification (race, sex, age)
# Why novel: Therapeutic inertia is understudied in diverse populations
#            Health equity angle
# ============================================================================

print("\n" + "="*100)
print("NOVEL ANALYSIS 7: THERAPEUTIC INERTIA ANALYSIS")
print("="*100)
print("\nResearch Question: How long do patients stay on suboptimal therapy before intensification?")
print("Clinical Relevance: Therapeutic inertia is a known barrier to diabetes management.")
print("Health Equity Angle: Understudied in diverse populations.\n")

# -----------------------------------------------------------------------------
# 7A. Identify Patients with Treatment Intensification
# -----------------------------------------------------------------------------

print("-"*80)
print("7A. IDENTIFYING TREATMENT INTENSIFICATION PATTERNS")
print("-"*80)

# Focus on patients who eventually got combination therapy
# Calculate time from initial mono/dual therapy to intensification

# Get first drug start dates for each patient
patient_drug_history = []

all_treated_patients = statin_users | metformin_users | glp1_users

for pid in all_treated_patients:
    person_drugs = dataset_28107006_drug_df[dataset_28107006_drug_df['person_id'] == pid]

    statin_start = person_drugs[
        person_drugs['standard_concept_name'].str.contains(statin_pattern, case=False, na=False)
    ]['drug_exposure_start_datetime'].min()

    met_start = person_drugs[
        person_drugs['standard_concept_name'].str.contains(metformin_pattern, case=False, na=False)
    ]['drug_exposure_start_datetime'].min()

    glp1_start = person_drugs[
        person_drugs['standard_concept_name'].str.contains(glp1_pattern, case=False, na=False)
    ]['drug_exposure_start_datetime'].min()

    # Count number of drug classes ever used
    n_classes = sum([pd.notna(statin_start), pd.notna(met_start), pd.notna(glp1_start)])

    # Determine first drug start
    starts = []
    if pd.notna(statin_start):
        starts.append(('Statin', statin_start))
    if pd.notna(met_start):
        starts.append(('Metformin', met_start))
    if pd.notna(glp1_start):
        starts.append(('GLP1', glp1_start))

    if len(starts) >= 1:
        starts_sorted = sorted(starts, key=lambda x: x[1])
        first_drug = starts_sorted[0][0]
        first_date = starts_sorted[0][1]

        last_drug = starts_sorted[-1][0]
        last_date = starts_sorted[-1][1]

        time_to_intensification = (last_date - first_date).days if len(starts) > 1 else np.nan

        patient_drug_history.append({
            'person_id': pid,
            'n_drug_classes': n_classes,
            'first_drug': first_drug,
            'first_drug_date': first_date,
            'statin_start': statin_start,
            'metformin_start': met_start,
            'glp1_start': glp1_start,
            'time_to_intensification_days': time_to_intensification,
            'intensified': n_classes > 1
        })

patient_drug_history_df = pd.DataFrame(patient_drug_history)

# Merge with demographics
patient_drug_history_df = patient_drug_history_df.merge(
    dataset_28107006_person_df[['person_id', 'sex_at_birth', 'race', 'ethnicity']],
    on='person_id',
    how='left'
)

print(f"\nPatients with drug history data: {len(patient_drug_history_df)}")

print(f"\nNumber of Drug Classes Used:")
for n_classes in [1, 2, 3]:
    n = (patient_drug_history_df['n_drug_classes'] == n_classes).sum()
    pct = n / len(patient_drug_history_df) * 100
    print(f"  {n_classes} class{'es' if n_classes > 1 else '':>3}: {n:>6} ({pct:>5.1f}%)")

# -----------------------------------------------------------------------------
# 7B. Time to Intensification Analysis
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("7B. TIME TO TREATMENT INTENSIFICATION")
print("-"*80)

intensified_patients = patient_drug_history_df[patient_drug_history_df['intensified']].copy()
intensified_patients['time_to_intensification_months'] = intensified_patients['time_to_intensification_days'] / 30.44

print(f"\nPatients who intensified therapy: {len(intensified_patients)}")
print(f"\nTime to Intensification (First Drug -> Last Added Drug):")
print(f"  Median: {intensified_patients['time_to_intensification_months'].median():.1f} months")
print(f"  Mean:   {intensified_patients['time_to_intensification_months'].mean():.1f} months")
print(f"  IQR:    [{intensified_patients['time_to_intensification_months'].quantile(0.25):.1f}, "
      f"{intensified_patients['time_to_intensification_months'].quantile(0.75):.1f}] months")

# Distribution of time to intensification
print(f"\nTime to Intensification Distribution:")
bins = [0, 90, 180, 365, 730, float('inf')]
labels = ['<3 mo', '3-6 mo', '6-12 mo', '1-2 yr', '>2 yr']

intensified_patients['intensification_bin'] = pd.cut(
    intensified_patients['time_to_intensification_days'],
    bins=bins,
    labels=labels,
    right=False
)

for label in labels:
    n = (intensified_patients['intensification_bin'] == label).sum()
    pct = n / len(intensified_patients) * 100 if len(intensified_patients) > 0 else 0
    print(f"  {label:<10}: {n:>6} ({pct:>5.1f}%)")

# -----------------------------------------------------------------------------
# 7C. Therapeutic Inertia - HbA1c While on Suboptimal Therapy
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("7C. GLYCEMIC CONTROL WHILE ON SUBOPTIMAL THERAPY")
print("-"*80)

# For patients who intensified, check HbA1c levels BEFORE intensification
# This identifies patients who stayed on monotherapy despite poor control

inertia_data = []

for _, patient in intensified_patients.iterrows():
    pid = patient['person_id']
    first_date = patient['first_drug_date']

    # Get all HbA1c measurements
    patient_hba1c = hba1c_measurements[
        (hba1c_measurements['person_id'] == pid)
    ].copy()

    if len(patient_hba1c) == 0:
        continue

    patient_hba1c = patient_hba1c.sort_values('measurement_datetime')

    # HbA1c before any treatment intensification
    # If they started on mono, look at HbA1c between first drug and second drug
    if patient['n_drug_classes'] >= 2:
        # Find second drug start date
        starts = []
        if pd.notna(patient['statin_start']):
            starts.append(patient['statin_start'])
        if pd.notna(patient['metformin_start']):
            starts.append(patient['metformin_start'])
        if pd.notna(patient['glp1_start']):
            starts.append(patient['glp1_start'])

        starts_sorted = sorted(starts)
        second_drug_date = starts_sorted[1] if len(starts_sorted) >= 2 else None

        if second_drug_date:
            # HbA1c between first drug start and second drug start (monotherapy period)
            mono_hba1c = patient_hba1c[
                (patient_hba1c['measurement_datetime'] >= first_date) &
                (patient_hba1c['measurement_datetime'] < second_drug_date)
            ]

            if len(mono_hba1c) > 0:
                avg_hba1c = mono_hba1c['value_as_number'].mean()
                max_hba1c = mono_hba1c['value_as_number'].max()
                pct_uncontrolled = (mono_hba1c['value_as_number'] >= 7.0).sum() / len(mono_hba1c) * 100
                time_uncontrolled = (second_drug_date - first_date).days

                inertia_data.append({
                    'person_id': pid,
                    'time_on_monotherapy_days': time_uncontrolled,
                    'time_on_monotherapy_months': time_uncontrolled / 30.44,
                    'avg_hba1c_during_mono': avg_hba1c,
                    'max_hba1c_during_mono': max_hba1c,
                    'pct_readings_uncontrolled': pct_uncontrolled,
                    'n_hba1c_readings': len(mono_hba1c),
                    'sex_at_birth': patient['sex_at_birth'],
                    'race': patient['race']
                })

inertia_df = pd.DataFrame(inertia_data)

if len(inertia_df) > 0:
    print(f"\nPatients with HbA1c data during monotherapy period: {len(inertia_df)}")

    # Identify therapeutic inertia: HbA1c >=7% for extended period
    inertia_df['therapeutic_inertia'] = (
        (inertia_df['avg_hba1c_during_mono'] >= 7.0) &
        (inertia_df['time_on_monotherapy_days'] > 180)  # >6 months on mono with poor control
    )

    inertia_count = inertia_df['therapeutic_inertia'].sum()
    inertia_pct = inertia_count / len(inertia_df) * 100

    print(f"\nTherapeutic Inertia (HbA1c >=7% for >6 months on monotherapy):")
    print(f"  Patients with inertia: {inertia_count} ({inertia_pct:.1f}%)")

    inertia_patients = inertia_df[inertia_df['therapeutic_inertia']]
    if len(inertia_patients) > 0:
        print(f"  Mean time on suboptimal mono: {inertia_patients['time_on_monotherapy_months'].mean():.1f} months")
        print(f"  Mean HbA1c during inertia period: {inertia_patients['avg_hba1c_during_mono'].mean():.1f}%")

# -----------------------------------------------------------------------------
# 7D. Predictors of Therapeutic Inertia
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("7D. PREDICTORS OF THERAPEUTIC INERTIA")
print("-"*80)

if len(inertia_df) >= 50:
    print(f"\nTherapeutic Inertia by Demographics:")

    # By sex
    print(f"\n  By Sex:")
    for sex in ['Male', 'Female']:
        sex_data = inertia_df[inertia_df['sex_at_birth'] == sex]
        if len(sex_data) >= 10:
            inertia_rate = sex_data['therapeutic_inertia'].mean() * 100
            avg_time = sex_data['time_on_monotherapy_months'].median()
            print(f"    {sex:<10}: Inertia rate = {inertia_rate:.1f}%, Median time on mono = {avg_time:.1f} mo (n={len(sex_data)})")

    # Statistical comparison by sex
    male_inertia = inertia_df[inertia_df['sex_at_birth'] == 'Male']['therapeutic_inertia']
    female_inertia = inertia_df[inertia_df['sex_at_birth'] == 'Female']['therapeutic_inertia']

    if len(male_inertia) >= 10 and len(female_inertia) >= 10:
        try:
            contingency = [[male_inertia.sum(), len(male_inertia) - male_inertia.sum()],
                           [female_inertia.sum(), len(female_inertia) - female_inertia.sum()]]
            _, p_sex, _, _ = stats.chi2_contingency(contingency)
            p_str = f"{p_sex:.4f}" if p_sex >= 0.0001 else "<0.0001"
            sig = "*" if p_sex < 0.05 else ""
            print(f"    Male vs Female inertia rate: p = {p_str}{sig}")
        except:
            pass

    # By race
    print(f"\n  By Race:")
    for race in ['White', 'Black or African American', 'Asian']:
        race_data = inertia_df[inertia_df['race'].astype(str).str.contains(race, case=False, na=False)]
        if len(race_data) >= 10:
            inertia_rate = race_data['therapeutic_inertia'].mean() * 100
            avg_time = race_data['time_on_monotherapy_months'].median()
            race_label = race[:15] if len(race) > 15 else race
            print(f"    {race_label:<15}: Inertia rate = {inertia_rate:.1f}%, Median time on mono = {avg_time:.1f} mo (n={len(race_data)})")

    # Compare Black vs White inertia rates
    white_inertia = inertia_df[inertia_df['race'].astype(str).str.contains('White', case=False, na=False)]['therapeutic_inertia']
    black_inertia = inertia_df[inertia_df['race'].astype(str).str.contains('Black|African', case=False, na=False)]['therapeutic_inertia']

    if len(white_inertia) >= 10 and len(black_inertia) >= 10:
        try:
            contingency = [[black_inertia.sum(), len(black_inertia) - black_inertia.sum()],
                           [white_inertia.sum(), len(white_inertia) - white_inertia.sum()]]
            _, p_race, _, _ = stats.chi2_contingency(contingency)
            p_str = f"{p_race:.4f}" if p_race >= 0.0001 else "<0.0001"
            sig = "*" if p_race < 0.05 else ""
            print(f"    Black vs White inertia rate: p = {p_str}{sig}")
        except:
            pass

# -----------------------------------------------------------------------------
# 7E. Time-to-Intensification by Demographics
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("7E. TIME TO INTENSIFICATION BY DEMOGRAPHICS")
print("-"*80)

if len(intensified_patients) >= 50:
    print(f"\nTime to Intensification (months) by Demographics:")

    # By sex
    print(f"\n  By Sex:")
    print(f"  {'Sex':<10} {'N':>8} {'Median':>10} {'IQR':>20}")
    print("  " + "-"*55)

    for sex in ['Male', 'Female']:
        sex_data = intensified_patients[intensified_patients['sex_at_birth'] == sex]['time_to_intensification_months']
        if len(sex_data) >= 10:
            median = sex_data.median()
            iqr_low = sex_data.quantile(0.25)
            iqr_high = sex_data.quantile(0.75)
            print(f"  {sex:<10} {len(sex_data):>8} {median:>10.1f} [{iqr_low:.1f}, {iqr_high:.1f}]")

    # Statistical test
    male_time = intensified_patients[intensified_patients['sex_at_birth'] == 'Male']['time_to_intensification_months'].dropna()
    female_time = intensified_patients[intensified_patients['sex_at_birth'] == 'Female']['time_to_intensification_months'].dropna()

    if len(male_time) >= 10 and len(female_time) >= 10:
        try:
            _, p = stats.mannwhitneyu(male_time, female_time, alternative='two-sided')
            p_str = f"{p:.4f}" if p >= 0.0001 else "<0.0001"
            sig = "*" if p < 0.05 else ""
            print(f"  Male vs Female: p = {p_str}{sig}")
        except:
            pass

    # By race
    print(f"\n  By Race:")
    print(f"  {'Race':<25} {'N':>8} {'Median':>10} {'IQR':>20}")
    print("  " + "-"*70)

    for race in ['White', 'Black or African American', 'Asian', 'Hispanic']:
        race_data = intensified_patients[
            intensified_patients['race'].astype(str).str.contains(race, case=False, na=False)
        ]['time_to_intensification_months'].dropna()

        if len(race_data) >= 10:
            median = race_data.median()
            iqr_low = race_data.quantile(0.25)
            iqr_high = race_data.quantile(0.75)
            race_label = race[:25] if len(race) > 25 else race
            print(f"  {race_label:<25} {len(race_data):>8} {median:>10.1f} [{iqr_low:.1f}, {iqr_high:.1f}]")

# -----------------------------------------------------------------------------
# 7F. Summary
# -----------------------------------------------------------------------------

print("\n" + "="*80)
print("KEY FINDINGS - THERAPEUTIC INERTIA ANALYSIS")
print("="*80)
print("""
This analysis examines therapeutic inertia in diabetes management:

Definition:
- Therapeutic Inertia: Failure to intensify therapy despite suboptimal control
- Measured as: HbA1c >=7% for >6 months on monotherapy before adding second agent

Key Metrics:
- Time to intensification: How long patients stay on single-agent therapy
- Proportion with inertia: % of patients with prolonged suboptimal control
- Demographic patterns: Differences by sex, race in intensification timing

Clinical Implications:
- Identifies systemic delays in treatment intensification
- May reveal health equity gaps if certain groups experience more inertia
- Supports interventions targeting timely treatment escalation
- Quantifies the real-world gap between guidelines and practice

Novel Contribution:
- Therapeutic inertia is understudied in diverse, real-world populations
- All of Us data enables analysis across demographic groups
- Health equity angle: Do certain groups experience more delayed care?
- Actionable: Identifies patients at risk for delayed intensification
""")

# ============================================================================
# PART 2: BIAS CHECKS & COHORT VALIDITY
# ============================================================================

print("\n" + "="*100)
print("PART 2: BIAS CHECKS & COHORT VALIDITY")
print("="*100)

output_dir = os.getcwd()

# -----------------------------------------------------------------------------
# Build per-person exposure timeline for bias diagnostics
# -----------------------------------------------------------------------------

drug_for_bias = dataset_28107006_drug_df.copy()
drug_for_bias['drug_exposure_start_datetime'] = pd.to_datetime(drug_for_bias['drug_exposure_start_datetime'])

person_timeline = all_groups[['person_id', 'group', 'index_date']].drop_duplicates().copy()
person_timeline = person_timeline.rename(columns={
    'group': 'original_group',
    'index_date': 'time_zero_original'
})

def get_first_class_start(drug_df, pattern, out_col):
    starts = drug_df[
        drug_df['standard_concept_name'].str.contains(pattern, case=False, na=False)
    ].groupby('person_id')['drug_exposure_start_datetime'].min().reset_index()
    starts.columns = ['person_id', out_col]
    return starts

statin_first_df = get_first_class_start(drug_for_bias, statin_pattern, 'statin_start')
metformin_first_df = get_first_class_start(drug_for_bias, metformin_pattern, 'metformin_start')
glp1_first_df = get_first_class_start(drug_for_bias, glp1_pattern, 'glp1_start')

person_timeline = person_timeline.merge(statin_first_df, on='person_id', how='left')
person_timeline = person_timeline.merge(metformin_first_df, on='person_id', how='left')
person_timeline = person_timeline.merge(glp1_first_df, on='person_id', how='left')

person_timeline['first_any_exposure_start'] = person_timeline[
    ['statin_start', 'metformin_start', 'glp1_start']
].min(axis=1)

person_timeline['immortal_time_days'] = (
    person_timeline['time_zero_original'] - person_timeline['first_any_exposure_start']
).dt.days
person_timeline['immortal_time_days'] = person_timeline['immortal_time_days'].fillna(0).clip(lower=0)

measurement_dates = dataset_28107006_measurement_df[['person_id', 'measurement_datetime']].copy()
measurement_dates['measurement_datetime'] = pd.to_datetime(measurement_dates['measurement_datetime'])
first_measurement = measurement_dates.groupby('person_id')['measurement_datetime'].min().reset_index()
first_measurement.columns = ['person_id', 'first_measurement_date']
last_measurement = measurement_dates.groupby('person_id')['measurement_datetime'].max().reset_index()
last_measurement.columns = ['person_id', 'last_measurement_date']

first_drug = drug_for_bias.groupby('person_id')['drug_exposure_start_datetime'].min().reset_index()
first_drug.columns = ['person_id', 'first_drug_date']

person_timeline = person_timeline.merge(first_measurement, on='person_id', how='left')
person_timeline = person_timeline.merge(first_drug, on='person_id', how='left')
person_timeline = person_timeline.merge(last_measurement, on='person_id', how='left')

person_timeline['observable_start'] = person_timeline[
    ['first_measurement_date', 'first_drug_date']
].min(axis=1)
person_timeline['lookback_days_before_first_exposure'] = (
    person_timeline['first_any_exposure_start'] - person_timeline['observable_start']
).dt.days

# Original design windows
person_timeline['pre_window_start_original'] = person_timeline['time_zero_original'] - pd.to_timedelta(PRE_WINDOW_DAYS, unit='D')
person_timeline['pre_window_end_original'] = person_timeline['time_zero_original'] - pd.to_timedelta(1, unit='D')
person_timeline['post_window_start_original'] = person_timeline['time_zero_original'] + pd.to_timedelta(1, unit='D')
person_timeline['post_window_end_original'] = person_timeline['time_zero_original'] + pd.to_timedelta(POST_WINDOW_DAYS, unit='D')
person_timeline['followup_start_original'] = person_timeline['post_window_start_original']
person_timeline['followup_end_original'] = person_timeline[
    ['last_measurement_date', 'post_window_end_original']
].min(axis=1)

immortal_group_summary = person_timeline.groupby('original_group').agg(
    n_patients=('person_id', 'nunique'),
    mean_immortal_time_days=('immortal_time_days', 'mean'),
    median_immortal_time_days=('immortal_time_days', 'median'),
    pct_with_immortal_time=('immortal_time_days', lambda x: (x > 0).mean() * 100)
).reset_index()

at_risk_groups = ['Statin_GLP1', 'Statin_Metformin', 'Metformin_GLP1', 'Statin_GLP1_Metformin']
at_risk_subset = person_timeline[person_timeline['original_group'].isin(at_risk_groups)]
immortal_time_detected = (
    len(at_risk_subset) > 0 and
    (at_risk_subset['immortal_time_days'] > 0).mean() > 0.05
)

print("\n" + "-"*80)
print("A) IMMORTAL TIME BIAS DIAGNOSTICS")
print("-"*80)
print(f"{'Group':<25} {'N':>8} {'Mean IT(days)':>14} {'Median IT(days)':>16} {'% IT>0':>10}")
print("-"*80)
for _, row in immortal_group_summary.iterrows():
    print(f"{row['original_group']:<25} {int(row['n_patients']):>8} {row['mean_immortal_time_days']:>14.1f} "
          f"{row['median_immortal_time_days']:>16.1f} {row['pct_with_immortal_time']:>9.1f}%")

print(f"\nImmortal time detected in at-risk combination groups: {'YES' if immortal_time_detected else 'NO'}")

# -----------------------------------------------------------------------------
# A) Correction implemented: Landmark time-zero alignment
# -----------------------------------------------------------------------------

LANDMARK_DAYS = 90
person_timeline['time_zero_landmark'] = person_timeline['first_any_exposure_start'] + pd.to_timedelta(LANDMARK_DAYS, unit='D')

person_timeline['statin_by_landmark'] = (
    person_timeline['statin_start'].notna() &
    (person_timeline['statin_start'] <= person_timeline['time_zero_landmark'])
)
person_timeline['metformin_by_landmark'] = (
    person_timeline['metformin_start'].notna() &
    (person_timeline['metformin_start'] <= person_timeline['time_zero_landmark'])
)
person_timeline['glp1_by_landmark'] = (
    person_timeline['glp1_start'].notna() &
    (person_timeline['glp1_start'] <= person_timeline['time_zero_landmark'])
)

def classify_landmark_group(row):
    has_statin = bool(row['statin_by_landmark'])
    has_met = bool(row['metformin_by_landmark'])
    has_glp1 = bool(row['glp1_by_landmark'])

    if has_statin and not has_met and not has_glp1:
        return 'Statin_mono'
    if has_met and not has_statin and not has_glp1:
        return 'Metformin_mono'
    if has_glp1 and not has_statin and not has_met:
        return 'GLP1_mono'
    if has_statin and has_glp1 and not has_met:
        return 'Statin_GLP1'
    if has_statin and has_met and not has_glp1:
        return 'Statin_Metformin'
    if has_met and has_glp1 and not has_statin:
        return 'Metformin_GLP1'
    if has_statin and has_met and has_glp1:
        return 'Statin_GLP1_Metformin'
    return 'Unclassified'

person_timeline['corrected_group'] = person_timeline.apply(classify_landmark_group, axis=1)
person_timeline['group_changed_after_alignment'] = person_timeline['corrected_group'] != person_timeline['original_group']

person_timeline['pre_window_start_landmark'] = person_timeline['time_zero_landmark'] - pd.to_timedelta(PRE_WINDOW_DAYS, unit='D')
person_timeline['pre_window_end_landmark'] = person_timeline['time_zero_landmark'] - pd.to_timedelta(1, unit='D')
person_timeline['post_window_start_landmark'] = person_timeline['time_zero_landmark'] + pd.to_timedelta(1, unit='D')
person_timeline['post_window_end_landmark'] = person_timeline['time_zero_landmark'] + pd.to_timedelta(POST_WINDOW_DAYS, unit='D')
person_timeline['followup_start_landmark'] = person_timeline['post_window_start_landmark']
person_timeline['followup_end_landmark'] = person_timeline[
    ['last_measurement_date', 'post_window_end_landmark']
].min(axis=1)

print(f"Landmark correction applied: fixed {LANDMARK_DAYS}-day alignment from first observed exposure")
print(f"Patients changing exposure group after alignment: "
      f"{int(person_timeline['group_changed_after_alignment'].sum())} / {len(person_timeline)}")

# HbA1c availability under original and corrected windows
hba1c_all = dataset_28107006_measurement_df[
    dataset_28107006_measurement_df['measurement_category'] == 'HbA1c'
][['person_id', 'measurement_datetime', 'value_as_number']].copy()
hba1c_all['measurement_datetime'] = pd.to_datetime(hba1c_all['measurement_datetime'])

hba1c_diag = hba1c_all.merge(
    person_timeline[['person_id', 'time_zero_original', 'time_zero_landmark']],
    on='person_id', how='inner'
)
hba1c_diag['days_from_original'] = (hba1c_diag['measurement_datetime'] - hba1c_diag['time_zero_original']).dt.days
hba1c_diag['days_from_landmark'] = (hba1c_diag['measurement_datetime'] - hba1c_diag['time_zero_landmark']).dt.days

orig_pre_counts = hba1c_diag[
    (hba1c_diag['days_from_original'] >= -PRE_WINDOW_DAYS) & (hba1c_diag['days_from_original'] < 0)
].groupby('person_id').size().rename('hba1c_pre_count_original')

orig_post_counts = hba1c_diag[
    (hba1c_diag['days_from_original'] > 0) & (hba1c_diag['days_from_original'] <= POST_WINDOW_DAYS)
].groupby('person_id').size().rename('hba1c_post_count_original')

landmark_pre_counts = hba1c_diag[
    (hba1c_diag['days_from_landmark'] >= -PRE_WINDOW_DAYS) & (hba1c_diag['days_from_landmark'] < 0)
].groupby('person_id').size().rename('hba1c_pre_count_landmark')

landmark_post_counts = hba1c_diag[
    (hba1c_diag['days_from_landmark'] > 0) & (hba1c_diag['days_from_landmark'] <= POST_WINDOW_DAYS)
].groupby('person_id').size().rename('hba1c_post_count_landmark')

person_timeline = person_timeline.merge(orig_pre_counts, on='person_id', how='left')
person_timeline = person_timeline.merge(orig_post_counts, on='person_id', how='left')
person_timeline = person_timeline.merge(landmark_pre_counts, on='person_id', how='left')
person_timeline = person_timeline.merge(landmark_post_counts, on='person_id', how='left')

for col in ['hba1c_pre_count_original', 'hba1c_post_count_original', 'hba1c_pre_count_landmark', 'hba1c_post_count_landmark']:
    person_timeline[col] = person_timeline[col].fillna(0).astype(int)

person_timeline['has_hba1c_pre_original'] = person_timeline['hba1c_pre_count_original'] > 0
person_timeline['has_hba1c_post_original'] = person_timeline['hba1c_post_count_original'] > 0
person_timeline['has_hba1c_pre_landmark'] = person_timeline['hba1c_pre_count_landmark'] > 0
person_timeline['has_hba1c_post_landmark'] = person_timeline['hba1c_post_count_landmark'] > 0

immortal_time_diagnostics_df = person_timeline[[
    'person_id',
    'original_group',
    'corrected_group',
    'first_any_exposure_start',
    'statin_start',
    'metformin_start',
    'glp1_start',
    'time_zero_original',
    'time_zero_landmark',
    'immortal_time_days',
    'pre_window_start_original',
    'pre_window_end_original',
    'post_window_start_original',
    'post_window_end_original',
    'followup_start_original',
    'followup_end_original',
    'pre_window_start_landmark',
    'pre_window_end_landmark',
    'post_window_start_landmark',
    'post_window_end_landmark',
    'followup_start_landmark',
    'followup_end_landmark',
    'has_hba1c_pre_original',
    'has_hba1c_post_original',
    'has_hba1c_pre_landmark',
    'has_hba1c_post_landmark'
]].copy()

immortal_time_path = os.path.join(output_dir, 'immortal_time_diagnostics.csv')
immortal_group_path = os.path.join(output_dir, 'immortal_time_group_summary.csv')
immortal_time_diagnostics_df.to_csv(immortal_time_path, index=False)
immortal_group_summary.to_csv(immortal_group_path, index=False)

# -----------------------------------------------------------------------------
# B) Left censoring and triple-therapy sensitivity analyses
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("B) LEFT CENSORING / TRIPLE THERAPY EXPOSURE VALIDITY")
print("-"*80)

lookback_windows = [90, 180, 365, 730]
left_censoring_group_rows = []
left_censoring_triple_rows = []

triple_hba1c_change = pre_post_complete[
    (pre_post_complete['group'] == 'Statin_GLP1_Metformin') &
    (pre_post_complete['measurement_category'] == 'HbA1c')
][['person_id', 'change']].dropna()

triple_mask = person_timeline['original_group'] == 'Statin_GLP1_Metformin'
n_triple_original = int(triple_mask.sum())

print(f"Original triple-therapy cohort size: {n_triple_original}")
print(f"\n{'Lookback':<12} {'Triple Retained':>16} {'Potential LC':>14} {'% Reclassified':>16} {'HbA1c pairs':>14}")
print("-"*80)

for w in lookback_windows:
    eligible = person_timeline['lookback_days_before_first_exposure'] >= w

    for g in GROUPS:
        g_mask = person_timeline['original_group'] == g
        n_orig = int(g_mask.sum())
        n_retained = int((g_mask & eligible).sum())
        pct_retained = (n_retained / n_orig * 100) if n_orig > 0 else np.nan
        left_censoring_group_rows.append({
            'lookback_days': w,
            'group': g,
            'n_original': n_orig,
            'n_retained_after_lookback': n_retained,
            'pct_retained': pct_retained
        })

    n_triple_retained = int((triple_mask & eligible).sum())
    n_triple_left_censored = n_triple_original - n_triple_retained
    pct_reclassified = (n_triple_left_censored / n_triple_original * 100) if n_triple_original > 0 else np.nan

    retained_ids = set(person_timeline[triple_mask & eligible]['person_id'])
    triple_hba1c_retained = triple_hba1c_change[triple_hba1c_change['person_id'].isin(retained_ids)]
    median_hba1c_change = triple_hba1c_retained['change'].median() if len(triple_hba1c_retained) > 0 else np.nan

    left_censoring_triple_rows.append({
        'lookback_days': w,
        'n_triple_original': n_triple_original,
        'n_triple_retained': n_triple_retained,
        'n_triple_potential_left_censored': n_triple_left_censored,
        'pct_triple_reclassified': pct_reclassified,
        'n_triple_hba1c_pairs_retained': len(triple_hba1c_retained),
        'median_triple_hba1c_change_retained': median_hba1c_change
    })

    print(f"{w:<12} {n_triple_retained:>16} {n_triple_left_censored:>14} {pct_reclassified:>15.1f}% "
          f"{len(triple_hba1c_retained):>14}")

left_censoring_group_df = pd.DataFrame(left_censoring_group_rows)
left_censoring_triple_df = pd.DataFrame(left_censoring_triple_rows)
left_censoring_sensitivity_df = pd.concat([
    left_censoring_group_df.assign(table_type='group_retention'),
    left_censoring_triple_df.assign(table_type='triple_sensitivity')
], ignore_index=True, sort=False)

left_censoring_path = os.path.join(output_dir, 'left_censoring_sensitivity.csv')
left_censoring_sensitivity_df.to_csv(left_censoring_path, index=False)

# -----------------------------------------------------------------------------
# C) HbA1c frequency/measurement bias and mitigation
# -----------------------------------------------------------------------------

print("\n" + "-"*80)
print("C) HbA1c MEASUREMENT FREQUENCY BIAS")
print("-"*80)

hba1c_frequency_patient = person_timeline[[
    'person_id', 'original_group', 'hba1c_pre_count_original', 'hba1c_post_count_original'
]].copy()
hba1c_frequency_patient = hba1c_frequency_patient.rename(columns={'original_group': 'group'})
hba1c_frequency_patient['hba1c_total_count_original'] = (
    hba1c_frequency_patient['hba1c_pre_count_original'] + hba1c_frequency_patient['hba1c_post_count_original']
)
hba1c_frequency_patient['has_pre_original'] = hba1c_frequency_patient['hba1c_pre_count_original'] > 0
hba1c_frequency_patient['has_post_original'] = hba1c_frequency_patient['hba1c_post_count_original'] > 0
hba1c_frequency_patient['has_both_original'] = (
    hba1c_frequency_patient['has_pre_original'] & hba1c_frequency_patient['has_post_original']
)

hba1c_frequency_group_df = hba1c_frequency_patient.groupby('group').agg(
    n_patients=('person_id', 'nunique'),
    median_pre_count=('hba1c_pre_count_original', 'median'),
    median_post_count=('hba1c_post_count_original', 'median'),
    median_total_count=('hba1c_total_count_original', 'median'),
    pre_availability_rate=('has_pre_original', 'mean'),
    post_availability_rate=('has_post_original', 'mean'),
    pre_post_availability_rate=('has_both_original', 'mean')
).reset_index()

hba1c_frequency_group_df['pre_availability_rate'] *= 100
hba1c_frequency_group_df['post_availability_rate'] *= 100
hba1c_frequency_group_df['pre_post_availability_rate'] *= 100

print(f"{'Group':<25} {'N':>7} {'Med Pre':>9} {'Med Post':>10} {'Pre %':>9} {'Post %':>9} {'Pre+Post %':>12}")
print("-"*85)
for _, row in hba1c_frequency_group_df.iterrows():
    print(f"{row['group']:<25} {int(row['n_patients']):>7} {row['median_pre_count']:>9.1f} "
          f"{row['median_post_count']:>10.1f} {row['pre_availability_rate']:>8.1f}% "
          f"{row['post_availability_rate']:>8.1f}% {row['pre_post_availability_rate']:>11.1f}%")

hba1c_change_analysis = pre_post_complete[
    pre_post_complete['measurement_category'] == 'HbA1c'
][['person_id', 'group', 'change']].copy()
hba1c_change_analysis = hba1c_change_analysis.merge(
    hba1c_frequency_patient[['person_id', 'hba1c_total_count_original']],
    on='person_id', how='left'
)
hba1c_change_analysis['hba1c_total_count_original'] = hba1c_change_analysis['hba1c_total_count_original'].fillna(1).clip(lower=1)
hba1c_change_analysis['inverse_intensity_weight'] = 1.0 / hba1c_change_analysis['hba1c_total_count_original']

mitigation_rows = []

for g in GROUPS:
    g_data = hba1c_change_analysis[hba1c_change_analysis['group'] == g]
    if len(g_data) == 0:
        continue
    mitigation_rows.append({
        'scenario': 'Unweighted',
        'group': g,
        'n': len(g_data),
        'median_change': g_data['change'].median(),
        'mean_change': g_data['change'].mean()
    })
    mitigation_rows.append({
        'scenario': 'Inverse_Intensity_Weighted',
        'group': g,
        'n': len(g_data),
        'median_change': g_data['change'].median(),
        'mean_change': np.average(g_data['change'], weights=g_data['inverse_intensity_weight'])
    })

q10 = hba1c_change_analysis.groupby('group')['hba1c_total_count_original'].quantile(0.10)
q90 = hba1c_change_analysis.groupby('group')['hba1c_total_count_original'].quantile(0.90)

overlap_low = int(np.ceil(q10.max())) if len(q10) > 0 else 1
overlap_high = int(np.floor(q90.min())) if len(q90) > 0 else int(hba1c_change_analysis['hba1c_total_count_original'].max())

if overlap_low > overlap_high:
    overlap_low = 1
    overlap_high = int(np.floor(hba1c_change_analysis['hba1c_total_count_original'].quantile(0.90)))

restricted_data = hba1c_change_analysis[
    (hba1c_change_analysis['hba1c_total_count_original'] >= overlap_low) &
    (hba1c_change_analysis['hba1c_total_count_original'] <= overlap_high)
]

for g in GROUPS:
    g_data = restricted_data[restricted_data['group'] == g]
    if len(g_data) == 0:
        continue
    mitigation_rows.append({
        'scenario': f'Restricted_Comparable_Intensity_{overlap_low}_to_{overlap_high}',
        'group': g,
        'n': len(g_data),
        'median_change': g_data['change'].median(),
        'mean_change': g_data['change'].mean()
    })

hba1c_mitigation_results_df = pd.DataFrame(mitigation_rows)

print(f"\nMitigation strategy applied: inverse-intensity weighting + restricted comparable intensity "
      f"({overlap_low}-{overlap_high} HbA1c measures)")
print(f"{'Scenario':<45} {'Group':<25} {'n':>6} {'Median Delta':>14} {'Mean Delta':>12}")
print("-"*110)
for _, row in hba1c_mitigation_results_df.iterrows():
    print(f"{row['scenario']:<45} {row['group']:<25} {int(row['n']):>6} {row['median_change']:>+14.3f} {row['mean_change']:>+12.3f}")

hba1c_frequency_path = os.path.join(output_dir, 'hba1c_frequency_diagnostics.csv')
hba1c_mitigation_path = os.path.join(output_dir, 'hba1c_frequency_mitigation_results.csv')
hba1c_frequency_group_df.to_csv(hba1c_frequency_path, index=False)
hba1c_mitigation_results_df.to_csv(hba1c_mitigation_path, index=False)

# -----------------------------------------------------------------------------
# Bias & validity report (printed and saved)
# -----------------------------------------------------------------------------

combo_immortal_pct = (
    (at_risk_subset['immortal_time_days'] > 0).mean() * 100 if len(at_risk_subset) > 0 else np.nan
)
triple_365 = left_censoring_triple_df[left_censoring_triple_df['lookback_days'] == 365]
triple_365_reclass = (
    float(triple_365['pct_triple_reclassified'].iloc[0]) if len(triple_365) > 0 else np.nan
)
measurement_variability = (
    hba1c_frequency_group_df['median_total_count'].max() - hba1c_frequency_group_df['median_total_count'].min()
    if len(hba1c_frequency_group_df) > 0 else np.nan
)

report_lines = [
    "="*90,
    "BIAS & VALIDITY REPORT",
    "="*90,
    "",
    "A) Immortal Time Bias",
    f"- Assessment: {'Detected' if immortal_time_detected else 'Not clearly detected'} "
    f"(combination-group IT>0 rate: {combo_immortal_pct:.1f}%).",
    f"- Fix implemented: Landmark time-zero alignment at {LANDMARK_DAYS} days from first observed exposure.",
    f"- Impact: {int(person_timeline['group_changed_after_alignment'].sum())} of {len(person_timeline)} "
    "patients changed exposure group after alignment.",
    f"- Diagnostics exported: {immortal_time_path}",
    "",
    "B) Left Censoring / Triple Therapy Misclassification",
    "- Assessment: evaluated observable lookback before first observed exposure.",
    f"- Triple-therapy reclassification at 365-day lookback: {triple_365_reclass:.1f}% potentially left-censored.",
    "- Sensitivity implemented with alternative lookback windows: 90, 180, 365, 730 days.",
    f"- Diagnostics exported: {left_censoring_path}",
    "",
    "C) HbA1c Frequency / Measurement Bias",
    f"- Assessment: median total HbA1c count varied by {measurement_variability:.1f} across groups.",
    "- Mitigation implemented: inverse-intensity weighting and restricted comparable-intensity analysis.",
    "- Reported results include unweighted and mitigated estimates.",
    f"- Diagnostics exported: {hba1c_frequency_path}",
    f"- Mitigation results exported: {hba1c_mitigation_path}",
    "",
    "Bias checks complete."
]

bias_report_path = os.path.join(output_dir, 'bias_validity_report.txt')
with open(bias_report_path, 'w') as f:
    f.write("\n".join(report_lines))

print("\n" + "\n".join(report_lines))
print(f"\nBias report saved to: {bias_report_path}")

# ============================================================================
# FINAL SUMMARY
# ============================================================================

print("\n" + "="*100)
print("ANALYSIS COMPLETE - NOVEL ANALYSES SUMMARY")
print("="*100)

print("""
This comprehensive analysis file includes 7 NOVEL RESEARCH ANALYSES
plus PART 2 BIAS & COHORT VALIDITY checks:

PART 1 ENHANCEMENTS (Data Definitions & Extraction Consistency):
A) Centralized concept sets (concept_sets.py) with inventory table
B) Standardized BMI extraction with validation checks
C) Binary smoking variable from survey data with QA table
D) Measurement timing rules (minimum pre/post gap enforcement)
   - Inflammatory biomarkers (CRP, ESR, Fibrinogen) excluded per protocol

PART 2 BIAS & VALIDITY DELIVERABLES:
A) Immortal time bias diagnostics + landmark time-zero correction
B) Left-censoring sensitivity for triple therapy (90/180/365/730-day lookback)
C) HbA1c measurement-frequency diagnostics + mitigation analyses
Saved outputs:
- bias_validity_report.txt
- immortal_time_diagnostics.csv
- left_censoring_sensitivity.csv
- hba1c_frequency_diagnostics.csv

1. TREATMENT SEQUENCING / INTENSIFICATION PATTERNS
   - Does medication order matter? (Metformin first vs GLP-1 first)
   - Early triple therapy vs stepwise escalation

2. TIME-TO-GLYCEMIC CONTROL (SURVIVAL ANALYSIS)
   - How quickly do patients achieve HbA1c <7%?
   - Kaplan-Meier and Cox regression

3. TREATMENT RESPONSE HETEROGENEITY / PHENOTYPING
   - ML clustering to identify patient phenotypes
   - Who benefits most from GLP-1?

4. DISCORDANT RESPONDERS
   - Patients with glycemic but not lipid response (or vice versa)
   - Characterization of discordant response patterns

5. GLP-1 EFFECT IN STATIN-NAIVE POPULATION
   - Independent lipid effects of GLP-1 without statin confounding
   - Relevant for statin-intolerant patients

6. INFLAMMATORY TRAJECTORY AS PREDICTOR -- REMOVED
   - Excluded: inflammatory biomarkers not in concept set inventory

7. THERAPEUTIC INERTIA
   - Time on suboptimal therapy before intensification
   - Health equity analysis by demographics

Each analysis includes:
- Research question and clinical relevance
- Detailed statistical methodology
- Results with 95% CI and p-values
- Clinical interpretation and novel contribution
""")

print("\n" + "="*100)
print("END OF ADVANCED ANALYSIS")
print("="*100)
